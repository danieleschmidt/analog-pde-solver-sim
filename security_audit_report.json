{
  "timestamp": "2025-08-07T12:07:35.238550",
  "project_root": ".",
  "statistics": {
    "files_scanned": 69,
    "lines_scanned": 1101,
    "high_risk": 4,
    "medium_risk": 6,
    "low_risk": 8
  },
  "findings": [
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docker-compose.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docs/github-workflows/autonomous-value-discovery.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docs/github-workflows/ci.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docs/github-workflows/performance.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "git_history_secrets",
      "severity": "medium",
      "description": "Potential secrets found in git commit messages",
      "file_path": "git_history",
      "line_number": null,
      "details": "ad1cf68 feat(ci): add comprehensive GitHub Actions workflows and documentation\n14ef4ca \ud83d\ude80 Comprehensive SDLC Enhancement: Developing to Maturing Maturity\n3e48f33 \ud83d\ude80 Foundational SDLC Enhancement: Nascent to Developing Maturity"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/conftest.py",
      "line_number": 1,
      "details": "import pytest\nimport numpy as np\nimport tempfile\nimport os\nfrom pathlib import Path\n\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield Path(tmpdir)\n\n\n@pytest.fixture\ndef sample_grid():\n    \"\"\"Create a sample 2D grid for testing.\"\"\"\n    return np.meshgrid(np.linspace(0, 1, 32), np.linspace(0, 1, 32))\n\n\n@pytest.fixture\ndef mock_spice_simulator():\n    \"\"\"Mock SPICE simulator for testing without external dependencies.\"\"\"\n    class MockSpiceSimulator:\n        def __init__(self):\n            self.components = []\n            self.simulated = False\n            \n        def add_component(self, name, type_, **kwargs):\n            self.components.append({\"name\": name, \"type\": type_, **kwargs})\n            \n        def simulate(self, **kwargs):\n            self.simulated = True\n            return {\"success\": True, \"results\": np.random.random((10, 10))}\n    \n    return MockSpiceSimulator()\n\n\n@pytest.fixture\ndef sample_conductance_matrix():\n    \"\"\"Sample conductance matrix for crossbar testing.\"\"\"\n    return np.random.uniform(1e-9, 1e-6, (16, 16))\n\n\n@pytest.fixture(scope=\"session\")\ndef test_data_dir():\n    \"\"\"Path to test data directory.\"\"\"\n    return Path(__file__).parent / \"data\"\n\n\n@pytest.fixture\ndef enable_slow_tests(request):\n    \"\"\"Enable slow tests when explicitly requested.\"\"\"\n    if request.config.getoption(\"--runslow\"):\n        return True\n    pytest.skip(\"need --runslow option to run\")\n\n\ndef pytest_addoption(parser):\n    \"\"\"Add custom command line options.\"\"\"\n    parser.addoption(\n        \"--runslow\", action=\"store_true\", default=False, help=\"run slow tests\"\n    )\n    parser.addoption(\n        \"--runhardware\", action=\"store_true\", default=False, help=\"run hardware tests\"\n    )"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/core/solver.py",
      "line_number": 1,
      "details": "\"\"\"Main analog PDE solver implementation.\"\"\"\n\nimport numpy as np\nfrom typing import Dict, Any, Optional\nfrom .crossbar import AnalogCrossbarArray\n\n\nclass AnalogPDESolver:\n    \"\"\"Analog crossbar-based PDE solver with noise modeling.\"\"\"\n    \n    def __init__(\n        self,\n        crossbar_size: int = 128,\n        conductance_range: tuple = (1e-9, 1e-6),\n        noise_model: str = \"realistic\"\n    ):\n        \"\"\"Initialize analog PDE solver.\n        \n        Args:\n            crossbar_size: Size of crossbar array\n            conductance_range: Min/max conductance values in Siemens\n            noise_model: Noise modeling approach ('none', 'gaussian', 'realistic')\n        \"\"\"\n        self.crossbar_size = crossbar_size\n        self.conductance_range = conductance_range\n        self.noise_model = noise_model\n        self.crossbar = AnalogCrossbarArray(crossbar_size, crossbar_size)\n        \n    def map_pde_to_crossbar(self, pde) -> Dict[str, Any]:\n        \"\"\"Map PDE discretization matrix to crossbar conductances.\"\"\"\n        # Generate finite difference Laplacian matrix\n        size = self.crossbar_size\n        laplacian = self._create_laplacian_matrix(size)\n        \n        # Program crossbar with Laplacian operator\n        self.crossbar.program_conductances(laplacian)\n        \n        return {\n            \"matrix_size\": size,\n            \"conductance_range\": self.conductance_range,\n            \"programming_success\": True\n        }\n        \n    def solve(\n        self, \n        pde,\n        iterations: int = 100,\n        convergence_threshold: float = 1e-6\n    ) -> np.ndarray:\n        \"\"\"Solve PDE using analog crossbar computation.\"\"\"\n        # Map PDE to crossbar\n        config = self.map_pde_to_crossbar(pde)\n        \n        # Initialize solution vector\n        size = config[\"matrix_size\"]\n        phi = np.random.random(size) * 0.1\n        \n        # Create source term\n        if hasattr(pde, 'source_function') and pde.source_function:\n            x = np.linspace(0, 1, size)\n            source = np.array([pde.source_function(xi, 0) for xi in x])\n        else:\n            source = np.ones(size) * 0.1\n        \n        # Iterative analog solver\n        for i in range(iterations):\n            # Analog matrix-vector multiplication\n            residual = self.crossbar.compute_vmm(phi) + source\n            \n            # Jacobi-style update\n            phi_new = phi - 0.1 * residual\n            \n            # Apply boundary conditions\n            phi_new[0] = 0.0  # Dirichlet BC\n            phi_new[-1] = 0.0\n            \n            # Check convergence\n            error = np.linalg.norm(phi_new - phi)\n            phi = phi_new\n            \n            if error < convergence_threshold:\n                break\n                \n        return phi\n    \n    def _create_laplacian_matrix(self, size: int) -> np.ndarray:\n        \"\"\"Create finite difference Laplacian matrix.\"\"\"\n        laplacian = np.zeros((size, size))\n        \n        # Main diagonal\n        np.fill_diagonal(laplacian, -2.0)\n        \n        # Off-diagonals\n        for i in range(size - 1):\n            laplacian[i, i + 1] = 1.0\n            laplacian[i + 1, i] = 1.0\n            \n        return laplacian"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/core/crossbar_robust.py",
      "line_number": 1,
      "details": "\"\"\"Enhanced analog crossbar array with comprehensive device modeling.\"\"\"\n\nimport numpy as np\nfrom typing import Tuple, Optional\nimport logging\nfrom ..utils.logging_config import get_logger\n\n\nclass RobustAnalogCrossbarArray:\n    \"\"\"Enhanced analog crossbar array with realistic device modeling and error handling.\"\"\"\n    \n    def __init__(\n        self, \n        rows: int, \n        cols: int, \n        cell_type: str = \"1T1R\",\n        noise_model: str = \"realistic\"\n    ):\n        \"\"\"Initialize enhanced crossbar array.\n        \n        Args:\n            rows: Number of rows (input size)\n            cols: Number of columns (output size)  \n            cell_type: Crossbar cell type ('1T1R', 'ReRAM', 'PCM')\n            noise_model: Noise modeling approach\n        \"\"\"\n        self.rows = rows\n        self.cols = cols\n        self.cell_type = cell_type\n        self.noise_model = noise_model\n        \n        # Initialize conductance matrices\n        self.g_positive = np.zeros((rows, cols), dtype=np.float64)\n        self.g_negative = np.zeros((rows, cols), dtype=np.float64)\n        \n        # Device parameters\n        self.device_params = self._get_device_parameters()\n        \n        # Initialize logger\n        self.logger = get_logger('crossbar')\n        \n        # State tracking\n        self.is_programmed = False\n        self.programming_errors = 0\n        self.operation_count = 0\n        \n        self.logger.info(\n            f\"Initialized {rows}x{cols} crossbar array \"\n            f\"(cell_type={cell_type}, noise_model={noise_model})\"\n        )\n    \n    def program_conductances(self, target_matrix: np.ndarray) -> None:\n        \"\"\"Map target matrix to positive/negative conductance pairs with validation.\"\"\"\n        try:\n            self.logger.debug(f\"Programming {target_matrix.shape} matrix to crossbar\")\n            \n            # Validate input matrix\n            if target_matrix.shape != (self.rows, self.cols):\n                raise ValueError(\n                    f\"Matrix shape {target_matrix.shape} does not match \"\n                    f\"crossbar size {(self.rows, self.cols)}\"\n                )\n            \n            if not np.isfinite(target_matrix).all():\n                raise ValueError(\"Target matrix contains NaN or infinity\")\n            \n            g_min, g_max = self.device_params['g_range']\n            \n            # Decompose into positive and negative components\n            pos_matrix = np.maximum(target_matrix, 0)\n            neg_matrix = np.maximum(-target_matrix, 0)\n            \n            # Scale to conductance range with error handling\n            self.g_positive = self._scale_to_conductance(pos_matrix, g_min, g_max)\n            self.g_negative = self._scale_to_conductance(neg_matrix, g_min, g_max)\n            \n            # Add programming variations\n            self._add_programming_variations()\n            \n            self.is_programmed = True\n            self.programming_errors = 0\n            \n            self.logger.info(\n                f\"Successfully programmed crossbar with matrix range \"\n                f\"[{target_matrix.min():.2e}, {target_matrix.max():.2e}]\"\n            )\n            \n        except Exception as e:\n            self.programming_errors += 1\n            self.logger.error(f\"Programming failed: {e}\")\n            raise\n    \n    def compute_vmm(self, input_vector: np.ndarray) -> np.ndarray:\n        \"\"\"Analog vector-matrix multiplication with comprehensive error handling.\"\"\"\n        try:\n            # Validate inputs\n            if not self.is_programmed:\n                raise RuntimeError(\"Crossbar not programmed\")\n            \n            if len(input_vector) != self.rows:\n                raise ValueError(\n                    f\"Input vector length {len(input_vector)} does not match \"\n                    f\"crossbar rows {self.rows}\"\n                )\n            \n            if not np.isfinite(input_vector).all():\n                raise ValueError(\"Input vector contains NaN or infinity\")\n            \n            # Clamp input voltage range for realistic operation\n            input_clamped = np.clip(input_vector, -1.0, 1.0)\n            \n            # Ohm's law: I = G \u00d7 V with error handling\n            try:\n                i_pos = np.dot(self.g_positive.T, input_clamped)\n                i_neg = np.dot(self.g_negative.T, input_clamped)\n            except np.linalg.LinAlgError as e:\n                raise RuntimeError(f\"Matrix multiplication failed: {e}\")\n            \n            # Differential current sensing\n            output_current = i_pos - i_neg\n            \n            # Add device noise based on model\n            noise = self._compute_noise(output_current)\n            output_with_noise = output_current + noise\n            \n            # Add device non-linearities\n            output_final = self._apply_device_nonlinearities(output_with_noise)\n            \n            # Check for numerical issues\n            if not np.isfinite(output_final).all():\n                self.logger.warning(\"VMM output contains non-finite values\")\n                output_final = np.nan_to_num(output_final, nan=0.0, posinf=1e6, neginf=-1e6)\n            \n            # Update operation counter\n            self.operation_count += 1\n            \n            # Apply device aging effects\n            if self.operation_count % 1000 == 0:\n                self._apply_aging_effects()\n            \n            return output_final\n            \n        except Exception as e:\n            self.logger.error(f\"VMM computation failed: {e}\")\n            raise\n    \n    def _scale_to_conductance(\n        self, \n        matrix: np.ndarray, \n        g_min: float, \n        g_max: float\n    ) -> np.ndarray:\n        \"\"\"Scale matrix values to conductance range with robust handling.\"\"\"\n        try:\n            # Handle zero matrix\n            if np.allclose(matrix, 0):\n                return np.full_like(matrix, g_min, dtype=np.float64)\n            \n            # Robust scaling\n            matrix_max = np.max(np.abs(matrix))\n            if matrix_max == 0:\n                return np.full_like(matrix, g_min, dtype=np.float64)\n            \n            # Linear scaling with bounds checking\n            scaled = g_min + (g_max - g_min) * np.abs(matrix) / matrix_max\n            \n            # Ensure values are within bounds\n            scaled = np.clip(scaled, g_min, g_max)\n            \n            return scaled.astype(np.float64)\n            \n        except Exception as e:\n            self.logger.error(f\"Conductance scaling failed: {e}\")\n            return np.full_like(matrix, g_min, dtype=np.float64)\n    \n    def _compute_noise(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"Add realistic device noise based on specified model.\"\"\"\n        if self.noise_model == \"none\":\n            return np.zeros_like(signal)\n        \n        try:\n            noise = np.zeros_like(signal, dtype=np.float64)\n            \n            if self.noise_model in [\"gaussian\", \"realistic\"]:\n                # Thermal noise (Johnson-Nyquist)\n                thermal_std = self.device_params['thermal_noise'] * np.sqrt(np.abs(signal))\n                thermal_noise = np.random.normal(0, thermal_std)\n                noise += thermal_noise\n            \n            if self.noise_model == \"realistic\":\n                # Shot noise (Poissonian)\n                shot_std = self.device_params['shot_noise'] * np.sqrt(np.abs(signal))\n                shot_noise = np.random.normal(0, shot_std)\n                noise += shot_noise\n                \n                # 1/f (flicker) noise\n                flicker_std = self.device_params['flicker_noise'] * np.abs(signal)\n                flicker_noise = np.random.normal(0, flicker_std)\n                noise += flicker_noise\n                \n                # Random telegraph signal (RTS) noise for memristors\n                if self.cell_type in ['ReRAM', 'PCM']:\n                    rts_noise = self._generate_rts_noise(signal)\n                    noise += rts_noise\n            \n            return noise\n            \n        except Exception as e:\n            self.logger.warning(f\"Noise generation failed: {e}, using no noise\")\n            return np.zeros_like(signal)\n    \n    def _get_device_parameters(self) -> dict:\n        \"\"\"Get device-specific parameters.\"\"\"\n        base_params = {\n            'g_range': (1e-9, 1e-6),  # 1nS to 1\u03bcS\n            'thermal_noise': 0.01,\n            'shot_noise': 0.005,\n            'flicker_noise': 0.001,\n            'programming_variation': 0.05,\n            'drift_rate': 1e-6,\n            'aging_rate': 1e-8\n        }\n        \n        # Device-specific modifications\n        if self.cell_type == \"ReRAM\":\n            base_params['g_range'] = (1e-8, 1e-5)\n            base_params['programming_variation'] = 0.1\n            base_params['aging_rate'] = 5e-8\n        elif self.cell_type == \"PCM\":\n            base_params['g_range'] = (1e-7, 1e-4)\n            base_params['drift_rate'] = 1e-5\n            base_params['aging_rate'] = 2e-8\n        elif self.cell_type == \"1T1R\":\n            base_params['programming_variation'] = 0.02\n            base_params['aging_rate'] = 1e-9\n        \n        return base_params\n    \n    def _add_programming_variations(self):\n        \"\"\"Add realistic programming variations to conductances.\"\"\"\n        variation = self.device_params['programming_variation']\n        \n        # Multiplicative variations (log-normal distribution)\n        pos_variation = np.random.lognormal(0, variation, self.g_positive.shape)\n        neg_variation = np.random.lognormal(0, variation, self.g_negative.shape)\n        \n        self.g_positive *= pos_variation\n        self.g_negative *= neg_variation\n        \n        # Ensure bounds are maintained\n        g_min, g_max = self.device_params['g_range']\n        self.g_positive = np.clip(self.g_positive, g_min, g_max)\n        self.g_negative = np.clip(self.g_negative, g_min, g_max)\n    \n    def _generate_rts_noise(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"Generate random telegraph signal noise for memristive devices.\"\"\"\n        # Simplified RTS model\n        rts_amplitude = 0.001 * np.abs(signal)\n        rts_probability = 0.1  # 10% chance of switching\n        \n        rts_noise = np.zeros_like(signal)\n        for i in range(len(signal)):\n            if np.random.random() < rts_probability:\n                rts_noise[i] = np.random.choice([-1, 1]) * rts_amplitude[i]\n        \n        return rts_noise\n    \n    def _apply_device_nonlinearities(self, current: np.ndarray) -> np.ndarray:\n        \"\"\"Apply device non-linearities and saturation effects.\"\"\"\n        # Current saturation\n        max_current = 1e-3  # 1mA max per column\n        current_saturated = np.tanh(current / max_current) * max_current\n        \n        # Add conductance modulation effects\n        if self.cell_type == \"ReRAM\":\n            # ReRAM shows conductance modulation under high currents\n            modulation = 1 - 0.1 * np.tanh(np.abs(current_saturated) / 1e-4)\n            current_saturated *= modulation\n        elif self.cell_type == \"PCM\":\n            # PCM shows threshold switching behavior\n            threshold = 1e-5\n            mask = np.abs(current_saturated) > threshold\n            current_saturated[mask] *= 1.2  # Increased conductance above threshold\n        \n        return current_saturated\n    \n    def _apply_aging_effects(self):\n        \"\"\"Apply long-term device aging effects.\"\"\"\n        if not self.is_programmed:\n            return\n            \n        aging_rate = self.device_params['aging_rate']\n        \n        # Gradual conductance drift\n        drift_factor = 1 - aging_rate * self.operation_count\n        drift_factor = max(0.8, drift_factor)  # Limit maximum drift\n        \n        self.g_positive *= drift_factor\n        self.g_negative *= drift_factor\n        \n        # Ensure minimum conductances\n        g_min, _ = self.device_params['g_range']\n        self.g_positive = np.maximum(self.g_positive, g_min)\n        self.g_negative = np.maximum(self.g_negative, g_min)\n        \n        if self.operation_count % 10000 == 0:\n            self.logger.info(f\"Applied aging effects after {self.operation_count} operations\")\n    \n    def get_device_stats(self) -> dict:\n        \"\"\"Get comprehensive device statistics and health metrics.\"\"\"\n        stats = {\n            \"is_programmed\": self.is_programmed,\n            \"programming_errors\": self.programming_errors,\n            \"operation_count\": self.operation_count,\n            \"device_type\": self.cell_type,\n            \"noise_model\": self.noise_model,\n            \"array_size\": (self.rows, self.cols),\n            \"total_devices\": self.rows * self.cols\n        }\n        \n        if self.is_programmed:\n            # Conductance statistics\n            stats.update({\n                \"g_positive_range\": (float(self.g_positive.min()), float(self.g_positive.max())),\n                \"g_negative_range\": (float(self.g_negative.min()), float(self.g_negative.max())),\n                \"g_positive_mean\": float(self.g_positive.mean()),\n                \"g_negative_mean\": float(self.g_negative.mean()),\n                \"g_positive_std\": float(self.g_positive.std()),\n                \"g_negative_std\": float(self.g_negative.std())\n            })\n            \n            # Health metrics\n            g_min, g_max = self.device_params['g_range']\n            stuck_low = np.sum(self.g_positive <= g_min * 1.1) + np.sum(self.g_negative <= g_min * 1.1)\n            stuck_high = np.sum(self.g_positive >= g_max * 0.9) + np.sum(self.g_negative >= g_max * 0.9)\n            \n            stats.update({\n                \"health_stuck_low_devices\": int(stuck_low),\n                \"health_stuck_high_devices\": int(stuck_high),\n                \"health_percentage\": float(100 * (1 - (stuck_low + stuck_high) / (2 * self.rows * self.cols)))\n            })\n        \n        return stats\n    \n    def perform_calibration(self) -> dict:\n        \"\"\"Perform device calibration and return calibration data.\"\"\"\n        if not self.is_programmed:\n            return {\"status\": \"error\", \"message\": \"Crossbar not programmed\"}\n        \n        try:\n            # Test with known input patterns\n            test_patterns = [\n                np.ones(self.rows),\n                np.zeros(self.rows),\n                np.random.random(self.rows)\n            ]\n            \n            calibration_data = {\n                \"status\": \"success\",\n                \"timestamp\": np.datetime64('now').item().isoformat(),\n                \"test_results\": []\n            }\n            \n            for i, pattern in enumerate(test_patterns):\n                result = self.compute_vmm(pattern)\n                calibration_data[\"test_results\"].append({\n                    \"pattern_id\": i,\n                    \"input_norm\": float(np.linalg.norm(pattern)),\n                    \"output_norm\": float(np.linalg.norm(result)),\n                    \"output_mean\": float(np.mean(result)),\n                    \"output_std\": float(np.std(result))\n                })\n            \n            self.logger.info(\"Device calibration completed successfully\")\n            return calibration_data\n            \n        except Exception as e:\n            self.logger.error(f\"Calibration failed: {e}\")\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def reset_device(self):\n        \"\"\"Reset device to initial state.\"\"\"\n        self.g_positive = np.zeros((self.rows, self.cols), dtype=np.float64)\n        self.g_negative = np.zeros((self.rows, self.cols), dtype=np.float64)\n        self.is_programmed = False\n        self.programming_errors = 0\n        self.operation_count = 0\n        \n        self.logger.info(\"Device reset to initial state\")"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/e2e/test_full_pipeline.py",
      "line_number": 1,
      "details": "\"\"\"\nEnd-to-end tests for the complete analog PDE solver pipeline.\n\"\"\"\nimport pytest\nimport numpy as np\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch, MagicMock\n\n# Import test fixtures\nfrom tests.fixtures.pde_fixtures import poisson_2d_simple, hardware_configurations\n\n\n@pytest.mark.integration\n@pytest.mark.slow\nclass TestFullPipeline:\n    \"\"\"End-to-end pipeline tests.\"\"\"\n    \n    def test_complete_poisson_solver_pipeline(self, poisson_2d_simple, temp_dir):\n        \"\"\"Test complete pipeline from PDE specification to analog solution.\"\"\"\n        \n        # Mock the analog PDE solver components for testing\n        with patch('analog_pde_solver.core.solver.AnalogPDESolver') as MockSolver:\n            # Setup mock solver\n            mock_solver = MagicMock()\n            MockSolver.return_value = mock_solver\n            \n            # Mock the solve method to return a reasonable solution\n            grid_size = poisson_2d_simple.grid_size\n            x = np.linspace(0, 1, grid_size[0])\n            y = np.linspace(0, 1, grid_size[1])\n            X, Y = np.meshgrid(x, y)\n            mock_solution = poisson_2d_simple.analytical_solution(X, Y)\n            mock_solver.solve.return_value = mock_solution\n            \n            # Test the pipeline\n            try:\n                # 1. Create PDE specification\n                pde_spec = {\n                    \"type\": \"poisson\",\n                    \"grid_size\": grid_size,\n                    \"boundary_conditions\": poisson_2d_simple.boundary_conditions\n                }\n                \n                # 2. Initialize analog solver\n                solver = MockSolver(\n                    crossbar_size=max(grid_size),\n                    conductance_range=(1e-9, 1e-6),\n                    noise_model=\"realistic\"\n                )\n                \n                # 3. Map PDE to analog hardware\n                hardware_config = {\"mapped\": True}\n                mock_solver.map_pde_to_crossbar.return_value = hardware_config\n                \n                # 4. Solve the PDE\n                solution = mock_solver.solve(\n                    iterations=100,\n                    convergence_threshold=poisson_2d_simple.tolerance\n                )\n                \n                # 5. Verify solution quality\n                assert solution is not None\n                assert solution.shape == grid_size\n                \n                # Check that solution is reasonable (not all zeros or NaN)\n                assert not np.all(solution == 0), \"Solution is all zeros\"\n                assert not np.any(np.isnan(solution)), \"Solution contains NaN\"\n                assert not np.any(np.isinf(solution)), \"Solution contains infinity\"\n                \n                # 6. Generate RTL (mocked)\n                rtl_output = temp_dir / \"poisson_solver.v\"\n                mock_solver.to_rtl.return_value.save.return_value = str(rtl_output)\n                \n                # Verify RTL generation was called\n                mock_solver.to_rtl.assert_called_once()\n                \n            except Exception as e:\n                pytest.fail(f\"Pipeline test failed with exception: {e}\")\n    \n    def test_navier_stokes_pipeline(self, temp_dir):\n        \"\"\"Test Navier-Stokes solver pipeline.\"\"\"\n        \n        with patch('analog_pde_solver.NavierStokesAnalog') as MockNS:\n            mock_ns = MagicMock()\n            MockNS.return_value = mock_ns\n            \n            # Mock fluid simulation results\n            grid_size = (128, 128)\n            mock_u = np.random.random(grid_size) * 0.1\n            mock_v = np.random.random(grid_size) * 0.1\n            mock_pressure = np.random.random(grid_size)\n            \n            mock_ns.update_velocity.return_value = (mock_u, mock_v)\n            mock_ns.solve_pressure_poisson.return_value = mock_pressure\n            mock_ns.apply_pressure_correction.return_value = (mock_u, mock_v)\n            \n            try:\n                # Initialize Navier-Stokes solver\n                ns_solver = MockNS(\n                    resolution=(128, 128),\n                    reynolds_number=1000,\n                    time_step=0.001\n                )\n                \n                # Configure hardware\n                ns_solver.configure_hardware(\n                    num_crossbars=4,\n                    precision_bits=8,\n                    update_scheme=\"semi-implicit\"\n                )\n                \n                # Run simulation for a few timesteps\n                for timestep in range(10):\n                    u, v = ns_solver.update_velocity()\n                    pressure = ns_solver.solve_pressure_poisson()\n                    u, v = ns_solver.apply_pressure_correction(u, v, pressure)\n                    \n                    # Verify results are reasonable\n                    assert u is not None and v is not None\n                    assert pressure is not None\n                    assert u.shape == grid_size\n                    assert v.shape == grid_size\n                    assert pressure.shape == grid_size\n                \n                # Analyze power consumption\n                mock_power_analysis = MagicMock()\n                mock_power_analysis.avg_power_mw = 5.2\n                mock_power_analysis.energy_per_iter_nj = 0.8\n                mock_ns.analyze_power.return_value = mock_power_analysis\n                \n                power_analysis = ns_solver.analyze_power()\n                assert power_analysis.avg_power_mw > 0\n                assert power_analysis.energy_per_iter_nj > 0\n                \n            except Exception as e:\n                pytest.fail(f\"Navier-Stokes pipeline test failed: {e}\")\n    \n    @pytest.mark.hardware\n    def test_spice_integration_pipeline(self, hardware_configurations, temp_dir):\n        \"\"\"Test SPICE integration pipeline.\"\"\"\n        \n        config = hardware_configurations[0]  # Use small crossbar config\n        \n        with patch('analog_pde_solver.spice.SPICESimulator') as MockSpice:\n            mock_spice = MagicMock()\n            MockSpice.return_value = mock_spice\n            \n            # Mock SPICE simulation results\n            mock_results = MagicMock()\n            mock_results.get_node_voltages.return_value = np.random.random((64, 64))\n            mock_spice.transient.return_value = mock_results\n            \n            try:\n                # Create SPICE simulator\n                spice_sim = MockSpice()\n                \n                # Add crossbar components\n                rows, cols = config[\"size\"]\n                for i in range(min(rows, 8)):  # Limit for testing\n                    for j in range(min(cols, 8)):\n                        spice_sim.add_component(\n                            f\"R_{i}_{j}\",\n                            \"memristor\",\n                            nodes=(f\"row_{i}\", f\"col_{j}\"),\n                            params={\n                                \"ron\": 1e3,\n                                \"roff\": 1e6,\n                                \"rinit\": 1e4,\n                                \"model\": \"hp_memristor\"\n                            }\n                        )\n                \n                # Add peripheral circuits\n                spice_sim.add_dac_array(\"input_dac\", resolution=config[\"dac_bits\"], voltage_range=1.0)\n                spice_sim.add_adc_array(\"output_adc\", resolution=config[\"adc_bits\"], sampling_rate=1e6)\n                \n                # Run simulation\n                results = spice_sim.transient(\n                    stop_time=1e-3,\n                    time_step=1e-6,\n                    initial_conditions={\"initial\": \"state\"}\n                )\n                \n                # Extract and verify solution\n                analog_solution = results.get_node_voltages(\"output_nodes\")\n                assert analog_solution is not None\n                assert analog_solution.shape == (64, 64)\n                assert not np.any(np.isnan(analog_solution))\n                \n            except Exception as e:\n                pytest.fail(f\"SPICE integration pipeline test failed: {e}\")\n    \n    def test_rtl_generation_pipeline(self, temp_dir):\n        \"\"\"Test RTL generation pipeline.\"\"\"\n        \n        with patch('analog_pde_solver.compiler.TorchToAnalog') as MockCompiler:\n            mock_compiler = MagicMock()\n            MockCompiler.return_value = mock_compiler\n            \n            # Mock compiled model\n            mock_analog_model = MagicMock()\n            mock_compiler.compile.return_value = mock_analog_model\n            \n            try:\n                # Create compiler\n                compiler = MockCompiler()\n                \n                # Mock PyTorch model compilation\n                mock_model = MagicMock()\n                analog_model = compiler.compile(\n                    model=mock_model,\n                    target_hardware=\"crossbar_array\",\n                    optimization_level=3\n                )\n                \n                # Generate RTL files\n                rtl_file = temp_dir / \"pde_accelerator.v\"\n                constraints_file = temp_dir / \"constraints.xdc\"\n                \n                analog_model.export_rtl.return_value = str(rtl_file)\n                analog_model.export_constraints.return_value = str(constraints_file)\n                \n                # Verify RTL generation\n                rtl_output = analog_model.export_rtl(\"pde_accelerator.v\")\n                constraints_output = analog_model.export_constraints(\"constraints.xdc\")\n                \n                assert rtl_output is not None\n                assert constraints_output is not None\n                \n                # Verify methods were called\n                analog_model.export_rtl.assert_called_once()\n                analog_model.export_constraints.assert_called_once()\n                \n            except Exception as e:\n                pytest.fail(f\"RTL generation pipeline test failed: {e}\")\n    \n    def test_error_handling_pipeline(self):\n        \"\"\"Test error handling throughout the pipeline.\"\"\"\n        \n        with patch('analog_pde_solver.core.solver.AnalogPDESolver') as MockSolver:\n            # Test convergence failure\n            mock_solver = MagicMock()\n            MockSolver.return_value = mock_solver\n            mock_solver.solve.side_effect = RuntimeError(\"Convergence failed\")\n            \n            with pytest.raises(RuntimeError, match=\"Convergence failed\"):\n                solver = MockSolver()\n                solver.solve()\n            \n            # Test invalid hardware configuration\n            mock_solver.solve.side_effect = ValueError(\"Invalid crossbar size\")\n            \n            with pytest.raises(ValueError, match=\"Invalid crossbar size\"):\n                solver = MockSolver()\n                solver.solve()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/performance/test_performance_benchmarks.py",
      "line_number": 1,
      "details": "\"\"\"\nPerformance benchmarking tests for analog PDE solver.\n\"\"\"\nimport pytest\nimport time\nimport psutil\nimport numpy as np\nfrom contextlib import contextmanager\nfrom typing import Dict, Any\n\n# Import the performance fixtures\nfrom tests.fixtures.pde_fixtures import performance_benchmarks\n\n\nclass PerformanceMonitor:\n    \"\"\"Monitor performance metrics during test execution.\"\"\"\n    \n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n        self.start_memory = None\n        self.end_memory = None\n        self.peak_memory = None\n        \n    @contextmanager\n    def monitor(self):\n        \"\"\"Context manager for performance monitoring.\"\"\"\n        # Record start metrics\n        self.start_time = time.perf_counter()\n        self.start_memory = psutil.Process().memory_info().rss\n        self.peak_memory = self.start_memory\n        \n        try:\n            yield self\n        finally:\n            # Record end metrics\n            self.end_time = time.perf_counter()\n            self.end_memory = psutil.Process().memory_info().rss\n            self.peak_memory = max(self.peak_memory, self.end_memory)\n    \n    @property\n    def elapsed_time(self) -> float:\n        \"\"\"Get elapsed time in seconds.\"\"\"\n        if self.start_time and self.end_time:\n            return self.end_time - self.start_time\n        return 0.0\n    \n    @property\n    def memory_usage(self) -> int:\n        \"\"\"Get peak memory usage in bytes.\"\"\"\n        return self.peak_memory - self.start_memory if self.peak_memory and self.start_memory else 0\n\n\n@pytest.mark.slow\n@pytest.mark.performance\nclass TestPerformanceBenchmarks:\n    \"\"\"Performance benchmark test suite.\"\"\"\n    \n    def test_small_problem_performance(self, performance_benchmarks):\n        \"\"\"Test performance on small problems.\"\"\"\n        benchmark = performance_benchmarks[\"small\"]\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            # Simulate analog PDE solving\n            grid_size = benchmark[\"grid_size\"]\n            matrix = np.random.random(grid_size)\n            \n            # Simulate iterative solving\n            for _ in range(100):\n                matrix = 0.25 * (\n                    np.roll(matrix, 1, axis=0) +\n                    np.roll(matrix, -1, axis=0) +\n                    np.roll(matrix, 1, axis=1) +\n                    np.roll(matrix, -1, axis=1)\n                )\n        \n        # Verify performance constraints\n        assert monitor.elapsed_time < benchmark[\"max_time\"], \\\n            f\"Small problem took {monitor.elapsed_time:.2f}s, expected < {benchmark['max_time']}s\"\n        \n        assert monitor.memory_usage < benchmark[\"max_memory\"], \\\n            f\"Small problem used {monitor.memory_usage} bytes, expected < {benchmark['max_memory']} bytes\"\n    \n    @pytest.mark.slow\n    def test_medium_problem_performance(self, performance_benchmarks):\n        \"\"\"Test performance on medium problems.\"\"\"\n        benchmark = performance_benchmarks[\"medium\"]\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            grid_size = benchmark[\"grid_size\"]\n            matrix = np.random.random(grid_size)\n            \n            # Simulate more intensive computation\n            for _ in range(50):\n                matrix = 0.25 * (\n                    np.roll(matrix, 1, axis=0) +\n                    np.roll(matrix, -1, axis=0) +\n                    np.roll(matrix, 1, axis=1) +\n                    np.roll(matrix, -1, axis=1)\n                )\n        \n        assert monitor.elapsed_time < benchmark[\"max_time\"]\n        assert monitor.memory_usage < benchmark[\"max_memory\"]\n    \n    @pytest.mark.slow\n    @pytest.mark.hardware\n    def test_large_problem_performance(self, performance_benchmarks):\n        \"\"\"Test performance on large problems (requires hardware acceleration).\"\"\"\n        benchmark = performance_benchmarks[\"large\"]\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            grid_size = benchmark[\"grid_size\"]\n            # For large problems, we'd typically use hardware acceleration\n            matrix = np.random.random(grid_size)\n            \n            # Simulate hardware-accelerated computation\n            for _ in range(10):  # Fewer iterations due to hardware speedup\n                matrix = 0.25 * (\n                    np.roll(matrix, 1, axis=0) +\n                    np.roll(matrix, -1, axis=0) +\n                    np.roll(matrix, 1, axis=1) +\n                    np.roll(matrix, -1, axis=1)\n                )\n        \n        assert monitor.elapsed_time < benchmark[\"max_time\"]\n        assert monitor.memory_usage < benchmark[\"max_memory\"]\n    \n    def test_convergence_performance(self):\n        \"\"\"Test convergence rate performance.\"\"\"\n        grid_size = (128, 128)\n        tolerance = 1e-6\n        max_iterations = 1000\n        \n        # Create test problem\n        solution = np.random.random(grid_size)\n        target = np.zeros_like(solution)\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            for iteration in range(max_iterations):\n                old_solution = solution.copy()\n                \n                # Gauss-Seidel iteration\n                solution[1:-1, 1:-1] = 0.25 * (\n                    solution[0:-2, 1:-1] +  # up\n                    solution[2:, 1:-1] +    # down\n                    solution[1:-1, 0:-2] +  # left\n                    solution[1:-1, 2:]      # right\n                )\n                \n                # Check convergence\n                residual = np.max(np.abs(solution - old_solution))\n                if residual < tolerance:\n                    break\n        \n        # Verify reasonable convergence\n        assert iteration < max_iterations * 0.8, \\\n            f\"Convergence took {iteration} iterations, expected < {max_iterations * 0.8}\"\n        \n        # Verify performance\n        time_per_iteration = monitor.elapsed_time / (iteration + 1)\n        assert time_per_iteration < 0.1, \\\n            f\"Average time per iteration: {time_per_iteration:.3f}s, expected < 0.1s\"\n\n\n@pytest.mark.performance\ndef test_memory_scaling():\n    \"\"\"Test memory usage scaling with problem size.\"\"\"\n    sizes = [(32, 32), (64, 64), (128, 128)]\n    memory_usage = []\n    \n    for size in sizes:\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            # Allocate arrays similar to analog solver\n            matrix = np.random.random(size)\n            conductance_pos = np.random.random(size)\n            conductance_neg = np.random.random(size)\n            \n            # Simulate some computation\n            result = matrix @ conductance_pos.T - matrix @ conductance_neg.T\n        \n        memory_usage.append(monitor.memory_usage)\n    \n    # Verify roughly linear scaling (within 2x tolerance)\n    for i in range(1, len(memory_usage)):\n        size_ratio = (sizes[i][0] * sizes[i][1]) / (sizes[i-1][0] * sizes[i-1][1])\n        memory_ratio = memory_usage[i] / memory_usage[i-1]\n        \n        assert memory_ratio < size_ratio * 2, \\\n            f\"Memory scaling worse than expected: {memory_ratio:.2f} vs {size_ratio:.2f}\"\n\n\nif __name__ == \"__main__\":\n    # Run performance tests\n    pytest.main([__file__, \"-v\", \"--tb=short\"])"
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "automation/dependency-updater.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "metrics/collect-metrics.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/autonomous-discovery.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/run-benchmarks.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/simple-discovery.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/update-backlog.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/validate-workflows.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "monitoring/health-checks/health_check.py",
      "line_number": null,
      "details": null
    }
  ],
  "summary": {
    "total_findings": 18,
    "risk_score": 78,
    "risk_level": "HIGH",
    "categories": {
      "weak_crypto": {
        "count": 5,
        "severity_breakdown": {
          "medium": 5
        }
      },
      "executable_python": {
        "count": 8,
        "severity_breakdown": {
          "low": 8
        }
      },
      "git_history_secrets": {
        "count": 1,
        "severity_breakdown": {
          "medium": 1
        }
      },
      "config_credentials": {
        "count": 4,
        "severity_breakdown": {
          "high": 4
        }
      }
    },
    "recommendations": [
      "Address all HIGH severity findings immediately",
      "Review and fix MEDIUM severity findings",
      "Upgrade to stronger cryptographic algorithms",
      "Remove hardcoded credentials from configuration files",
      "Implement automated security scanning in CI/CD pipeline",
      "Conduct regular security code reviews",
      "Keep dependencies updated",
      "Use static analysis security testing (SAST) tools"
    ]
  }
}