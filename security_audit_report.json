{
  "timestamp": "2025-08-13T22:36:03.917216",
  "project_root": ".",
  "statistics": {
    "files_scanned": 606,
    "lines_scanned": 1487,
    "high_risk": 29,
    "medium_risk": 54,
    "low_risk": 9
  },
  "findings": [
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "fix_quality_issues.py",
      "line_number": 2,
      "details": "Test result: {'\u2705 PASSED' if success else '\u274c FAILED'}\")\n    sys.exit(0 if success else 1)\n'''\n    \n    with open(test_path, 'w') as f:\n        f.write(test_content)\n    \n    print(f\"  \u2705 Created {test_path}\")\n\n\ndef fix_security_issues():\n    \"\"\"Fix security issues by removing problematic patterns.\"\"\"\n    print(\"\ud83d\udd27 Fixing security issues...\")\n    \n    # This is a placeholder - in practice, we'd scan and fix specific issues\n    security_fixes = [\n        # Remove any eval() or exec() usage\n        # Sanitize shell commands\n        # Remove hardcoded credentials\n    ]\n    \n    print(\"  \u2705 Security review completed (manual fixes may be needed)\")\n\n\ndef create_production_deployment_config():\n    \"\"\"Create production deployment configuration.\"\"\"\n    print(\"\ud83d\udd27 Creating production deployment configuration...\")\n    \n    # Create deployment configuration\n    deploy_config_path = project_root / 'deployment_config.json'\n    \n    deploy_config = '''{\n  \"deployment\": {\n    \"environment\": \"production\",\n    \"scaling\": {\n      \"min_instances\": 2,\n      \"max_instances\": 10,\n      \"cpu_threshold\": 70,\n      \"memory_threshold\": 80\n    },\n    \"monitoring\": {\n      \"health_check_interval\": 30,\n      \"metrics_collection\": true,\n      \"logging_level\": \"INFO\"\n    },\n    \"security\": {\n      \"enable_ssl\": true,\n      \"authentication_required\": true,\n      \"rate_limiting\": true\n    },\n    \"quality_gates\": {\n      \"minimum_test_coverage\": 85,\n      \"performance_threshold\": \"5s\",\n      \"security_scan_required\": true\n    }\n  }\n}'''\n    \n    with open(deploy_config_path, 'w') as f:\n        f.write(deploy_config)\n    \n    print(f\"  \u2705 Created {deploy_config_path}\")\n\n\ndef main():\n    \"\"\"Main function to fix all quality issues.\"\"\"\n    print(\"\ud83d\ude80 TERRAGON SDLC - Quality Issue Resolution\")\n    print(\"=\" * 50)\n    \n    # Fix specific issues\n    fix_import_issues()\n    create_missing_utils()\n    fix_numpy_import_issue()\n    fix_security_issues()\n    create_production_deployment_config()\n    \n    print(\"=\" * 50)\n    print(\"\u2705 Quality issue resolution completed!\")\n    print(\""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "fix_quality_issues.py",
      "line_number": 2,
      "details": "Test result: {'\u2705 PASSED' if success else '\u274c FAILED'}\")\n    sys.exit(0 if success else 1)\n'''\n    \n    with open(test_path, 'w') as f:\n        f.write(test_content)\n    \n    print(f\"  \u2705 Created {test_path}\")\n\n\ndef fix_security_issues():\n    \"\"\"Fix security issues by removing problematic patterns.\"\"\"\n    print(\"\ud83d\udd27 Fixing security issues...\")\n    \n    # This is a placeholder - in practice, we'd scan and fix specific issues\n    security_fixes = [\n        # Remove any eval() or exec() usage\n        # Sanitize shell commands\n        # Remove hardcoded credentials\n    ]\n    \n    print(\"  \u2705 Security review completed (manual fixes may be needed)\")\n\n\ndef create_production_deployment_config():\n    \"\"\"Create production deployment configuration.\"\"\"\n    print(\"\ud83d\udd27 Creating production deployment configuration...\")\n    \n    # Create deployment configuration\n    deploy_config_path = project_root / 'deployment_config.json'\n    \n    deploy_config = '''{\n  \"deployment\": {\n    \"environment\": \"production\",\n    \"scaling\": {\n      \"min_instances\": 2,\n      \"max_instances\": 10,\n      \"cpu_threshold\": 70,\n      \"memory_threshold\": 80\n    },\n    \"monitoring\": {\n      \"health_check_interval\": 30,\n      \"metrics_collection\": true,\n      \"logging_level\": \"INFO\"\n    },\n    \"security\": {\n      \"enable_ssl\": true,\n      \"authentication_required\": true,\n      \"rate_limiting\": true\n    },\n    \"quality_gates\": {\n      \"minimum_test_coverage\": 85,\n      \"performance_threshold\": \"5s\",\n      \"security_scan_required\": true\n    }\n  }\n}'''\n    \n    with open(deploy_config_path, 'w') as f:\n        f.write(deploy_config)\n    \n    print(f\"  \u2705 Created {deploy_config_path}\")\n\n\ndef main():\n    \"\"\"Main function to fix all quality issues.\"\"\"\n    print(\"\ud83d\ude80 TERRAGON SDLC - Quality Issue Resolution\")\n    print(\"=\" * 50)\n    \n    # Fix specific issues\n    fix_import_issues()\n    create_missing_utils()\n    fix_numpy_import_issue()\n    fix_security_issues()\n    create_production_deployment_config()\n    \n    print(\"=\" * 50)\n    print(\"\u2705 Quality issue resolution completed!\")\n    print(\""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "run_quality_gates.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python3\n\"\"\"Quality gates execution script for analog PDE solver.\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Tuple\n\n# Add project root to Python path\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\nclass QualityGateRunner:\n    \"\"\"Quality gate execution and validation.\"\"\"\n    \n    def __init__(self):\n        self.project_root = Path(__file__).parent\n        self.results = {}\n        \n    def run_all_gates(self) -> Dict[str, Any]:\n        \"\"\"Run all quality gates and return results.\"\"\"\n        logger.info(\"\ud83d\ude80 Starting TERRAGON SDLC Quality Gates Execution\")\n        \n        # Gate 1: Code Structure Validation\n        self.results['code_structure'] = self._validate_code_structure()\n        \n        # Gate 2: Import and Syntax Validation\n        self.results['syntax_validation'] = self._validate_syntax()\n        \n        # Gate 3: Core Functionality Tests\n        self.results['functionality_tests'] = self._test_core_functionality()\n        \n        # Gate 4: Performance Benchmarks\n        self.results['performance_tests'] = self._run_performance_tests()\n        \n        # Gate 5: Security Audit\n        self.results['security_audit'] = self._run_security_audit()\n        \n        # Gate 6: Documentation Coverage\n        self.results['documentation_coverage'] = self._check_documentation()\n        \n        # Generate final report\n        self._generate_quality_report()\n        \n        return self.results\n    \n    def _validate_code_structure(self) -> Dict[str, Any]:\n        \"\"\"Validate project structure and organization.\"\"\"\n        logger.info(\"Gate 1: Code Structure Validation\")\n        \n        required_dirs = [\n            'analog_pde_solver',\n            'analog_pde_solver/core',\n            'analog_pde_solver/acceleration',\n            'analog_pde_solver/benchmarks',\n            'analog_pde_solver/monitoring',\n            'analog_pde_solver/optimization',\n            'analog_pde_solver/spice',\n            'analog_pde_solver/rtl',\n            'analog_pde_solver/validation',\n            'analog_pde_solver/visualization',\n            'tests',\n            'examples',\n            'docs'\n        ]\n        \n        required_files = [\n            'README.md',\n            'pyproject.toml',\n            'requirements.txt',\n            'analog_pde_solver/__init__.py'\n        ]\n        \n        structure_issues = []\n        \n        # Check directories\n        for dir_path in required_dirs:\n            full_path = self.project_root / dir_path\n            if not full_path.exists():\n                structure_issues.append(f\"Missing directory: {dir_path}\")\n        \n        # Check files\n        for file_path in required_files:\n            full_path = self.project_root / file_path\n            if not full_path.exists():\n                structure_issues.append(f\"Missing file: {file_path}\")\n        \n        return {\n            'passed': len(structure_issues) == 0,\n            'issues': structure_issues,\n            'score': max(0, (len(required_dirs + required_files) - len(structure_issues)) / len(required_dirs + required_files))\n        }\n    \n    def _validate_syntax(self) -> Dict[str, Any]:\n        \"\"\"Validate Python syntax across all modules.\"\"\"\n        logger.info(\"Gate 2: Syntax Validation\")\n        \n        syntax_errors = []\n        valid_files = 0\n        total_files = 0\n        \n        # Find all Python files\n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n                \n            total_files += 1\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    source = f.read()\n                \n                # Compile to check syntax\n                compile(source, str(py_file), 'exec')\n                valid_files += 1\n                \n            except SyntaxError as e:\n                syntax_errors.append(f\"{py_file}: {e}\")\n            except Exception as e:\n                syntax_errors.append(f\"{py_file}: {type(e).__name__}: {e}\")\n        \n        return {\n            'passed': len(syntax_errors) == 0,\n            'valid_files': valid_files,\n            'total_files': total_files,\n            'errors': syntax_errors,\n            'score': valid_files / max(1, total_files)\n        }\n    \n    def _test_core_functionality(self) -> Dict[str, Any]:\n        \"\"\"Test core functionality without external dependencies.\"\"\"\n        logger.info(\"Gate 3: Core Functionality Tests\")\n        \n        test_results = []\n        \n        # Test 1: Import core modules\n        try:\n            sys.path.insert(0, str(self.project_root))\n            \n            # Core imports\n            from analog_pde_solver.core import solver, equations, crossbar\n            from analog_pde_solver.spice import simulator\n            from analog_pde_solver.rtl import verilog_generator\n            from analog_pde_solver.validation import pde_validator, hardware_validator\n            from analog_pde_solver.optimization import performance_optimizer, advanced_algorithms, adaptive_scaling\n            from analog_pde_solver.acceleration import gpu_enhancements\n            from analog_pde_solver.visualization import pde_visualizer, hardware_monitor\n            \n            test_results.append({\"test\": \"core_imports\", \"passed\": True, \"message\": \"All core modules imported successfully\"})\n            \n        except Exception as e:\n            test_results.append({\"test\": \"core_imports\", \"passed\": False, \"message\": f\"Import error: {e}\"})\n        \n        # Test 2: Create basic solver instance\n        try:\n            from analog_pde_solver.core.solver import AnalogPDESolver\n            solver = AnalogPDESolver(crossbar_size=32)\n            test_results.append({\"test\": \"solver_creation\", \"passed\": True, \"message\": \"Solver created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"solver_creation\", \"passed\": False, \"message\": f\"Solver creation failed: {e}\"})\n        \n        # Test 3: Create PDE equation\n        try:\n            from analog_pde_solver.core.equations import PoissonEquation\n            pde = PoissonEquation(domain_size=(32,))\n            test_results.append({\"test\": \"pde_creation\", \"passed\": True, \"message\": \"PDE equation created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"pde_creation\", \"passed\": False, \"message\": f\"PDE creation failed: {e}\"})\n        \n        # Test 4: Validation tools\n        try:\n            from analog_pde_solver.validation.pde_validator import PDEValidator, ValidationLevel\n            validator = PDEValidator(ValidationLevel.BASIC)\n            test_results.append({\"test\": \"validator_creation\", \"passed\": True, \"message\": \"Validator created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"validator_creation\", \"passed\": False, \"message\": f\"Validator creation failed: {e}\"})\n        \n        # Test 5: RTL generation\n        try:\n            from analog_pde_solver.rtl.verilog_generator import VerilogGenerator, RTLConfig\n            rtl_gen = VerilogGenerator(RTLConfig())\n            verilog_code = rtl_gen.generate_top_module(32, 1, \"poisson\")\n            assert len(verilog_code) > 1000, \"Generated Verilog code too short\"\n            test_results.append({\"test\": \"rtl_generation\", \"passed\": True, \"message\": f\"RTL generated ({len(verilog_code)} chars)\"})\n        except Exception as e:\n            test_results.append({\"test\": \"rtl_generation\", \"passed\": False, \"message\": f\"RTL generation failed: {e}\"})\n        \n        passed_tests = sum(1 for t in test_results if t[\"passed\"])\n        \n        return {\n            'passed': passed_tests == len(test_results),\n            'test_results': test_results,\n            'score': passed_tests / len(test_results),\n            'tests_passed': passed_tests,\n            'tests_total': len(test_results)\n        }\n    \n    def _run_performance_tests(self) -> Dict[str, Any]:\n        \"\"\"Run performance benchmarks.\"\"\"\n        logger.info(\"Gate 4: Performance Tests\")\n        \n        try:\n            # Simple performance test without heavy dependencies\n            import time\n            import numpy as np\n            \n            from analog_pde_solver.core.solver import AnalogPDESolver\n            from analog_pde_solver.core.equations import PoissonEquation\n            \n            # Test solver performance\n            solver = AnalogPDESolver(crossbar_size=64)\n            pde = PoissonEquation(domain_size=(64,))\n            \n            start_time = time.perf_counter()\n            \n            # Simple solve test\n            solution = solver.solve(pde, iterations=50, convergence_threshold=1e-4)\n            \n            solve_time = time.perf_counter() - start_time\n            \n            # Performance criteria\n            max_solve_time = 5.0  # 5 seconds max\n            min_solution_norm = 1e-6\n            \n            performance_passed = (\n                solve_time < max_solve_time and\n                np.linalg.norm(solution) > min_solution_norm\n            )\n            \n            return {\n                'passed': performance_passed,\n                'solve_time': solve_time,\n                'solution_norm': float(np.linalg.norm(solution)),\n                'meets_timing': solve_time < max_solve_time,\n                'valid_solution': np.linalg.norm(solution) > min_solution_norm,\n                'score': 1.0 if performance_passed else 0.5\n            }\n            \n        except Exception as e:\n            return {\n                'passed': False,\n                'error': str(e),\n                'score': 0.0\n            }\n    \n    def _run_security_audit(self) -> Dict[str, Any]:\n        \"\"\"Run security audit checks.\"\"\"\n        logger.info(\"Gate 5: Security Audit\")\n        \n        security_issues = []\n        \n        # Check for common security issues in Python files\n        security_patterns = [\n            (r'eval\\s*\\(', 'Use of eval() function'),\n            (r'exec\\s*\\(', 'Use of exec() function'),\n            (r'subprocess\\.call\\s*\\(.*shell\\s*=\\s*True', 'Unsafe shell execution'),\n            (r'pickle\\.loads\\s*\\(', 'Unsafe pickle deserialization'),\n            (r'input\\s*\\(.*\\)', 'Use of raw input() - potential injection'),\n            (r'os\\.system\\s*\\(', 'Use of os.system() - potential injection'),\n        ]\n        \n        import re\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                for pattern, description in security_patterns:\n                    if re.search(pattern, content, re.IGNORECASE):\n                        security_issues.append(f\"{py_file}: {description}\")\n                        \n            except Exception as e:\n                security_issues.append(f\"{py_file}: Error reading file - {e}\")\n        \n        # Check for hardcoded secrets (simplified)\n        secret_patterns = [\n            (r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded password'),\n            (r'secret\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded secret'),\n            (r'api_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded API key'),\n        ]\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                for pattern, description in secret_patterns:\n                    if re.search(pattern, content, re.IGNORECASE):\n                        # Don't flag test files or examples\n                        if 'test' not in str(py_file).lower() and 'example' not in str(py_file).lower():\n                            security_issues.append(f\"{py_file}: {description}\")\n                        \n            except Exception:\n                pass\n        \n        return {\n            'passed': len(security_issues) == 0,\n            'issues': security_issues,\n            'score': 1.0 if len(security_issues) == 0 else max(0, 1.0 - len(security_issues) * 0.1)\n        }\n    \n    def _check_documentation(self) -> Dict[str, Any]:\n        \"\"\"Check documentation coverage.\"\"\"\n        logger.info(\"Gate 6: Documentation Coverage\")\n        \n        doc_files_found = []\n        missing_docs = []\n        \n        # Required documentation files\n        required_docs = [\n            'README.md',\n            'docs/index.rst',\n            'docs/tutorials/01_getting_started.md',\n            'CONTRIBUTING.md',\n            'CHANGELOG.md',\n        ]\n        \n        for doc_file in required_docs:\n            doc_path = self.project_root / doc_file\n            if doc_path.exists():\n                doc_files_found.append(doc_file)\n            else:\n                missing_docs.append(doc_file)\n        \n        # Check Python docstring coverage (simplified)\n        python_files_with_docs = 0\n        total_python_files = 0\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file) or '__init__.py' in str(py_file):\n                continue\n            \n            total_python_files += 1\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                # Simple check for docstrings\n                if '\"\"\"' in content or \"'''\" in content:\n                    python_files_with_docs += 1\n                    \n            except Exception:\n                pass\n        \n        doc_coverage = python_files_with_docs / max(1, total_python_files)\n        \n        return {\n            'passed': len(missing_docs) == 0 and doc_coverage >= 0.7,\n            'doc_files_found': doc_files_found,\n            'missing_docs': missing_docs,\n            'python_doc_coverage': doc_coverage,\n            'python_files_with_docs': python_files_with_docs,\n            'total_python_files': total_python_files,\n            'score': (len(doc_files_found) / len(required_docs) + doc_coverage) / 2\n        }\n    \n    def _generate_quality_report(self):\n        \"\"\"Generate comprehensive quality report.\"\"\"\n        logger.info(\"Generating Quality Gates Report\")\n        \n        total_score = sum(result.get('score', 0) for result in self.results.values())\n        average_score = total_score / len(self.results)\n        \n        gates_passed = sum(1 for result in self.results.values() if result.get('passed', False))\n        total_gates = len(self.results)\n        \n        report_lines = [\n            \"=\" * 80,\n            \"\ud83d\ude80 TERRAGON SDLC - QUALITY GATES EXECUTION REPORT\",\n            \"=\" * 80,\n            f\"Overall Status: {'\u2705 PASSED' if gates_passed == total_gates else '\u274c FAILED'}\",\n            f\"Gates Passed: {gates_passed}/{total_gates}\",\n            f\"Average Score: {average_score:.2%}\",\n            f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n            \"\",\n        ]\n        \n        # Individual gate results\n        gate_names = {\n            'code_structure': 'Gate 1: Code Structure Validation',\n            'syntax_validation': 'Gate 2: Syntax Validation', \n            'functionality_tests': 'Gate 3: Core Functionality Tests',\n            'performance_tests': 'Gate 4: Performance Tests',\n            'security_audit': 'Gate 5: Security Audit',\n            'documentation_coverage': 'Gate 6: Documentation Coverage'\n        }\n        \n        for gate_key, gate_name in gate_names.items():\n            result = self.results.get(gate_key, {})\n            status = \"\u2705 PASS\" if result.get('passed', False) else \"\u274c FAIL\"\n            score = result.get('score', 0)\n            \n            report_lines.append(f\"{gate_name}\")\n            report_lines.append(f\"  Status: {status}\")\n            report_lines.append(f\"  Score: {score:.2%}\")\n            \n            # Add specific details for each gate\n            if gate_key == 'functionality_tests' and 'test_results' in result:\n                passed = result.get('tests_passed', 0)\n                total = result.get('tests_total', 0)\n                report_lines.append(f\"  Tests: {passed}/{total} passed\")\n            \n            elif gate_key == 'performance_tests' and 'solve_time' in result:\n                solve_time = result.get('solve_time', 0)\n                report_lines.append(f\"  Solve Time: {solve_time:.3f}s\")\n            \n            elif gate_key == 'security_audit' and 'issues' in result:\n                issues = len(result.get('issues', []))\n                report_lines.append(f\"  Security Issues: {issues}\")\n            \n            elif gate_key == 'documentation_coverage':\n                doc_coverage = result.get('python_doc_coverage', 0)\n                report_lines.append(f\"  Doc Coverage: {doc_coverage:.1%}\")\n            \n            report_lines.append(\"\")\n        \n        # Summary and recommendations\n        report_lines.extend([\n            \"\ud83d\udccb RECOMMENDATIONS:\",\n        ])\n        \n        if gates_passed < total_gates:\n            report_lines.append(\"  \u26a0\ufe0f  Some quality gates failed - review issues above\")\n        \n        if average_score < 0.85:\n            report_lines.append(\"  \ud83d\udcc8 Consider improving overall quality score\")\n        \n        if self.results.get('security_audit', {}).get('issues'):\n            report_lines.append(\"  \ud83d\udd12 Address security issues before production deployment\")\n        \n        if self.results.get('performance_tests', {}).get('solve_time', 0) > 3.0:\n            report_lines.append(\"  \u26a1 Consider performance optimizations\")\n        \n        report_lines.extend([\n            \"\",\n            \"\ud83c\udf89 TERRAGON SDLC AUTONOMOUS EXECUTION COMPLETE\",\n            \"\",\n            \"\u2705 Generation 1: MAKE IT WORK - Basic functionality implemented\",\n            \"\u2705 Generation 2: MAKE IT ROBUST - Error handling and validation added\", \n            \"\u2705 Generation 3: MAKE IT SCALE - Performance optimization completed\",\n            \"\u2705 Quality Gates: Comprehensive testing and validation executed\",\n            \"\",\n            \"\ud83d\ude80 System is ready for production deployment!\",\n            \"\",\n            \"=\" * 80,\n            \"Report generated by Terragon Labs Autonomous SDLC System\",\n            \"=\" * 80\n        ])\n        \n        # Write report to file\n        report_content = \""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "run_quality_gates.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python3\n\"\"\"Quality gates execution script for analog PDE solver.\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Tuple\n\n# Add project root to Python path\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\nclass QualityGateRunner:\n    \"\"\"Quality gate execution and validation.\"\"\"\n    \n    def __init__(self):\n        self.project_root = Path(__file__).parent\n        self.results = {}\n        \n    def run_all_gates(self) -> Dict[str, Any]:\n        \"\"\"Run all quality gates and return results.\"\"\"\n        logger.info(\"\ud83d\ude80 Starting TERRAGON SDLC Quality Gates Execution\")\n        \n        # Gate 1: Code Structure Validation\n        self.results['code_structure'] = self._validate_code_structure()\n        \n        # Gate 2: Import and Syntax Validation\n        self.results['syntax_validation'] = self._validate_syntax()\n        \n        # Gate 3: Core Functionality Tests\n        self.results['functionality_tests'] = self._test_core_functionality()\n        \n        # Gate 4: Performance Benchmarks\n        self.results['performance_tests'] = self._run_performance_tests()\n        \n        # Gate 5: Security Audit\n        self.results['security_audit'] = self._run_security_audit()\n        \n        # Gate 6: Documentation Coverage\n        self.results['documentation_coverage'] = self._check_documentation()\n        \n        # Generate final report\n        self._generate_quality_report()\n        \n        return self.results\n    \n    def _validate_code_structure(self) -> Dict[str, Any]:\n        \"\"\"Validate project structure and organization.\"\"\"\n        logger.info(\"Gate 1: Code Structure Validation\")\n        \n        required_dirs = [\n            'analog_pde_solver',\n            'analog_pde_solver/core',\n            'analog_pde_solver/acceleration',\n            'analog_pde_solver/benchmarks',\n            'analog_pde_solver/monitoring',\n            'analog_pde_solver/optimization',\n            'analog_pde_solver/spice',\n            'analog_pde_solver/rtl',\n            'analog_pde_solver/validation',\n            'analog_pde_solver/visualization',\n            'tests',\n            'examples',\n            'docs'\n        ]\n        \n        required_files = [\n            'README.md',\n            'pyproject.toml',\n            'requirements.txt',\n            'analog_pde_solver/__init__.py'\n        ]\n        \n        structure_issues = []\n        \n        # Check directories\n        for dir_path in required_dirs:\n            full_path = self.project_root / dir_path\n            if not full_path.exists():\n                structure_issues.append(f\"Missing directory: {dir_path}\")\n        \n        # Check files\n        for file_path in required_files:\n            full_path = self.project_root / file_path\n            if not full_path.exists():\n                structure_issues.append(f\"Missing file: {file_path}\")\n        \n        return {\n            'passed': len(structure_issues) == 0,\n            'issues': structure_issues,\n            'score': max(0, (len(required_dirs + required_files) - len(structure_issues)) / len(required_dirs + required_files))\n        }\n    \n    def _validate_syntax(self) -> Dict[str, Any]:\n        \"\"\"Validate Python syntax across all modules.\"\"\"\n        logger.info(\"Gate 2: Syntax Validation\")\n        \n        syntax_errors = []\n        valid_files = 0\n        total_files = 0\n        \n        # Find all Python files\n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n                \n            total_files += 1\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    source = f.read()\n                \n                # Compile to check syntax\n                compile(source, str(py_file), 'exec')\n                valid_files += 1\n                \n            except SyntaxError as e:\n                syntax_errors.append(f\"{py_file}: {e}\")\n            except Exception as e:\n                syntax_errors.append(f\"{py_file}: {type(e).__name__}: {e}\")\n        \n        return {\n            'passed': len(syntax_errors) == 0,\n            'valid_files': valid_files,\n            'total_files': total_files,\n            'errors': syntax_errors,\n            'score': valid_files / max(1, total_files)\n        }\n    \n    def _test_core_functionality(self) -> Dict[str, Any]:\n        \"\"\"Test core functionality without external dependencies.\"\"\"\n        logger.info(\"Gate 3: Core Functionality Tests\")\n        \n        test_results = []\n        \n        # Test 1: Import core modules\n        try:\n            sys.path.insert(0, str(self.project_root))\n            \n            # Core imports\n            from analog_pde_solver.core import solver, equations, crossbar\n            from analog_pde_solver.spice import simulator\n            from analog_pde_solver.rtl import verilog_generator\n            from analog_pde_solver.validation import pde_validator, hardware_validator\n            from analog_pde_solver.optimization import performance_optimizer, advanced_algorithms, adaptive_scaling\n            from analog_pde_solver.acceleration import gpu_enhancements\n            from analog_pde_solver.visualization import pde_visualizer, hardware_monitor\n            \n            test_results.append({\"test\": \"core_imports\", \"passed\": True, \"message\": \"All core modules imported successfully\"})\n            \n        except Exception as e:\n            test_results.append({\"test\": \"core_imports\", \"passed\": False, \"message\": f\"Import error: {e}\"})\n        \n        # Test 2: Create basic solver instance\n        try:\n            from analog_pde_solver.core.solver import AnalogPDESolver\n            solver = AnalogPDESolver(crossbar_size=32)\n            test_results.append({\"test\": \"solver_creation\", \"passed\": True, \"message\": \"Solver created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"solver_creation\", \"passed\": False, \"message\": f\"Solver creation failed: {e}\"})\n        \n        # Test 3: Create PDE equation\n        try:\n            from analog_pde_solver.core.equations import PoissonEquation\n            pde = PoissonEquation(domain_size=(32,))\n            test_results.append({\"test\": \"pde_creation\", \"passed\": True, \"message\": \"PDE equation created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"pde_creation\", \"passed\": False, \"message\": f\"PDE creation failed: {e}\"})\n        \n        # Test 4: Validation tools\n        try:\n            from analog_pde_solver.validation.pde_validator import PDEValidator, ValidationLevel\n            validator = PDEValidator(ValidationLevel.BASIC)\n            test_results.append({\"test\": \"validator_creation\", \"passed\": True, \"message\": \"Validator created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"validator_creation\", \"passed\": False, \"message\": f\"Validator creation failed: {e}\"})\n        \n        # Test 5: RTL generation\n        try:\n            from analog_pde_solver.rtl.verilog_generator import VerilogGenerator, RTLConfig\n            rtl_gen = VerilogGenerator(RTLConfig())\n            verilog_code = rtl_gen.generate_top_module(32, 1, \"poisson\")\n            assert len(verilog_code) > 1000, \"Generated Verilog code too short\"\n            test_results.append({\"test\": \"rtl_generation\", \"passed\": True, \"message\": f\"RTL generated ({len(verilog_code)} chars)\"})\n        except Exception as e:\n            test_results.append({\"test\": \"rtl_generation\", \"passed\": False, \"message\": f\"RTL generation failed: {e}\"})\n        \n        passed_tests = sum(1 for t in test_results if t[\"passed\"])\n        \n        return {\n            'passed': passed_tests == len(test_results),\n            'test_results': test_results,\n            'score': passed_tests / len(test_results),\n            'tests_passed': passed_tests,\n            'tests_total': len(test_results)\n        }\n    \n    def _run_performance_tests(self) -> Dict[str, Any]:\n        \"\"\"Run performance benchmarks.\"\"\"\n        logger.info(\"Gate 4: Performance Tests\")\n        \n        try:\n            # Simple performance test without heavy dependencies\n            import time\n            import numpy as np\n            \n            from analog_pde_solver.core.solver import AnalogPDESolver\n            from analog_pde_solver.core.equations import PoissonEquation\n            \n            # Test solver performance\n            solver = AnalogPDESolver(crossbar_size=64)\n            pde = PoissonEquation(domain_size=(64,))\n            \n            start_time = time.perf_counter()\n            \n            # Simple solve test\n            solution = solver.solve(pde, iterations=50, convergence_threshold=1e-4)\n            \n            solve_time = time.perf_counter() - start_time\n            \n            # Performance criteria\n            max_solve_time = 5.0  # 5 seconds max\n            min_solution_norm = 1e-6\n            \n            performance_passed = (\n                solve_time < max_solve_time and\n                np.linalg.norm(solution) > min_solution_norm\n            )\n            \n            return {\n                'passed': performance_passed,\n                'solve_time': solve_time,\n                'solution_norm': float(np.linalg.norm(solution)),\n                'meets_timing': solve_time < max_solve_time,\n                'valid_solution': np.linalg.norm(solution) > min_solution_norm,\n                'score': 1.0 if performance_passed else 0.5\n            }\n            \n        except Exception as e:\n            return {\n                'passed': False,\n                'error': str(e),\n                'score': 0.0\n            }\n    \n    def _run_security_audit(self) -> Dict[str, Any]:\n        \"\"\"Run security audit checks.\"\"\"\n        logger.info(\"Gate 5: Security Audit\")\n        \n        security_issues = []\n        \n        # Check for common security issues in Python files\n        security_patterns = [\n            (r'eval\\s*\\(', 'Use of eval() function'),\n            (r'exec\\s*\\(', 'Use of exec() function'),\n            (r'subprocess\\.call\\s*\\(.*shell\\s*=\\s*True', 'Unsafe shell execution'),\n            (r'pickle\\.loads\\s*\\(', 'Unsafe pickle deserialization'),\n            (r'input\\s*\\(.*\\)', 'Use of raw input() - potential injection'),\n            (r'os\\.system\\s*\\(', 'Use of os.system() - potential injection'),\n        ]\n        \n        import re\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                for pattern, description in security_patterns:\n                    if re.search(pattern, content, re.IGNORECASE):\n                        security_issues.append(f\"{py_file}: {description}\")\n                        \n            except Exception as e:\n                security_issues.append(f\"{py_file}: Error reading file - {e}\")\n        \n        # Check for hardcoded secrets (simplified)\n        secret_patterns = [\n            (r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded password'),\n            (r'secret\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded secret'),\n            (r'api_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded API key'),\n        ]\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                for pattern, description in secret_patterns:\n                    if re.search(pattern, content, re.IGNORECASE):\n                        # Don't flag test files or examples\n                        if 'test' not in str(py_file).lower() and 'example' not in str(py_file).lower():\n                            security_issues.append(f\"{py_file}: {description}\")\n                        \n            except Exception:\n                pass\n        \n        return {\n            'passed': len(security_issues) == 0,\n            'issues': security_issues,\n            'score': 1.0 if len(security_issues) == 0 else max(0, 1.0 - len(security_issues) * 0.1)\n        }\n    \n    def _check_documentation(self) -> Dict[str, Any]:\n        \"\"\"Check documentation coverage.\"\"\"\n        logger.info(\"Gate 6: Documentation Coverage\")\n        \n        doc_files_found = []\n        missing_docs = []\n        \n        # Required documentation files\n        required_docs = [\n            'README.md',\n            'docs/index.rst',\n            'docs/tutorials/01_getting_started.md',\n            'CONTRIBUTING.md',\n            'CHANGELOG.md',\n        ]\n        \n        for doc_file in required_docs:\n            doc_path = self.project_root / doc_file\n            if doc_path.exists():\n                doc_files_found.append(doc_file)\n            else:\n                missing_docs.append(doc_file)\n        \n        # Check Python docstring coverage (simplified)\n        python_files_with_docs = 0\n        total_python_files = 0\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file) or '__init__.py' in str(py_file):\n                continue\n            \n            total_python_files += 1\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                # Simple check for docstrings\n                if '\"\"\"' in content or \"'''\" in content:\n                    python_files_with_docs += 1\n                    \n            except Exception:\n                pass\n        \n        doc_coverage = python_files_with_docs / max(1, total_python_files)\n        \n        return {\n            'passed': len(missing_docs) == 0 and doc_coverage >= 0.7,\n            'doc_files_found': doc_files_found,\n            'missing_docs': missing_docs,\n            'python_doc_coverage': doc_coverage,\n            'python_files_with_docs': python_files_with_docs,\n            'total_python_files': total_python_files,\n            'score': (len(doc_files_found) / len(required_docs) + doc_coverage) / 2\n        }\n    \n    def _generate_quality_report(self):\n        \"\"\"Generate comprehensive quality report.\"\"\"\n        logger.info(\"Generating Quality Gates Report\")\n        \n        total_score = sum(result.get('score', 0) for result in self.results.values())\n        average_score = total_score / len(self.results)\n        \n        gates_passed = sum(1 for result in self.results.values() if result.get('passed', False))\n        total_gates = len(self.results)\n        \n        report_lines = [\n            \"=\" * 80,\n            \"\ud83d\ude80 TERRAGON SDLC - QUALITY GATES EXECUTION REPORT\",\n            \"=\" * 80,\n            f\"Overall Status: {'\u2705 PASSED' if gates_passed == total_gates else '\u274c FAILED'}\",\n            f\"Gates Passed: {gates_passed}/{total_gates}\",\n            f\"Average Score: {average_score:.2%}\",\n            f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n            \"\",\n        ]\n        \n        # Individual gate results\n        gate_names = {\n            'code_structure': 'Gate 1: Code Structure Validation',\n            'syntax_validation': 'Gate 2: Syntax Validation', \n            'functionality_tests': 'Gate 3: Core Functionality Tests',\n            'performance_tests': 'Gate 4: Performance Tests',\n            'security_audit': 'Gate 5: Security Audit',\n            'documentation_coverage': 'Gate 6: Documentation Coverage'\n        }\n        \n        for gate_key, gate_name in gate_names.items():\n            result = self.results.get(gate_key, {})\n            status = \"\u2705 PASS\" if result.get('passed', False) else \"\u274c FAIL\"\n            score = result.get('score', 0)\n            \n            report_lines.append(f\"{gate_name}\")\n            report_lines.append(f\"  Status: {status}\")\n            report_lines.append(f\"  Score: {score:.2%}\")\n            \n            # Add specific details for each gate\n            if gate_key == 'functionality_tests' and 'test_results' in result:\n                passed = result.get('tests_passed', 0)\n                total = result.get('tests_total', 0)\n                report_lines.append(f\"  Tests: {passed}/{total} passed\")\n            \n            elif gate_key == 'performance_tests' and 'solve_time' in result:\n                solve_time = result.get('solve_time', 0)\n                report_lines.append(f\"  Solve Time: {solve_time:.3f}s\")\n            \n            elif gate_key == 'security_audit' and 'issues' in result:\n                issues = len(result.get('issues', []))\n                report_lines.append(f\"  Security Issues: {issues}\")\n            \n            elif gate_key == 'documentation_coverage':\n                doc_coverage = result.get('python_doc_coverage', 0)\n                report_lines.append(f\"  Doc Coverage: {doc_coverage:.1%}\")\n            \n            report_lines.append(\"\")\n        \n        # Summary and recommendations\n        report_lines.extend([\n            \"\ud83d\udccb RECOMMENDATIONS:\",\n        ])\n        \n        if gates_passed < total_gates:\n            report_lines.append(\"  \u26a0\ufe0f  Some quality gates failed - review issues above\")\n        \n        if average_score < 0.85:\n            report_lines.append(\"  \ud83d\udcc8 Consider improving overall quality score\")\n        \n        if self.results.get('security_audit', {}).get('issues'):\n            report_lines.append(\"  \ud83d\udd12 Address security issues before production deployment\")\n        \n        if self.results.get('performance_tests', {}).get('solve_time', 0) > 3.0:\n            report_lines.append(\"  \u26a1 Consider performance optimizations\")\n        \n        report_lines.extend([\n            \"\",\n            \"\ud83c\udf89 TERRAGON SDLC AUTONOMOUS EXECUTION COMPLETE\",\n            \"\",\n            \"\u2705 Generation 1: MAKE IT WORK - Basic functionality implemented\",\n            \"\u2705 Generation 2: MAKE IT ROBUST - Error handling and validation added\", \n            \"\u2705 Generation 3: MAKE IT SCALE - Performance optimization completed\",\n            \"\u2705 Quality Gates: Comprehensive testing and validation executed\",\n            \"\",\n            \"\ud83d\ude80 System is ready for production deployment!\",\n            \"\",\n            \"=\" * 80,\n            \"Report generated by Terragon Labs Autonomous SDLC System\",\n            \"=\" * 80\n        ])\n        \n        # Write report to file\n        report_content = \""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "run_quality_gates.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python3\n\"\"\"Quality gates execution script for analog PDE solver.\"\"\"\n\nimport os\nimport sys\nimport subprocess\nimport time\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Tuple\n\n# Add project root to Python path\nproject_root = Path(__file__).parent\nsys.path.insert(0, str(project_root))\n\n# Set up logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\nclass QualityGateRunner:\n    \"\"\"Quality gate execution and validation.\"\"\"\n    \n    def __init__(self):\n        self.project_root = Path(__file__).parent\n        self.results = {}\n        \n    def run_all_gates(self) -> Dict[str, Any]:\n        \"\"\"Run all quality gates and return results.\"\"\"\n        logger.info(\"\ud83d\ude80 Starting TERRAGON SDLC Quality Gates Execution\")\n        \n        # Gate 1: Code Structure Validation\n        self.results['code_structure'] = self._validate_code_structure()\n        \n        # Gate 2: Import and Syntax Validation\n        self.results['syntax_validation'] = self._validate_syntax()\n        \n        # Gate 3: Core Functionality Tests\n        self.results['functionality_tests'] = self._test_core_functionality()\n        \n        # Gate 4: Performance Benchmarks\n        self.results['performance_tests'] = self._run_performance_tests()\n        \n        # Gate 5: Security Audit\n        self.results['security_audit'] = self._run_security_audit()\n        \n        # Gate 6: Documentation Coverage\n        self.results['documentation_coverage'] = self._check_documentation()\n        \n        # Generate final report\n        self._generate_quality_report()\n        \n        return self.results\n    \n    def _validate_code_structure(self) -> Dict[str, Any]:\n        \"\"\"Validate project structure and organization.\"\"\"\n        logger.info(\"Gate 1: Code Structure Validation\")\n        \n        required_dirs = [\n            'analog_pde_solver',\n            'analog_pde_solver/core',\n            'analog_pde_solver/acceleration',\n            'analog_pde_solver/benchmarks',\n            'analog_pde_solver/monitoring',\n            'analog_pde_solver/optimization',\n            'analog_pde_solver/spice',\n            'analog_pde_solver/rtl',\n            'analog_pde_solver/validation',\n            'analog_pde_solver/visualization',\n            'tests',\n            'examples',\n            'docs'\n        ]\n        \n        required_files = [\n            'README.md',\n            'pyproject.toml',\n            'requirements.txt',\n            'analog_pde_solver/__init__.py'\n        ]\n        \n        structure_issues = []\n        \n        # Check directories\n        for dir_path in required_dirs:\n            full_path = self.project_root / dir_path\n            if not full_path.exists():\n                structure_issues.append(f\"Missing directory: {dir_path}\")\n        \n        # Check files\n        for file_path in required_files:\n            full_path = self.project_root / file_path\n            if not full_path.exists():\n                structure_issues.append(f\"Missing file: {file_path}\")\n        \n        return {\n            'passed': len(structure_issues) == 0,\n            'issues': structure_issues,\n            'score': max(0, (len(required_dirs + required_files) - len(structure_issues)) / len(required_dirs + required_files))\n        }\n    \n    def _validate_syntax(self) -> Dict[str, Any]:\n        \"\"\"Validate Python syntax across all modules.\"\"\"\n        logger.info(\"Gate 2: Syntax Validation\")\n        \n        syntax_errors = []\n        valid_files = 0\n        total_files = 0\n        \n        # Find all Python files\n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n                \n            total_files += 1\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    source = f.read()\n                \n                # Compile to check syntax\n                compile(source, str(py_file), 'exec')\n                valid_files += 1\n                \n            except SyntaxError as e:\n                syntax_errors.append(f\"{py_file}: {e}\")\n            except Exception as e:\n                syntax_errors.append(f\"{py_file}: {type(e).__name__}: {e}\")\n        \n        return {\n            'passed': len(syntax_errors) == 0,\n            'valid_files': valid_files,\n            'total_files': total_files,\n            'errors': syntax_errors,\n            'score': valid_files / max(1, total_files)\n        }\n    \n    def _test_core_functionality(self) -> Dict[str, Any]:\n        \"\"\"Test core functionality without external dependencies.\"\"\"\n        logger.info(\"Gate 3: Core Functionality Tests\")\n        \n        test_results = []\n        \n        # Test 1: Import core modules\n        try:\n            sys.path.insert(0, str(self.project_root))\n            \n            # Core imports\n            from analog_pde_solver.core import solver, equations, crossbar\n            from analog_pde_solver.spice import simulator\n            from analog_pde_solver.rtl import verilog_generator\n            from analog_pde_solver.validation import pde_validator, hardware_validator\n            from analog_pde_solver.optimization import performance_optimizer, advanced_algorithms, adaptive_scaling\n            from analog_pde_solver.acceleration import gpu_enhancements\n            from analog_pde_solver.visualization import pde_visualizer, hardware_monitor\n            \n            test_results.append({\"test\": \"core_imports\", \"passed\": True, \"message\": \"All core modules imported successfully\"})\n            \n        except Exception as e:\n            test_results.append({\"test\": \"core_imports\", \"passed\": False, \"message\": f\"Import error: {e}\"})\n        \n        # Test 2: Create basic solver instance\n        try:\n            from analog_pde_solver.core.solver import AnalogPDESolver\n            solver = AnalogPDESolver(crossbar_size=32)\n            test_results.append({\"test\": \"solver_creation\", \"passed\": True, \"message\": \"Solver created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"solver_creation\", \"passed\": False, \"message\": f\"Solver creation failed: {e}\"})\n        \n        # Test 3: Create PDE equation\n        try:\n            from analog_pde_solver.core.equations import PoissonEquation\n            pde = PoissonEquation(domain_size=(32,))\n            test_results.append({\"test\": \"pde_creation\", \"passed\": True, \"message\": \"PDE equation created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"pde_creation\", \"passed\": False, \"message\": f\"PDE creation failed: {e}\"})\n        \n        # Test 4: Validation tools\n        try:\n            from analog_pde_solver.validation.pde_validator import PDEValidator, ValidationLevel\n            validator = PDEValidator(ValidationLevel.BASIC)\n            test_results.append({\"test\": \"validator_creation\", \"passed\": True, \"message\": \"Validator created successfully\"})\n        except Exception as e:\n            test_results.append({\"test\": \"validator_creation\", \"passed\": False, \"message\": f\"Validator creation failed: {e}\"})\n        \n        # Test 5: RTL generation\n        try:\n            from analog_pde_solver.rtl.verilog_generator import VerilogGenerator, RTLConfig\n            rtl_gen = VerilogGenerator(RTLConfig())\n            verilog_code = rtl_gen.generate_top_module(32, 1, \"poisson\")\n            assert len(verilog_code) > 1000, \"Generated Verilog code too short\"\n            test_results.append({\"test\": \"rtl_generation\", \"passed\": True, \"message\": f\"RTL generated ({len(verilog_code)} chars)\"})\n        except Exception as e:\n            test_results.append({\"test\": \"rtl_generation\", \"passed\": False, \"message\": f\"RTL generation failed: {e}\"})\n        \n        passed_tests = sum(1 for t in test_results if t[\"passed\"])\n        \n        return {\n            'passed': passed_tests == len(test_results),\n            'test_results': test_results,\n            'score': passed_tests / len(test_results),\n            'tests_passed': passed_tests,\n            'tests_total': len(test_results)\n        }\n    \n    def _run_performance_tests(self) -> Dict[str, Any]:\n        \"\"\"Run performance benchmarks.\"\"\"\n        logger.info(\"Gate 4: Performance Tests\")\n        \n        try:\n            # Simple performance test without heavy dependencies\n            import time\n            import numpy as np\n            \n            from analog_pde_solver.core.solver import AnalogPDESolver\n            from analog_pde_solver.core.equations import PoissonEquation\n            \n            # Test solver performance\n            solver = AnalogPDESolver(crossbar_size=64)\n            pde = PoissonEquation(domain_size=(64,))\n            \n            start_time = time.perf_counter()\n            \n            # Simple solve test\n            solution = solver.solve(pde, iterations=50, convergence_threshold=1e-4)\n            \n            solve_time = time.perf_counter() - start_time\n            \n            # Performance criteria\n            max_solve_time = 5.0  # 5 seconds max\n            min_solution_norm = 1e-6\n            \n            performance_passed = (\n                solve_time < max_solve_time and\n                np.linalg.norm(solution) > min_solution_norm\n            )\n            \n            return {\n                'passed': performance_passed,\n                'solve_time': solve_time,\n                'solution_norm': float(np.linalg.norm(solution)),\n                'meets_timing': solve_time < max_solve_time,\n                'valid_solution': np.linalg.norm(solution) > min_solution_norm,\n                'score': 1.0 if performance_passed else 0.5\n            }\n            \n        except Exception as e:\n            return {\n                'passed': False,\n                'error': str(e),\n                'score': 0.0\n            }\n    \n    def _run_security_audit(self) -> Dict[str, Any]:\n        \"\"\"Run security audit checks.\"\"\"\n        logger.info(\"Gate 5: Security Audit\")\n        \n        security_issues = []\n        \n        # Check for common security issues in Python files\n        security_patterns = [\n            (r'eval\\s*\\(', 'Use of eval() function'),\n            (r'exec\\s*\\(', 'Use of exec() function'),\n            (r'subprocess\\.call\\s*\\(.*shell\\s*=\\s*True', 'Unsafe shell execution'),\n            (r'pickle\\.loads\\s*\\(', 'Unsafe pickle deserialization'),\n            (r'input\\s*\\(.*\\)', 'Use of raw input() - potential injection'),\n            (r'os\\.system\\s*\\(', 'Use of os.system() - potential injection'),\n        ]\n        \n        import re\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                for pattern, description in security_patterns:\n                    if re.search(pattern, content, re.IGNORECASE):\n                        security_issues.append(f\"{py_file}: {description}\")\n                        \n            except Exception as e:\n                security_issues.append(f\"{py_file}: Error reading file - {e}\")\n        \n        # Check for hardcoded secrets (simplified)\n        secret_patterns = [\n            (r'password\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded password'),\n            (r'secret\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded secret'),\n            (r'api_key\\s*=\\s*[\"\\'][^\"\\']+[\"\\']', 'Hardcoded API key'),\n        ]\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file):\n                continue\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                for pattern, description in secret_patterns:\n                    if re.search(pattern, content, re.IGNORECASE):\n                        # Don't flag test files or examples\n                        if 'test' not in str(py_file).lower() and 'example' not in str(py_file).lower():\n                            security_issues.append(f\"{py_file}: {description}\")\n                        \n            except Exception:\n                pass\n        \n        return {\n            'passed': len(security_issues) == 0,\n            'issues': security_issues,\n            'score': 1.0 if len(security_issues) == 0 else max(0, 1.0 - len(security_issues) * 0.1)\n        }\n    \n    def _check_documentation(self) -> Dict[str, Any]:\n        \"\"\"Check documentation coverage.\"\"\"\n        logger.info(\"Gate 6: Documentation Coverage\")\n        \n        doc_files_found = []\n        missing_docs = []\n        \n        # Required documentation files\n        required_docs = [\n            'README.md',\n            'docs/index.rst',\n            'docs/tutorials/01_getting_started.md',\n            'CONTRIBUTING.md',\n            'CHANGELOG.md',\n        ]\n        \n        for doc_file in required_docs:\n            doc_path = self.project_root / doc_file\n            if doc_path.exists():\n                doc_files_found.append(doc_file)\n            else:\n                missing_docs.append(doc_file)\n        \n        # Check Python docstring coverage (simplified)\n        python_files_with_docs = 0\n        total_python_files = 0\n        \n        for py_file in self.project_root.rglob(\"*.py\"):\n            if 'venv' in str(py_file) or '__pycache__' in str(py_file) or '__init__.py' in str(py_file):\n                continue\n            \n            total_python_files += 1\n            \n            try:\n                with open(py_file, 'r', encoding='utf-8') as f:\n                    content = f.read()\n                \n                # Simple check for docstrings\n                if '\"\"\"' in content or \"'''\" in content:\n                    python_files_with_docs += 1\n                    \n            except Exception:\n                pass\n        \n        doc_coverage = python_files_with_docs / max(1, total_python_files)\n        \n        return {\n            'passed': len(missing_docs) == 0 and doc_coverage >= 0.7,\n            'doc_files_found': doc_files_found,\n            'missing_docs': missing_docs,\n            'python_doc_coverage': doc_coverage,\n            'python_files_with_docs': python_files_with_docs,\n            'total_python_files': total_python_files,\n            'score': (len(doc_files_found) / len(required_docs) + doc_coverage) / 2\n        }\n    \n    def _generate_quality_report(self):\n        \"\"\"Generate comprehensive quality report.\"\"\"\n        logger.info(\"Generating Quality Gates Report\")\n        \n        total_score = sum(result.get('score', 0) for result in self.results.values())\n        average_score = total_score / len(self.results)\n        \n        gates_passed = sum(1 for result in self.results.values() if result.get('passed', False))\n        total_gates = len(self.results)\n        \n        report_lines = [\n            \"=\" * 80,\n            \"\ud83d\ude80 TERRAGON SDLC - QUALITY GATES EXECUTION REPORT\",\n            \"=\" * 80,\n            f\"Overall Status: {'\u2705 PASSED' if gates_passed == total_gates else '\u274c FAILED'}\",\n            f\"Gates Passed: {gates_passed}/{total_gates}\",\n            f\"Average Score: {average_score:.2%}\",\n            f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n            \"\",\n        ]\n        \n        # Individual gate results\n        gate_names = {\n            'code_structure': 'Gate 1: Code Structure Validation',\n            'syntax_validation': 'Gate 2: Syntax Validation', \n            'functionality_tests': 'Gate 3: Core Functionality Tests',\n            'performance_tests': 'Gate 4: Performance Tests',\n            'security_audit': 'Gate 5: Security Audit',\n            'documentation_coverage': 'Gate 6: Documentation Coverage'\n        }\n        \n        for gate_key, gate_name in gate_names.items():\n            result = self.results.get(gate_key, {})\n            status = \"\u2705 PASS\" if result.get('passed', False) else \"\u274c FAIL\"\n            score = result.get('score', 0)\n            \n            report_lines.append(f\"{gate_name}\")\n            report_lines.append(f\"  Status: {status}\")\n            report_lines.append(f\"  Score: {score:.2%}\")\n            \n            # Add specific details for each gate\n            if gate_key == 'functionality_tests' and 'test_results' in result:\n                passed = result.get('tests_passed', 0)\n                total = result.get('tests_total', 0)\n                report_lines.append(f\"  Tests: {passed}/{total} passed\")\n            \n            elif gate_key == 'performance_tests' and 'solve_time' in result:\n                solve_time = result.get('solve_time', 0)\n                report_lines.append(f\"  Solve Time: {solve_time:.3f}s\")\n            \n            elif gate_key == 'security_audit' and 'issues' in result:\n                issues = len(result.get('issues', []))\n                report_lines.append(f\"  Security Issues: {issues}\")\n            \n            elif gate_key == 'documentation_coverage':\n                doc_coverage = result.get('python_doc_coverage', 0)\n                report_lines.append(f\"  Doc Coverage: {doc_coverage:.1%}\")\n            \n            report_lines.append(\"\")\n        \n        # Summary and recommendations\n        report_lines.extend([\n            \"\ud83d\udccb RECOMMENDATIONS:\",\n        ])\n        \n        if gates_passed < total_gates:\n            report_lines.append(\"  \u26a0\ufe0f  Some quality gates failed - review issues above\")\n        \n        if average_score < 0.85:\n            report_lines.append(\"  \ud83d\udcc8 Consider improving overall quality score\")\n        \n        if self.results.get('security_audit', {}).get('issues'):\n            report_lines.append(\"  \ud83d\udd12 Address security issues before production deployment\")\n        \n        if self.results.get('performance_tests', {}).get('solve_time', 0) > 3.0:\n            report_lines.append(\"  \u26a1 Consider performance optimizations\")\n        \n        report_lines.extend([\n            \"\",\n            \"\ud83c\udf89 TERRAGON SDLC AUTONOMOUS EXECUTION COMPLETE\",\n            \"\",\n            \"\u2705 Generation 1: MAKE IT WORK - Basic functionality implemented\",\n            \"\u2705 Generation 2: MAKE IT ROBUST - Error handling and validation added\", \n            \"\u2705 Generation 3: MAKE IT SCALE - Performance optimization completed\",\n            \"\u2705 Quality Gates: Comprehensive testing and validation executed\",\n            \"\",\n            \"\ud83d\ude80 System is ready for production deployment!\",\n            \"\",\n            \"=\" * 80,\n            \"Report generated by Terragon Labs Autonomous SDLC System\",\n            \"=\" * 80\n        ])\n        \n        # Write report to file\n        report_content = \""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/six.py",
      "line_number": 1,
      "details": "# Copyright (c) 2010-2020 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n__version__ = \"1.16.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] >= (3, 4)\n\nif PY3:\n    string_types = str,\n    integer_types = int,\n    class_types = type,\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = basestring,\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 << 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n\n            def __len__(self):\n                return 1 << 31\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 << 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 << 63) - 1)\n        del X\n\nif PY34:\n    from importlib.util import spec_from_loader\nelse:\n    spec_from_loader = None\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def find_spec(self, fullname, path, target=None):\n        if fullname in self.known_modules:\n            return spec_from_loader(fullname, self)\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n    get_source = get_code  # same as get_code\n\n    def create_module(self, spec):\n        return self.load_module(spec.name)\n\n    def exec_module(self, module):\n        pass\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\"collections_abc\", \"collections\", \"collections.abc\" if sys.version_info >= (3, 3) else \"collections\"),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"dbm_ndbm\", \"dbm\", \"dbm.ndbm\"),\n    MovedModule(\"_dummy_thread\", \"dummy_thread\", \"_dummy_thread\" if sys.version_info < (3, 9) else \"_thread\"),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\",\n                \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\",\n                \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\",\n                \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n                      \"moves.urllib_parse\", \"moves.urllib.parse\")\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n                      \"moves.urllib_error\", \"moves.urllib.error\")\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n                      \"moves.urllib_request\", \"moves.urllib.request\")\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n                      \"moves.urllib_response\", \"moves.urllib.response\")\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = _urllib_robotparser_moved_attributes\n\n_importer._add_module(Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n                      \"moves.urllib_robotparser\", \"moves.urllib.robotparser\")\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return ['parse', 'error', 'request', 'response', 'robotparser']\n\n_importer._add_module(Module_six_moves_urllib(__name__ + \".moves.urllib\"),\n                      \"moves.urllib\")\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n    def advance_iterator(it):\n        return it.next()\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(get_unbound_function,\n         \"\"\"Get the function out of a possibly unbound function\"\"\")\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems,\n         \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(iterlists,\n         \"Return an iterator over the (key, [values]) pairs of a dictionary.\")\n\n\nif PY3:\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n    unichr = chr\n    import struct\n    int2byte = struct.Struct(\">B\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    del io\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] <= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n        _assertNotRegex = \"assertNotRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\n        _assertNotRegex = \"assertNotRegex\"\nelse:\n    def b(s):\n        return s\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r'\\\\', r'\\\\\\\\'), \"unicode_escape\")\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n    _assertNotRegex = \"assertNotRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\ndef assertNotRegex(self, *args, **kwargs):\n    return getattr(self, _assertNotRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        try:\n            if value is None:\n                value = tp()\n            if value.__traceback__ is not tb:\n                raise value.with_traceback(tb)\n            raise value\n        finally:\n            value = None\n            tb = None\n\nelse:\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\"\"\"def reraise(tp, value, tb=None):\n    try:\n        raise tp, value, tb\n    finally:\n        tb = None\n\"\"\")\n\n\nif sys.version_info[:2] > (3,):\n    exec_(\"\"\"def raise_from(value, from_value):\n    try:\n        raise value from from_value\n    finally:\n        value = None\n\"\"\")\nelse:\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (isinstance(fp, file) and\n                    isinstance(data, unicode) and\n                    fp.encoding is not None):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/typing_extensions.py",
      "line_number": 1,
      "details": "import abc\nimport collections\nimport collections.abc\nimport functools\nimport inspect\nimport operator\nimport sys\nimport types as _types\nimport typing\nimport warnings\n\n__all__ = [\n    # Super-special typing primitives.\n    'Any',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'LiteralString',\n    'ParamSpec',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'Self',\n    'Type',\n    'TypeVar',\n    'TypeVarTuple',\n    'Unpack',\n\n    # ABCs (from collections.abc).\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'AsyncGenerator',\n    'AsyncContextManager',\n    'Buffer',\n    'ChainMap',\n\n    # Concrete collection types.\n    'ContextManager',\n    'Counter',\n    'Deque',\n    'DefaultDict',\n    'NamedTuple',\n    'OrderedDict',\n    'TypedDict',\n\n    # Structural checks, a.k.a. protocols.\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # One-off things.\n    'Annotated',\n    'assert_never',\n    'assert_type',\n    'clear_overloads',\n    'dataclass_transform',\n    'deprecated',\n    'get_overloads',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_original_bases',\n    'get_protocol_members',\n    'get_type_hints',\n    'IntVar',\n    'is_protocol',\n    'is_typeddict',\n    'Literal',\n    'NewType',\n    'overload',\n    'override',\n    'Protocol',\n    'reveal_type',\n    'runtime',\n    'runtime_checkable',\n    'Text',\n    'TypeAlias',\n    'TypeAliasType',\n    'TypeGuard',\n    'TYPE_CHECKING',\n    'Never',\n    'NoReturn',\n    'Required',\n    'NotRequired',\n\n    # Pure aliases, have always been in typing\n    'AbstractSet',\n    'AnyStr',\n    'BinaryIO',\n    'Callable',\n    'Collection',\n    'Container',\n    'Dict',\n    'ForwardRef',\n    'FrozenSet',\n    'Generator',\n    'Generic',\n    'Hashable',\n    'IO',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'List',\n    'Mapping',\n    'MappingView',\n    'Match',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Optional',\n    'Pattern',\n    'Reversible',\n    'Sequence',\n    'Set',\n    'Sized',\n    'TextIO',\n    'Tuple',\n    'Union',\n    'ValuesView',\n    'cast',\n    'no_type_check',\n    'no_type_check_decorator',\n]\n\n# for backward compatibility\nPEP_560 = True\nGenericMeta = type\n\n# The functions below are modified copies of typing internal helpers.\n# They are needed by _ProtocolMeta and they provide support for PEP 646.\n\n\nclass _Sentinel:\n    def __repr__(self):\n        return \"<sentinel>\"\n\n\n_marker = _Sentinel()\n\n\ndef _check_generic(cls, parameters, elen=_marker):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not elen:\n        raise TypeError(f\"{cls} is not a generic class\")\n    if elen is _marker:\n        if not hasattr(cls, \"__parameters__\") or not cls.__parameters__:\n            raise TypeError(f\"{cls} is not a generic class\")\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, \"__parameters__\"):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum(isinstance(p, TypeVarTuple) for p in parameters)\n            if (num_tv_tuples > 0) and (alen >= elen - num_tv_tuples):\n                return\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} parameters for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\n\nif sys.version_info >= (3, 10):\n    def _should_collect_from_parameters(t):\n        return isinstance(\n            t, (typing._GenericAlias, _types.GenericAlias, _types.UnionType)\n        )\nelif sys.version_info >= (3, 9):\n    def _should_collect_from_parameters(t):\n        return isinstance(t, (typing._GenericAlias, _types.GenericAlias))\nelse:\n    def _should_collect_from_parameters(t):\n        return isinstance(t, typing._GenericAlias) and not t._special\n\n\ndef _collect_type_vars(types, typevar_types=None):\n    \"\"\"Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    \"\"\"\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if (\n            isinstance(t, typevar_types) and\n            t not in tvars and\n            not _is_unpack(t)\n        ):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\nNoReturn = typing.NoReturn\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = typing.TypeVar('T')  # Any type.\nKT = typing.TypeVar('KT')  # Key type.\nVT = typing.TypeVar('VT')  # Value type.\nT_co = typing.TypeVar('T_co', covariant=True)  # Any type covariant containers.\nT_contra = typing.TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n\n\nif sys.version_info >= (3, 11):\n    from typing import Any\nelse:\n\n    class _AnyMeta(type):\n        def __instancecheck__(self, obj):\n            if self is Any:\n                raise TypeError(\"typing_extensions.Any cannot be used with isinstance()\")\n            return super().__instancecheck__(obj)\n\n        def __repr__(self):\n            if self is Any:\n                return \"typing_extensions.Any\"\n            return super().__repr__()\n\n    class Any(metaclass=_AnyMeta):\n        \"\"\"Special type indicating an unconstrained type.\n        - Any is compatible with every type.\n        - Any assumed to have all methods.\n        - All values assumed to be instances of Any.\n        Note that all the above statements are true from the point of view of\n        static type checkers. At runtime, Any should not be used with instance\n        checks.\n        \"\"\"\n        def __new__(cls, *args, **kwargs):\n            if cls is Any:\n                raise TypeError(\"Any cannot be instantiated\")\n            return super().__new__(cls, *args, **kwargs)\n\n\nClassVar = typing.ClassVar\n\n\nclass _ExtensionsSpecialForm(typing._SpecialForm, _root=True):\n    def __repr__(self):\n        return 'typing_extensions.' + self._name\n\n\n# On older versions of typing there is an internal class named \"Final\".\n# 3.8+\nif hasattr(typing, 'Final') and sys.version_info[:2] >= (3, 7):\n    Final = typing.Final\n# 3.7\nelse:\n    class _FinalForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    Final = _FinalForm('Final',\n                       doc=\"\"\"A special typing construct to indicate that a name\n                       cannot be re-assigned or overridden in a subclass.\n                       For example:\n\n                           MAX_SIZE: Final = 9000\n                           MAX_SIZE += 1  # Error reported by type checker\n\n                           class Connection:\n                               TIMEOUT: Final[int] = 10\n                           class FastConnector(Connection):\n                               TIMEOUT = 1  # Error reported by type checker\n\n                       There is no runtime checking of these properties.\"\"\")\n\nif sys.version_info >= (3, 11):\n    final = typing.final\nelse:\n    # @final exists in 3.8+, but we backport it for all versions\n    # before 3.11 to keep support for the __final__ attribute.\n    # See https://bugs.python.org/issue46342\n    def final(f):\n        \"\"\"This decorator can be used to indicate to type checkers that\n        the decorated method cannot be overridden, and decorated class\n        cannot be subclassed. For example:\n\n            class Base:\n                @final\n                def done(self) -> None:\n                    ...\n            class Sub(Base):\n                def done(self) -> None:  # Error reported by type checker\n                    ...\n            @final\n            class Leaf:\n                ...\n            class Other(Leaf):  # Error reported by type checker\n                ...\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__final__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n        \"\"\"\n        try:\n            f.__final__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return f\n\n\ndef IntVar(name):\n    return typing.TypeVar(name)\n\n\n# A Literal bug was fixed in 3.11.0, 3.10.1 and 3.9.8\nif sys.version_info >= (3, 10, 1):\n    Literal = typing.Literal\nelse:\n    def _flatten_literal_params(parameters):\n        \"\"\"An internal helper for Literal creation: flatten Literals among parameters\"\"\"\n        params = []\n        for p in parameters:\n            if isinstance(p, _LiteralGenericAlias):\n                params.extend(p.__args__)\n            else:\n                params.append(p)\n        return tuple(params)\n\n    def _value_and_type_iter(params):\n        for p in params:\n            yield p, type(p)\n\n    class _LiteralGenericAlias(typing._GenericAlias, _root=True):\n        def __eq__(self, other):\n            if not isinstance(other, _LiteralGenericAlias):\n                return NotImplemented\n            these_args_deduped = set(_value_and_type_iter(self.__args__))\n            other_args_deduped = set(_value_and_type_iter(other.__args__))\n            return these_args_deduped == other_args_deduped\n\n        def __hash__(self):\n            return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n    class _LiteralForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, doc: str):\n            self._name = 'Literal'\n            self._doc = self.__doc__ = doc\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n\n            parameters = _flatten_literal_params(parameters)\n\n            val_type_pairs = list(_value_and_type_iter(parameters))\n            try:\n                deduped_pairs = set(val_type_pairs)\n            except TypeError:\n                # unhashable parameters\n                pass\n            else:\n                # similar logic to typing._deduplicate on Python 3.9+\n                if len(deduped_pairs) < len(val_type_pairs):\n                    new_parameters = []\n                    for pair in val_type_pairs:\n                        if pair in deduped_pairs:\n                            new_parameters.append(pair[0])\n                            deduped_pairs.remove(pair)\n                    assert not deduped_pairs, deduped_pairs\n                    parameters = tuple(new_parameters)\n\n            return _LiteralGenericAlias(self, parameters)\n\n    Literal = _LiteralForm(doc=\"\"\"\\\n                           A type that can be used to indicate to type checkers\n                           that the corresponding value has a value literally equivalent\n                           to the provided parameter. For example:\n\n                               var: Literal[4] = 4\n\n                           The type checker understands that 'var' is literally equal to\n                           the value 4 and no other value.\n\n                           Literal[...] cannot be subclassed. There is no runtime\n                           checking verifying that the parameter is actually a value\n                           instead of a type.\"\"\")\n\n\n_overload_dummy = typing._overload_dummy\n\n\nif hasattr(typing, \"get_overloads\"):  # 3.11+\n    overload = typing.overload\n    get_overloads = typing.get_overloads\n    clear_overloads = typing.clear_overloads\nelse:\n    # {module: {qualname: {firstlineno: func}}}\n    _overload_registry = collections.defaultdict(\n        functools.partial(collections.defaultdict, dict)\n    )\n\n    def overload(func):\n        \"\"\"Decorator for overloaded functions/methods.\n\n        In a stub file, place two or more stub definitions for the same\n        function in a row, each decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n        In a non-stub file (i.e. a regular .py file), do the same but\n        follow it with an implementation.  The implementation should *not*\n        be decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            # implementation goes here\n\n        The overloads for a function can be retrieved at runtime using the\n        get_overloads() function.\n        \"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        try:\n            _overload_registry[f.__module__][f.__qualname__][\n                f.__code__.co_firstlineno\n            ] = func\n        except AttributeError:\n            # Not a normal function; ignore.\n            pass\n        return _overload_dummy\n\n    def get_overloads(func):\n        \"\"\"Return all defined overloads for *func* as a sequence.\"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        if f.__module__ not in _overload_registry:\n            return []\n        mod_dict = _overload_registry[f.__module__]\n        if f.__qualname__ not in mod_dict:\n            return []\n        return list(mod_dict[f.__qualname__].values())\n\n    def clear_overloads():\n        \"\"\"Clear all overloads in the registry.\"\"\"\n        _overload_registry.clear()\n\n\n# This is not a real generic class.  Don't use outside annotations.\nType = typing.Type\n\n# Various ABCs mimicking those in collections.abc.\n# A few are simply re-exported for completeness.\n\n\nAwaitable = typing.Awaitable\nCoroutine = typing.Coroutine\nAsyncIterable = typing.AsyncIterable\nAsyncIterator = typing.AsyncIterator\nDeque = typing.Deque\nContextManager = typing.ContextManager\nAsyncContextManager = typing.AsyncContextManager\nDefaultDict = typing.DefaultDict\n\n# 3.7.2+\nif hasattr(typing, 'OrderedDict'):\n    OrderedDict = typing.OrderedDict\n# 3.7.0-3.7.2\nelse:\n    OrderedDict = typing._alias(collections.OrderedDict, (KT, VT))\n\nCounter = typing.Counter\nChainMap = typing.ChainMap\nAsyncGenerator = typing.AsyncGenerator\nText = typing.Text\nTYPE_CHECKING = typing.TYPE_CHECKING\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible', 'Buffer',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n    'typing_extensions': ['Buffer'],\n}\n\n\n_EXCLUDED_ATTRS = {\n    \"__abstractmethods__\", \"__annotations__\", \"__weakref__\", \"_is_protocol\",\n    \"_is_runtime_protocol\", \"__dict__\", \"__slots__\", \"__parameters__\",\n    \"__orig_bases__\", \"__module__\", \"_MutableMapping__marker\", \"__doc__\",\n    \"__subclasshook__\", \"__orig_class__\", \"__init__\", \"__new__\",\n    \"__protocol_attrs__\", \"__callable_proto_members_only__\",\n}\n\nif sys.version_info < (3, 8):\n    _EXCLUDED_ATTRS |= {\n        \"_gorg\", \"__next_in_mro__\", \"__extra__\", \"__tree_hash__\", \"__args__\",\n        \"__origin__\"\n    }\n\nif sys.version_info >= (3, 9):\n    _EXCLUDED_ATTRS.add(\"__class_getitem__\")\n\nif sys.version_info >= (3, 12):\n    _EXCLUDED_ATTRS.add(\"__type_params__\")\n\n_EXCLUDED_ATTRS = frozenset(_EXCLUDED_ATTRS)\n\n\ndef _get_protocol_attrs(cls):\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in {'Protocol', 'Generic'}:\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in (*base.__dict__, *annotations):\n            if (not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS):\n                attrs.add(attr)\n    return attrs\n\n\ndef _maybe_adjust_parameters(cls):\n    \"\"\"Helper function used in Protocol.__init_subclass__ and _TypedDictMeta.__new__.\n\n    The contents of this function are very similar\n    to logic found in typing.Generic.__init_subclass__\n    on the CPython main branch.\n    \"\"\"\n    tvars = []\n    if '__orig_bases__' in cls.__dict__:\n        tvars = _collect_type_vars(cls.__orig_bases__)\n        # Look for Generic[T1, ..., Tn] or Protocol[T1, ..., Tn].\n        # If found, tvars must be a subset of it.\n        # If not found, tvars is it.\n        # Also check for and reject plain Generic,\n        # and reject multiple Generic[...] and/or Protocol[...].\n        gvars = None\n        for base in cls.__orig_bases__:\n            if (isinstance(base, typing._GenericAlias) and\n                    base.__origin__ in (typing.Generic, Protocol)):\n                # for error messages\n                the_base = base.__origin__.__name__\n                if gvars is not None:\n                    raise TypeError(\n                        \"Cannot inherit from Generic[...]\"\n                        \" and/or Protocol[...] multiple types.\")\n                gvars = base.__parameters__\n        if gvars is None:\n            gvars = tvars\n        else:\n            tvarset = set(tvars)\n            gvarset = set(gvars)\n            if not tvarset <= gvarset:\n                s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                s_args = ', '.join(str(g) for g in gvars)\n                raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                f\" not listed in {the_base}[{s_args}]\")\n            tvars = gvars\n    cls.__parameters__ = tuple(tvars)\n\n\ndef _caller(depth=2):\n    try:\n        return sys._getframe(depth).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        return None\n\n\n# The performance of runtime-checkable protocols is significantly improved on Python 3.12,\n# so we backport the 3.12 version of Protocol to Python <=3.11\nif sys.version_info >= (3, 12):\n    Protocol = typing.Protocol\nelse:\n    def _allow_reckless_class_checks(depth=3):\n        \"\"\"Allow instance and class checks for special stdlib modules.\n        The abc and functools modules indiscriminately call isinstance() and\n        issubclass() on the whole MRO of a user class, which may contain protocols.\n        \"\"\"\n        return _caller(depth) in {'abc', 'functools', None}\n\n    def _no_init(self, *args, **kwargs):\n        if type(self)._is_protocol:\n            raise TypeError('Protocols cannot be instantiated')\n\n    if sys.version_info >= (3, 8):\n        # Inheriting from typing._ProtocolMeta isn't actually desirable,\n        # but is necessary to allow typing.Protocol and typing_extensions.Protocol\n        # to mix without getting TypeErrors about \"metaclass conflict\"\n        _typing_Protocol = typing.Protocol\n        _ProtocolMetaBase = type(_typing_Protocol)\n    else:\n        _typing_Protocol = _marker\n        _ProtocolMetaBase = abc.ABCMeta\n\n    class _ProtocolMeta(_ProtocolMetaBase):\n        # This metaclass is somewhat unfortunate,\n        # but is necessary for several reasons...\n        #\n        # NOTE: DO NOT call super() in any methods in this class\n        # That would call the methods on typing._ProtocolMeta on Python 3.8-3.11\n        # and those are slow\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            if name == \"Protocol\" and len(bases) < 2:\n                pass\n            elif {Protocol, _typing_Protocol} & set(bases):\n                for base in bases:\n                    if not (\n                        base in {object, typing.Generic, Protocol, _typing_Protocol}\n                        or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__, [])\n                        or is_protocol(base)\n                    ):\n                        raise TypeError(\n                            f\"Protocols can only inherit from other protocols, \"\n                            f\"got {base!r}\"\n                        )\n            return abc.ABCMeta.__new__(mcls, name, bases, namespace, **kwargs)\n\n        def __init__(cls, *args, **kwargs):\n            abc.ABCMeta.__init__(cls, *args, **kwargs)\n            if getattr(cls, \"_is_protocol\", False):\n                cls.__protocol_attrs__ = _get_protocol_attrs(cls)\n                # PEP 544 prohibits using issubclass()\n                # with protocols that have non-method members.\n                cls.__callable_proto_members_only__ = all(\n                    callable(getattr(cls, attr, None)) for attr in cls.__protocol_attrs__\n                )\n\n        def __subclasscheck__(cls, other):\n            if cls is Protocol:\n                return type.__subclasscheck__(cls, other)\n            if (\n                getattr(cls, '_is_protocol', False)\n                and not _allow_reckless_class_checks()\n            ):\n                if not isinstance(other, type):\n                    # Same error message as for issubclass(1, int).\n                    raise TypeError('issubclass() arg 1 must be a class')\n                if (\n                    not cls.__callable_proto_members_only__\n                    and cls.__dict__.get(\"__subclasshook__\") is _proto_hook\n                ):\n                    raise TypeError(\n                        \"Protocols with non-method members don't support issubclass()\"\n                    )\n                if not getattr(cls, '_is_runtime_protocol', False):\n                    raise TypeError(\n                        \"Instance and class checks can only be used with \"\n                        \"@runtime_checkable protocols\"\n                    )\n            return abc.ABCMeta.__subclasscheck__(cls, other)\n\n        def __instancecheck__(cls, instance):\n            # We need this method for situations where attributes are\n            # assigned in __init__.\n            if cls is Protocol:\n                return type.__instancecheck__(cls, instance)\n            if not getattr(cls, \"_is_protocol\", False):\n                # i.e., it's a concrete subclass of a protocol\n                return abc.ABCMeta.__instancecheck__(cls, instance)\n\n            if (\n                not getattr(cls, '_is_runtime_protocol', False) and\n                not _allow_reckless_class_checks()\n            ):\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n\n            if abc.ABCMeta.__instancecheck__(cls, instance):\n                return True\n\n            for attr in cls.__protocol_attrs__:\n                try:\n                    val = inspect.getattr_static(instance, attr)\n                except AttributeError:\n                    break\n                if val is None and callable(getattr(cls, attr, None)):\n                    break\n            else:\n                return True\n\n            return False\n\n        def __eq__(cls, other):\n            # Hack so that typing.Generic.__class_getitem__\n            # treats typing_extensions.Protocol\n            # as equivalent to typing.Protocol on Python 3.8+\n            if abc.ABCMeta.__eq__(cls, other) is True:\n                return True\n            return (\n                cls is Protocol and other is getattr(typing, \"Protocol\", object())\n            )\n\n        # This has to be defined, or the abc-module cache\n        # complains about classes with this metaclass being unhashable,\n        # if we define only __eq__!\n        def __hash__(cls) -> int:\n            return type.__hash__(cls)\n\n    @classmethod\n    def _proto_hook(cls, other):\n        if not cls.__dict__.get('_is_protocol', False):\n            return NotImplemented\n\n        for attr in cls.__protocol_attrs__:\n            for base in other.__mro__:\n                # Check if the members appears in the class dictionary...\n                if attr in base.__dict__:\n                    if base.__dict__[attr] is None:\n                        return NotImplemented\n                    break\n\n                # ...or in annotations, if it is a sub-protocol.\n                annotations = getattr(base, '__annotations__', {})\n                if (\n                    isinstance(annotations, collections.abc.Mapping)\n                    and attr in annotations\n                    and is_protocol(other)\n                ):\n                    break\n            else:\n                return NotImplemented\n        return True\n\n    if sys.version_info >= (3, 8):\n        class Protocol(typing.Generic, metaclass=_ProtocolMeta):\n            __doc__ = typing.Protocol.__doc__\n            __slots__ = ()\n            _is_protocol = True\n            _is_runtime_protocol = False\n\n            def __init_subclass__(cls, *args, **kwargs):\n                super().__init_subclass__(*args, **kwargs)\n\n                # Determine if this is a protocol or a concrete subclass.\n                if not cls.__dict__.get('_is_protocol', False):\n                    cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n                # Set (or override) the protocol subclass hook.\n                if '__subclasshook__' not in cls.__dict__:\n                    cls.__subclasshook__ = _proto_hook\n\n                # Prohibit instantiation for protocol classes\n                if cls._is_protocol and cls.__init__ is Protocol.__init__:\n                    cls.__init__ = _no_init\n\n    else:\n        class Protocol(metaclass=_ProtocolMeta):\n            # There is quite a lot of overlapping code with typing.Generic.\n            # Unfortunately it is hard to avoid this on Python <3.8,\n            # as the typing module on Python 3.7 doesn't let us subclass typing.Generic!\n            \"\"\"Base class for protocol classes. Protocol classes are defined as::\n\n                class Proto(Protocol):\n                    def meth(self) -> int:\n                        ...\n\n            Such classes are primarily used with static type checkers that recognize\n            structural subtyping (static duck-typing), for example::\n\n                class C:\n                    def meth(self) -> int:\n                        return 0\n\n                def func(x: Proto) -> int:\n                    return x.meth()\n\n                func(C())  # Passes static type check\n\n            See PEP 544 for details. Protocol classes decorated with\n            @typing_extensions.runtime_checkable act\n            as simple-minded runtime-checkable protocols that check\n            only the presence of given attributes, ignoring their type signatures.\n\n            Protocol classes can be generic, they are defined as::\n\n                class GenProto(Protocol[T]):\n                    def meth(self) -> T:\n                        ...\n            \"\"\"\n            __slots__ = ()\n            _is_protocol = True\n            _is_runtime_protocol = False\n\n            def __new__(cls, *args, **kwds):\n                if cls is Protocol:\n                    raise TypeError(\"Type Protocol cannot be instantiated; \"\n                                    \"it can only be used as a base class\")\n                return super().__new__(cls)\n\n            @typing._tp_cache\n            def __class_getitem__(cls, params):\n                if not isinstance(params, tuple):\n                    params = (params,)\n                if not params and cls is not typing.Tuple:\n                    raise TypeError(\n                        f\"Parameter list to {cls.__qualname__}[...] cannot be empty\")\n                msg = \"Parameters to generic types must be types.\"\n                params = tuple(typing._type_check(p, msg) for p in params)\n                if cls is Protocol:\n                    # Generic can only be subscripted with unique type variables.\n                    if not all(isinstance(p, typing.TypeVar) for p in params):\n                        i = 0\n                        while isinstance(params[i], typing.TypeVar):\n                            i += 1\n                        raise TypeError(\n                            \"Parameters to Protocol[...] must all be type variables.\"\n                            f\" Parameter {i + 1} is {params[i]}\")\n                    if len(set(params)) != len(params):\n                        raise TypeError(\n                            \"Parameters to Protocol[...] must all be unique\")\n                else:\n                    # Subscripting a regular Generic subclass.\n                    _check_generic(cls, params, len(cls.__parameters__))\n                return typing._GenericAlias(cls, params)\n\n            def __init_subclass__(cls, *args, **kwargs):\n                if '__orig_bases__' in cls.__dict__:\n                    error = typing.Generic in cls.__orig_bases__\n                else:\n                    error = typing.Generic in cls.__bases__\n                if error:\n                    raise TypeError(\"Cannot inherit from plain Generic\")\n                _maybe_adjust_parameters(cls)\n\n                # Determine if this is a protocol or a concrete subclass.\n                if not cls.__dict__.get('_is_protocol', None):\n                    cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n                # Set (or override) the protocol subclass hook.\n                if '__subclasshook__' not in cls.__dict__:\n                    cls.__subclasshook__ = _proto_hook\n\n                # Prohibit instantiation for protocol classes\n                if cls._is_protocol and cls.__init__ is Protocol.__init__:\n                    cls.__init__ = _no_init\n\n\nif sys.version_info >= (3, 8):\n    runtime_checkable = typing.runtime_checkable\nelse:\n    def runtime_checkable(cls):\n        \"\"\"Mark a protocol class as a runtime protocol, so that it\n        can be used with isinstance() and issubclass(). Raise TypeError\n        if applied to a non-protocol class.\n\n        This allows a simple-minded structural check very similar to the\n        one-offs in collections.abc such as Hashable.\n        \"\"\"\n        if not (\n            (isinstance(cls, _ProtocolMeta) or issubclass(cls, typing.Generic))\n            and getattr(cls, \"_is_protocol\", False)\n        ):\n            raise TypeError('@runtime_checkable can be only applied to protocol classes,'\n                            f' got {cls!r}')\n        cls._is_runtime_protocol = True\n        return cls\n\n\n# Exists for backwards compatibility.\nruntime = runtime_checkable\n\n\n# Our version of runtime-checkable protocols is faster on Python 3.7-3.11\nif sys.version_info >= (3, 12):\n    SupportsInt = typing.SupportsInt\n    SupportsFloat = typing.SupportsFloat\n    SupportsComplex = typing.SupportsComplex\n    SupportsBytes = typing.SupportsBytes\n    SupportsIndex = typing.SupportsIndex\n    SupportsAbs = typing.SupportsAbs\n    SupportsRound = typing.SupportsRound\nelse:\n    @runtime_checkable\n    class SupportsInt(Protocol):\n        \"\"\"An ABC with one abstract method __int__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __int__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsFloat(Protocol):\n        \"\"\"An ABC with one abstract method __float__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __float__(self) -> float:\n            pass\n\n    @runtime_checkable\n    class SupportsComplex(Protocol):\n        \"\"\"An ABC with one abstract method __complex__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __complex__(self) -> complex:\n            pass\n\n    @runtime_checkable\n    class SupportsBytes(Protocol):\n        \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __bytes__(self) -> bytes:\n            pass\n\n    @runtime_checkable\n    class SupportsIndex(Protocol):\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __index__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsAbs(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __abs__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __abs__(self) -> T_co:\n            pass\n\n    @runtime_checkable\n    class SupportsRound(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __round__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __round__(self, ndigits: int = 0) -> T_co:\n            pass\n\n\ndef _ensure_subclassable(mro_entries):\n    def inner(func):\n        if sys.implementation.name == \"pypy\" and sys.version_info < (3, 9):\n            cls_dict = {\n                \"__call__\": staticmethod(func),\n                \"__mro_entries__\": staticmethod(mro_entries)\n            }\n            t = type(func.__name__, (), cls_dict)\n            return functools.update_wrapper(t(), func)\n        else:\n            func.__mro_entries__ = mro_entries\n            return func\n    return inner\n\n\nif sys.version_info >= (3, 13):\n    # The standard library TypedDict in Python 3.8 does not store runtime information\n    # about which (if any) keys are optional.  See https://bugs.python.org/issue38834\n    # The standard library TypedDict in Python 3.9.0/1 does not honour the \"total\"\n    # keyword with old-style TypedDict().  See https://bugs.python.org/issue42059\n    # The standard library TypedDict below Python 3.11 does not store runtime\n    # information about optional and required keys when using Required or NotRequired.\n    # Generic TypedDicts are also impossible using typing.TypedDict on Python <3.11.\n    # Aaaand on 3.12 we add __orig_bases__ to TypedDict\n    # to enable better runtime introspection.\n    # On 3.13 we deprecate some odd ways of creating TypedDicts.\n    TypedDict = typing.TypedDict\n    _TypedDictMeta = typing._TypedDictMeta\n    is_typeddict = typing.is_typeddict\nelse:\n    # 3.10.0 and later\n    _TAKES_MODULE = \"module\" in inspect.signature(typing._type_check).parameters\n\n    if sys.version_info >= (3, 8):\n        _fake_name = \"Protocol\"\n    else:\n        _fake_name = \"_Protocol\"\n\n    class _TypedDictMeta(type):\n        def __new__(cls, name, bases, ns, total=True):\n            \"\"\"Create new typed dict class object.\n\n            This method is called when TypedDict is subclassed,\n            or when TypedDict is instantiated. This way\n            TypedDict supports all three syntax forms described in its docstring.\n            Subclasses and instances of TypedDict return actual dictionaries.\n            \"\"\"\n            for base in bases:\n                if type(base) is not _TypedDictMeta and base is not typing.Generic:\n                    raise TypeError('cannot inherit from both a TypedDict type '\n                                    'and a non-TypedDict base class')\n\n            if any(issubclass(b, typing.Generic) for b in bases):\n                generic_base = (typing.Generic,)\n            else:\n                generic_base = ()\n\n            # typing.py generally doesn't let you inherit from plain Generic, unless\n            # the name of the class happens to be \"Protocol\" (or \"_Protocol\" on 3.7).\n            tp_dict = type.__new__(_TypedDictMeta, _fake_name, (*generic_base, dict), ns)\n            tp_dict.__name__ = name\n            if tp_dict.__qualname__ == _fake_name:\n                tp_dict.__qualname__ = name\n\n            if not hasattr(tp_dict, '__orig_bases__'):\n                tp_dict.__orig_bases__ = bases\n\n            annotations = {}\n            own_annotations = ns.get('__annotations__', {})\n            msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n            if _TAKES_MODULE:\n                own_annotations = {\n                    n: typing._type_check(tp, msg, module=tp_dict.__module__)\n                    for n, tp in own_annotations.items()\n                }\n            else:\n                own_annotations = {\n                    n: typing._type_check(tp, msg)\n                    for n, tp in own_annotations.items()\n                }\n            required_keys = set()\n            optional_keys = set()\n\n            for base in bases:\n                annotations.update(base.__dict__.get('__annotations__', {}))\n                required_keys.update(base.__dict__.get('__required_keys__', ()))\n                optional_keys.update(base.__dict__.get('__optional_keys__', ()))\n\n            annotations.update(own_annotations)\n            for annotation_key, annotation_type in own_annotations.items():\n                annotation_origin = get_origin(annotation_type)\n                if annotation_origin is Annotated:\n                    annotation_args = get_args(annotation_type)\n                    if annotation_args:\n                        annotation_type = annotation_args[0]\n                        annotation_origin = get_origin(annotation_type)\n\n                if annotation_origin is Required:\n                    required_keys.add(annotation_key)\n                elif annotation_origin is NotRequired:\n                    optional_keys.add(annotation_key)\n                elif total:\n                    required_keys.add(annotation_key)\n                else:\n                    optional_keys.add(annotation_key)\n\n            tp_dict.__annotations__ = annotations\n            tp_dict.__required_keys__ = frozenset(required_keys)\n            tp_dict.__optional_keys__ = frozenset(optional_keys)\n            if not hasattr(tp_dict, '__total__'):\n                tp_dict.__total__ = total\n            return tp_dict\n\n        __call__ = dict  # static method\n\n        def __subclasscheck__(cls, other):\n            # Typed dicts are only for static structural subtyping.\n            raise TypeError('TypedDict does not support instance and class checks')\n\n        __instancecheck__ = __subclasscheck__\n\n    _TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\n\n    @_ensure_subclassable(lambda bases: (_TypedDict,))\n    def TypedDict(__typename, __fields=_marker, *, total=True, **kwargs):\n        \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n        TypedDict creates a dictionary type such that a type checker will expect all\n        instances to have a certain set of keys, where each key is\n        associated with a value of a consistent type. This expectation\n        is not checked at runtime.\n\n        Usage::\n\n            class Point2D(TypedDict):\n                x: int\n                y: int\n                label: str\n\n            a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n            b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n\n            assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n\n        The type info can be accessed via the Point2D.__annotations__ dict, and\n        the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n        TypedDict supports an additional equivalent form::\n\n            Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n        By default, all keys must be present in a TypedDict. It is possible\n        to override this by specifying totality::\n\n            class Point2D(TypedDict, total=False):\n                x: int\n                y: int\n\n        This means that a Point2D TypedDict can have any of the keys omitted. A type\n        checker is only expected to support a literal False or True as the value of\n        the total argument. True is the default, and makes all items defined in the\n        class body be required.\n\n        The Required and NotRequired special forms can also be used to mark\n        individual keys as being required or not required::\n\n            class Point2D(TypedDict):\n                x: int  # the \"x\" key must always be present (Required is the default)\n                y: NotRequired[int]  # the \"y\" key can be omitted\n\n        See PEP 655 for more details on Required and NotRequired.\n        \"\"\"\n        if __fields is _marker or __fields is None:\n            if __fields is _marker:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n\n            example = f\"`{__typename} = TypedDict({__typename!r}, {{}})`\"\n            deprecation_msg = (\n                f\"{deprecated_thing} is deprecated and will be disallowed in \"\n                \"Python 3.15. To create a TypedDict class with 0 fields \"\n                \"using the functional syntax, pass an empty dictionary, e.g. \"\n            ) + example + \".\"\n            warnings.warn(deprecation_msg, DeprecationWarning, stacklevel=2)\n            __fields = kwargs\n        elif kwargs:\n            raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                            \" but not both\")\n        if kwargs:\n            warnings.warn(\n                \"The kwargs-based syntax for TypedDict definitions is deprecated \"\n                \"in Python 3.11, will be removed in Python 3.13, and may not be \"\n                \"understood by third-party type checkers.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        ns = {'__annotations__': dict(__fields)}\n        module = _caller()\n        if module is not None:\n            # Setting correct module is necessary to make typed dict classes pickleable.\n            ns['__module__'] = module\n\n        td = _TypedDictMeta(__typename, (), ns, total=total)\n        td.__orig_bases__ = (TypedDict,)\n        return td\n\n    if hasattr(typing, \"_TypedDictMeta\"):\n        _TYPEDDICT_TYPES = (typing._TypedDictMeta, _TypedDictMeta)\n    else:\n        _TYPEDDICT_TYPES = (_TypedDictMeta,)\n\n    def is_typeddict(tp):\n        \"\"\"Check if an annotation is a TypedDict class\n\n        For example::\n            class Film(TypedDict):\n                title: str\n                year: int\n\n            is_typeddict(Film)  # => True\n            is_typeddict(Union[list, str])  # => False\n        \"\"\"\n        # On 3.8, this would otherwise return True\n        if hasattr(typing, \"TypedDict\") and tp is typing.TypedDict:\n            return False\n        return isinstance(tp, _TYPEDDICT_TYPES)\n\n\nif hasattr(typing, \"assert_type\"):\n    assert_type = typing.assert_type\n\nelse:\n    def assert_type(__val, __typ):\n        \"\"\"Assert (to the type checker) that the value is of the given type.\n\n        When the type checker encounters a call to assert_type(), it\n        emits an error if the value is not of the specified type::\n\n            def greet(name: str) -> None:\n                assert_type(name, str)  # ok\n                assert_type(name, int)  # type checker error\n\n        At runtime this returns the first argument unchanged and otherwise\n        does nothing.\n        \"\"\"\n        return __val\n\n\nif hasattr(typing, \"Required\"):\n    get_type_hints = typing.get_type_hints\nelse:\n    # replaces _strip_annotations()\n    def _strip_extras(t):\n        \"\"\"Strips Annotated, Required and NotRequired from a given type.\"\"\"\n        if isinstance(t, _AnnotatedAlias):\n            return _strip_extras(t.__origin__)\n        if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired):\n            return _strip_extras(t.__args__[0])\n        if isinstance(t, typing._GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return t.copy_with(stripped_args)\n        if hasattr(_types, \"GenericAlias\") and isinstance(t, _types.GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return _types.GenericAlias(t.__origin__, stripped_args)\n        if hasattr(_types, \"UnionType\") and isinstance(t, _types.UnionType):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return functools.reduce(operator.or_, stripped_args)\n\n        return t\n\n    def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n        \"\"\"Return type hints for an object.\n\n        This is often the same as obj.__annotations__, but it handles\n        forward references encoded as string literals, adds Optional[t] if a\n        default value equal to None is set and recursively replaces all\n        'Annotated[T, ...]', 'Required[T]' or 'NotRequired[T]' with 'T'\n        (unless 'include_extras=True').\n\n        The argument may be a module, class, method, or function. The annotations\n        are returned as a dictionary. For classes, annotations include also\n        inherited members.\n\n        TypeError is raised if the argument is not of a type that can contain\n        annotations, and an empty dictionary is returned if no annotations are\n        present.\n\n        BEWARE -- the behavior of globalns and localns is counterintuitive\n        (unless you are familiar with how eval() and exec() work).  The\n        search order is locals first, then globals.\n\n        - If no dict arguments are passed, an attempt is made to use the\n          globals from obj (or the respective module's globals for classes),\n          and these are also used as the locals.  If the object does not appear\n          to have globals, an empty dictionary is used.\n\n        - If one dict argument is passed, it is used for both globals and\n          locals.\n\n        - If two dict arguments are passed, they specify globals and\n          locals, respectively.\n        \"\"\"\n        if hasattr(typing, \"Annotated\"):\n            hint = typing.get_type_hints(\n                obj, globalns=globalns, localns=localns, include_extras=True\n            )\n        else:\n            hint = typing.get_type_hints(obj, globalns=globalns, localns=localns)\n        if include_extras:\n            return hint\n        return {k: _strip_extras(t) for k, t in hint.items()}\n\n\n# Python 3.9+ has PEP 593 (Annotated)\nif hasattr(typing, 'Annotated'):\n    Annotated = typing.Annotated\n    # Not exported and not a public API, but needed for get_origin() and get_args()\n    # to work.\n    _AnnotatedAlias = typing._AnnotatedAlias\n# 3.7-3.8\nelse:\n    class _AnnotatedAlias(typing._GenericAlias, _root=True):\n        \"\"\"Runtime representation of an annotated type.\n\n        At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n        with extra annotations. The alias behaves like a normal typing alias,\n        instantiating is the same as instantiating the underlying type, binding\n        it to types is also the same.\n        \"\"\"\n        def __init__(self, origin, metadata):\n            if isinstance(origin, _AnnotatedAlias):\n                metadata = origin.__metadata__ + metadata\n                origin = origin.__origin__\n            super().__init__(origin, origin)\n            self.__metadata__ = metadata\n\n        def copy_with(self, params):\n            assert len(params) == 1\n            new_type = params[0]\n            return _AnnotatedAlias(new_type, self.__metadata__)\n\n        def __repr__(self):\n            return (f\"typing_extensions.Annotated[{typing._type_repr(self.__origin__)}, \"\n                    f\"{', '.join(repr(a) for a in self.__metadata__)}]\")\n\n        def __reduce__(self):\n            return operator.getitem, (\n                Annotated, (self.__origin__,) + self.__metadata__\n            )\n\n        def __eq__(self, other):\n            if not isinstance(other, _AnnotatedAlias):\n                return NotImplemented\n            if self.__origin__ != other.__origin__:\n                return False\n            return self.__metadata__ == other.__metadata__\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__metadata__))\n\n    class Annotated:\n        \"\"\"Add context specific metadata to a type.\n\n        Example: Annotated[int, runtime_check.Unsigned] indicates to the\n        hypothetical runtime_check module that this type is an unsigned int.\n        Every other consumer of this type can ignore this metadata and treat\n        this type as int.\n\n        The first argument to Annotated must be a valid type (and will be in\n        the __origin__ field), the remaining arguments are kept as a tuple in\n        the __extra__ field.\n\n        Details:\n\n        - It's an error to call `Annotated` with less than two arguments.\n        - Nested Annotated are flattened::\n\n            Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n        - Instantiating an annotated type is equivalent to instantiating the\n        underlying type::\n\n            Annotated[C, Ann1](5) == C(5)\n\n        - Annotated can be used as a generic type alias::\n\n            Optimized = Annotated[T, runtime.Optimize()]\n            Optimized[int] == Annotated[int, runtime.Optimize()]\n\n            OptimizedList = Annotated[List[T], runtime.Optimize()]\n            OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n        \"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, *args, **kwargs):\n            raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n        @typing._tp_cache\n        def __class_getitem__(cls, params):\n            if not isinstance(params, tuple) or len(params) < 2:\n                raise TypeError(\"Annotated[...] should be used \"\n                                \"with at least two arguments (a type and an \"\n                                \"annotation).\")\n            allowed_special_forms = (ClassVar, Final)\n            if get_origin(params[0]) in allowed_special_forms:\n                origin = params[0]\n            else:\n                msg = \"Annotated[t, ...]: t must be a type.\"\n                origin = typing._type_check(params[0], msg)\n            metadata = tuple(params[1:])\n            return _AnnotatedAlias(origin, metadata)\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                f\"Cannot subclass {cls.__module__}.Annotated\"\n            )\n\n# Python 3.8 has get_origin() and get_args() but those implementations aren't\n# Annotated-aware, so we can't use those. Python 3.9's versions don't support\n# ParamSpecArgs and ParamSpecKwargs, so only Python 3.10's versions will do.\nif sys.version_info[:2] >= (3, 10):\n    get_origin = typing.get_origin\n    get_args = typing.get_args\n# 3.7-3.9\nelse:\n    try:\n        # 3.9+\n        from typing import _BaseGenericAlias\n    except ImportError:\n        _BaseGenericAlias = typing._GenericAlias\n    try:\n        # 3.9+\n        from typing import GenericAlias as _typing_GenericAlias\n    except ImportError:\n        _typing_GenericAlias = typing._GenericAlias\n\n    def get_origin(tp):\n        \"\"\"Get the unsubscripted version of a type.\n\n        This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n        and Annotated. Return None for unsupported types. Examples::\n\n            get_origin(Literal[42]) is Literal\n            get_origin(int) is None\n            get_origin(ClassVar[int]) is ClassVar\n            get_origin(Generic) is Generic\n            get_origin(Generic[T]) is Generic\n            get_origin(Union[T, int]) is Union\n            get_origin(List[Tuple[T, T]][int]) == list\n            get_origin(P.args) is P\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return Annotated\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias, _BaseGenericAlias,\n                           ParamSpecArgs, ParamSpecKwargs)):\n            return tp.__origin__\n        if tp is typing.Generic:\n            return typing.Generic\n        return None\n\n    def get_args(tp):\n        \"\"\"Get type arguments with all substitutions performed.\n\n        For unions, basic simplifications used by Union constructor are performed.\n        Examples::\n            get_args(Dict[str, int]) == (str, int)\n            get_args(int) == ()\n            get_args(Union[int, Union[T, int], str][int]) == (int, str)\n            get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n            get_args(Callable[[], T][int]) == ([], int)\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return (tp.__origin__,) + tp.__metadata__\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias)):\n            if getattr(tp, \"_special\", False):\n                return ()\n            res = tp.__args__\n            if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return ()\n\n\n# 3.10+\nif hasattr(typing, 'TypeAlias'):\n    TypeAlias = typing.TypeAlias\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeAlias(self, parameters):\n        \"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example above.\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n# 3.7-3.8\nelse:\n    TypeAlias = _ExtensionsSpecialForm(\n        'TypeAlias',\n        doc=\"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example\n        above.\"\"\"\n    )\n\n\ndef _set_default(type_param, default):\n    if isinstance(default, (tuple, list)):\n        type_param.__default__ = tuple((typing._type_check(d, \"Default must be a type\")\n                                        for d in default))\n    elif default != _marker:\n        type_param.__default__ = typing._type_check(default, \"Default must be a type\")\n    else:\n        type_param.__default__ = None\n\n\ndef _set_module(typevarlike):\n    # for pickling:\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod\n\n\nclass _DefaultMixin:\n    \"\"\"Mixin for TypeVarLike defaults.\"\"\"\n\n    __slots__ = ()\n    __init__ = _set_default\n\n\n# Classes using this metaclass must provide a _backported_typevarlike ClassVar\nclass _TypeVarLikeMeta(type):\n    def __instancecheck__(cls, __instance: Any) -> bool:\n        return isinstance(__instance, cls._backported_typevarlike)\n\n\n# Add default and infer_variance parameters from PEP 696 and 695\nclass TypeVar(metaclass=_TypeVarLikeMeta):\n    \"\"\"Type variable.\"\"\"\n\n    _backported_typevarlike = typing.TypeVar\n\n    def __new__(cls, name, *constraints, bound=None,\n                covariant=False, contravariant=False,\n                default=_marker, infer_variance=False):\n        if hasattr(typing, \"TypeAliasType\"):\n            # PEP 695 implemented, can pass infer_variance to typing.TypeVar\n            typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                     covariant=covariant, contravariant=contravariant,\n                                     infer_variance=infer_variance)\n        else:\n            typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                     covariant=covariant, contravariant=contravariant)\n            if infer_variance and (covariant or contravariant):\n                raise ValueError(\"Variance cannot be specified with infer_variance.\")\n            typevar.__infer_variance__ = infer_variance\n        _set_default(typevar, default)\n        _set_module(typevar)\n        return typevar\n\n    def __init_subclass__(cls) -> None:\n        raise TypeError(f\"type '{__name__}.TypeVar' is not an acceptable base type\")\n\n\n# Python 3.10+ has PEP 612\nif hasattr(typing, 'ParamSpecArgs'):\n    ParamSpecArgs = typing.ParamSpecArgs\n    ParamSpecKwargs = typing.ParamSpecKwargs\n# 3.7-3.9\nelse:\n    class _Immutable:\n        \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n        __slots__ = ()\n\n        def __copy__(self):\n            return self\n\n        def __deepcopy__(self, memo):\n            return self\n\n    class ParamSpecArgs(_Immutable):\n        \"\"\"The args for a ParamSpec object.\n\n        Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n        ParamSpecArgs objects have a reference back to their ParamSpec:\n\n        P.args.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.args\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecArgs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n    class ParamSpecKwargs(_Immutable):\n        \"\"\"The kwargs for a ParamSpec object.\n\n        Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n        ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n        P.kwargs.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.kwargs\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecKwargs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n# 3.10+\nif hasattr(typing, 'ParamSpec'):\n\n    # Add default parameter - PEP 696\n    class ParamSpec(metaclass=_TypeVarLikeMeta):\n        \"\"\"Parameter specification.\"\"\"\n\n        _backported_typevarlike = typing.ParamSpec\n\n        def __new__(cls, name, *, bound=None,\n                    covariant=False, contravariant=False,\n                    infer_variance=False, default=_marker):\n            if hasattr(typing, \"TypeAliasType\"):\n                # PEP 695 implemented, can pass infer_variance to typing.TypeVar\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant,\n                                             infer_variance=infer_variance)\n            else:\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant)\n                paramspec.__infer_variance__ = infer_variance\n\n            _set_default(paramspec, default)\n            _set_module(paramspec)\n            return paramspec\n\n        def __init_subclass__(cls) -> None:\n            raise TypeError(f\"type '{__name__}.ParamSpec' is not an acceptable base type\")\n\n# 3.7-3.9\nelse:\n\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class ParamSpec(list, _DefaultMixin):\n        \"\"\"Parameter specification variable.\n\n        Usage::\n\n           P = ParamSpec('P')\n\n        Parameter specification variables exist primarily for the benefit of static\n        type checkers.  They are used to forward the parameter types of one\n        callable to another callable, a pattern commonly found in higher order\n        functions and decorators.  They are only valid when used in ``Concatenate``,\n        or s the first argument to ``Callable``. In Python 3.10 and higher,\n        they are also supported in user-defined Generics at runtime.\n        See class Generic for more information on generic types.  An\n        example for annotating a decorator::\n\n           T = TypeVar('T')\n           P = ParamSpec('P')\n\n           def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n               '''A type-safe decorator to add logging to a function.'''\n               def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n                   logging.info(f'{f.__name__} was called')\n                   return f(*args, **kwargs)\n               return inner\n\n           @add_logging\n           def add_two(x: float, y: float) -> float:\n               '''Add two numbers together.'''\n               return x + y\n\n        Parameter specification variables defined with covariant=True or\n        contravariant=True can be used to declare covariant or contravariant\n        generic types.  These keyword arguments are valid, but their actual semantics\n        are yet to be decided.  See PEP 612 for details.\n\n        Parameter specification variables can be introspected. e.g.:\n\n           P.__name__ == 'T'\n           P.__bound__ == None\n           P.__covariant__ == False\n           P.__contravariant__ == False\n\n        Note that only parameter specification variables defined in global scope can\n        be pickled.\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        @property\n        def args(self):\n            return ParamSpecArgs(self)\n\n        @property\n        def kwargs(self):\n            return ParamSpecKwargs(self)\n\n        def __init__(self, name, *, bound=None, covariant=False, contravariant=False,\n                     infer_variance=False, default=_marker):\n            super().__init__([self])\n            self.__name__ = name\n            self.__covariant__ = bool(covariant)\n            self.__contravariant__ = bool(contravariant)\n            self.__infer_variance__ = bool(infer_variance)\n            if bound:\n                self.__bound__ = typing._type_check(bound, 'Bound must be a type.')\n            else:\n                self.__bound__ = None\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __repr__(self):\n            if self.__infer_variance__:\n                prefix = ''\n            elif self.__covariant__:\n                prefix = '+'\n            elif self.__contravariant__:\n                prefix = '-'\n            else:\n                prefix = '~'\n            return prefix + self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        # Hack to get typing._type_check to pass.\n        def __call__(self, *args, **kwargs):\n            pass\n\n\n# 3.7-3.9\nif not hasattr(typing, 'Concatenate'):\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class _ConcatenateGenericAlias(list):\n\n        # Trick Generic into looking into this for __parameters__.\n        __class__ = typing._GenericAlias\n\n        # Flag in 3.8.\n        _special = False\n\n        def __init__(self, origin, args):\n            super().__init__(args)\n            self.__origin__ = origin\n            self.__args__ = args\n\n        def __repr__(self):\n            _type_repr = typing._type_repr\n            return (f'{_type_repr(self.__origin__)}'\n                    f'[{\", \".join(_type_repr(arg) for arg in self.__args__)}]')\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__args__))\n\n        # Hack to get typing._type_check to pass in Generic.\n        def __call__(self, *args, **kwargs):\n            pass\n\n        @property\n        def __parameters__(self):\n            return tuple(\n                tp for tp in self.__args__ if isinstance(tp, (typing.TypeVar, ParamSpec))\n            )\n\n\n# 3.7-3.9\n@typing._tp_cache\ndef _concatenate_getitem(self, parameters):\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not isinstance(parameters[-1], ParamSpec):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = tuple(typing._type_check(p, msg) for p in parameters)\n    return _ConcatenateGenericAlias(self, parameters)\n\n\n# 3.10+\nif hasattr(typing, 'Concatenate'):\n    Concatenate = typing.Concatenate\n    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias  # noqa: F811\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def Concatenate(self, parameters):\n        \"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\"\n        return _concatenate_getitem(self, parameters)\n# 3.7-8\nelse:\n    class _ConcatenateForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            return _concatenate_getitem(self, parameters)\n\n    Concatenate = _ConcatenateForm(\n        'Concatenate',\n        doc=\"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\")\n\n# 3.10+\nif hasattr(typing, 'TypeGuard'):\n    TypeGuard = typing.TypeGuard\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeGuard(self, parameters):\n        \"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\"\n        item = typing._type_check(parameters, f'{self} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n# 3.7-3.8\nelse:\n    class _TypeGuardForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type')\n            return typing._GenericAlias(self, (item,))\n\n    TypeGuard = _TypeGuardForm(\n        'TypeGuard',\n        doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\")\n\n\n# Vendored from cpython typing._SpecialFrom\nclass _SpecialForm(typing._Final, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return f'typing_extensions.{self._name}'\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return typing.Union[self, other]\n\n    def __ror__(self, other):\n        return typing.Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @typing._tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nif hasattr(typing, \"LiteralString\"):\n    LiteralString = typing.LiteralString\nelse:\n    @_SpecialForm\n    def LiteralString(self, params):\n        \"\"\"Represents an arbitrary literal string.\n\n        Example::\n\n          from pip._vendor.typing_extensions import LiteralString\n\n          def query(sql: LiteralString) -> ...:\n              ...\n\n          query(\"SELECT * FROM table\")  # ok\n          query(f\"SELECT * FROM {input()}\")  # not ok\n\n        See PEP 675 for details.\n\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Self\"):\n    Self = typing.Self\nelse:\n    @_SpecialForm\n    def Self(self, params):\n        \"\"\"Used to spell the type of \"self\" in classes.\n\n        Example::\n\n          from typing import Self\n\n          class ReturnsSelf:\n              def parse(self, data: bytes) -> Self:\n                  ...\n                  return self\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Never\"):\n    Never = typing.Never\nelse:\n    @_SpecialForm\n    def Never(self, params):\n        \"\"\"The bottom type, a type that has no members.\n\n        This can be used to define a function that should never be\n        called, or a function that never returns::\n\n            from pip._vendor.typing_extensions import Never\n\n            def never_call_me(arg: Never) -> None:\n                pass\n\n            def int_or_str(arg: int | str) -> None:\n                never_call_me(arg)  # type checker error\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        never_call_me(arg)  # ok, arg is of type Never\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, 'Required'):\n    Required = typing.Required\n    NotRequired = typing.NotRequired\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def Required(self, parameters):\n        \"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\n    @_ExtensionsSpecialForm\n    def NotRequired(self, parameters):\n        \"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\nelse:\n    class _RequiredForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    Required = _RequiredForm(\n        'Required',\n        doc=\"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\")\n    NotRequired = _RequiredForm(\n        'NotRequired',\n        doc=\"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\")\n\n\n_UNPACK_DOC = \"\"\"\\\nType unpack operator.\n\nThe type unpack operator takes the child types from some container type,\nsuch as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'. For\nexample:\n\n  # For some generic class `Foo`:\n  Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n  Ts = TypeVarTuple('Ts')\n  # Specifies that `Bar` is generic in an arbitrary number of types.\n  # (Think of `Ts` as a tuple of an arbitrary number of individual\n  #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n  #  `Generic[]`.)\n  class Bar(Generic[Unpack[Ts]]): ...\n  Bar[int]  # Valid\n  Bar[int, str]  # Also valid\n\nFrom Python 3.11, this can also be done using the `*` operator:\n\n    Foo[*tuple[int, str]]\n    class Bar(Generic[*Ts]): ...\n\nThe operator can also be used along with a `TypedDict` to annotate\n`**kwargs` in a function signature. For instance:\n\n  class Movie(TypedDict):\n    name: str\n    year: int\n\n  # This function expects two keyword arguments - *name* of type `str` and\n  # *year* of type `int`.\n  def foo(**kwargs: Unpack[Movie]): ...\n\nNote that there is only some runtime checking of this operator. Not\neverything the runtime allows may be accepted by static type checkers.\n\nFor more information, see PEP 646 and PEP 692.\n\"\"\"\n\n\nif sys.version_info >= (3, 12):  # PEP 692 changed the repr of Unpack[]\n    Unpack = typing.Unpack\n\n    def _is_unpack(obj):\n        return get_origin(obj) is Unpack\n\nelif sys.version_info[:2] >= (3, 9):\n    class _UnpackSpecialForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, getitem):\n            super().__init__(getitem)\n            self.__doc__ = _UNPACK_DOC\n\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    @_UnpackSpecialForm\n    def Unpack(self, parameters):\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return _UnpackAlias(self, (item,))\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\nelse:\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    class _UnpackForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return _UnpackAlias(self, (item,))\n\n    Unpack = _UnpackForm('Unpack', doc=_UNPACK_DOC)\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\n\nif hasattr(typing, \"TypeVarTuple\"):  # 3.11+\n\n    # Add default parameter - PEP 696\n    class TypeVarTuple(metaclass=_TypeVarLikeMeta):\n        \"\"\"Type variable tuple.\"\"\"\n\n        _backported_typevarlike = typing.TypeVarTuple\n\n        def __new__(cls, name, *, default=_marker):\n            tvt = typing.TypeVarTuple(name)\n            _set_default(tvt, default)\n            _set_module(tvt)\n            return tvt\n\n        def __init_subclass__(self, *args, **kwds):\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nelse:\n    class TypeVarTuple(_DefaultMixin):\n        \"\"\"Type variable tuple.\n\n        Usage::\n\n            Ts = TypeVarTuple('Ts')\n\n        In the same way that a normal type variable is a stand-in for a single\n        type such as ``int``, a type variable *tuple* is a stand-in for a *tuple*\n        type such as ``Tuple[int, str]``.\n\n        Type variable tuples can be used in ``Generic`` declarations.\n        Consider the following example::\n\n            class Array(Generic[*Ts]): ...\n\n        The ``Ts`` type variable tuple here behaves like ``tuple[T1, T2]``,\n        where ``T1`` and ``T2`` are type variables. To use these type variables\n        as type parameters of ``Array``, we must *unpack* the type variable tuple using\n        the star operator: ``*Ts``. The signature of ``Array`` then behaves\n        as if we had simply written ``class Array(Generic[T1, T2]): ...``.\n        In contrast to ``Generic[T1, T2]``, however, ``Generic[*Shape]`` allows\n        us to parameterise the class with an *arbitrary* number of type parameters.\n\n        Type variable tuples can be used anywhere a normal ``TypeVar`` can.\n        This includes class definitions, as shown above, as well as function\n        signatures and variable annotations::\n\n            class Array(Generic[*Ts]):\n\n                def __init__(self, shape: Tuple[*Ts]):\n                    self._shape: Tuple[*Ts] = shape\n\n                def get_shape(self) -> Tuple[*Ts]:\n                    return self._shape\n\n            shape = (Height(480), Width(640))\n            x: Array[Height, Width] = Array(shape)\n            y = abs(x)  # Inferred type is Array[Height, Width]\n            z = x + x   #        ...    is Array[Height, Width]\n            x.get_shape()  #     ...    is tuple[Height, Width]\n\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        def __iter__(self):\n            yield self.__unpacked__\n\n        def __init__(self, name, *, default=_marker):\n            self.__name__ = name\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n            self.__unpacked__ = Unpack[self]\n\n        def __repr__(self):\n            return self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(self, *args, **kwds):\n            if '_root' not in kwds:\n                raise TypeError(\"Cannot subclass special typing classes\")\n\n\nif hasattr(typing, \"reveal_type\"):\n    reveal_type = typing.reveal_type\nelse:\n    def reveal_type(__obj: T) -> T:\n        \"\"\"Reveal the inferred type of a variable.\n\n        When a static type checker encounters a call to ``reveal_type()``,\n        it will emit the inferred type of the argument::\n\n            x: int = 1\n            reveal_type(x)\n\n        Running a static type checker (e.g., ``mypy``) on this example\n        will produce output similar to 'Revealed type is \"builtins.int\"'.\n\n        At runtime, the function prints the runtime type of the\n        argument and returns it unchanged.\n\n        \"\"\"\n        print(f\"Runtime type is {type(__obj).__name__!r}\", file=sys.stderr)\n        return __obj\n\n\nif hasattr(typing, \"assert_never\"):\n    assert_never = typing.assert_never\nelse:\n    def assert_never(__arg: Never) -> Never:\n        \"\"\"Assert to the type checker that a line of code is unreachable.\n\n        Example::\n\n            def int_or_str(arg: int | str) -> None:\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        assert_never(arg)\n\n        If a type checker finds that a call to assert_never() is\n        reachable, it will emit an error.\n\n        At runtime, this throws an exception when called.\n\n        \"\"\"\n        raise AssertionError(\"Expected code to be unreachable\")\n\n\nif sys.version_info >= (3, 12):\n    # dataclass_transform exists in 3.11 but lacks the frozen_default parameter\n    dataclass_transform = typing.dataclass_transform\nelse:\n    def dataclass_transform(\n        *,\n        eq_default: bool = True,\n        order_default: bool = False,\n        kw_only_default: bool = False,\n        frozen_default: bool = False,\n        field_specifiers: typing.Tuple[\n            typing.Union[typing.Type[typing.Any], typing.Callable[..., typing.Any]],\n            ...\n        ] = (),\n        **kwargs: typing.Any,\n    ) -> typing.Callable[[T], T]:\n        \"\"\"Decorator that marks a function, class, or metaclass as providing\n        dataclass-like behavior.\n\n        Example:\n\n            from pip._vendor.typing_extensions import dataclass_transform\n\n            _T = TypeVar(\"_T\")\n\n            # Used on a decorator function\n            @dataclass_transform()\n            def create_model(cls: type[_T]) -> type[_T]:\n                ...\n                return cls\n\n            @create_model\n            class CustomerModel:\n                id: int\n                name: str\n\n            # Used on a base class\n            @dataclass_transform()\n            class ModelBase: ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n            # Used on a metaclass\n            @dataclass_transform()\n            class ModelMeta(type): ...\n\n            class ModelBase(metaclass=ModelMeta): ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n        Each of the ``CustomerModel`` classes defined in this example will now\n        behave similarly to a dataclass created with the ``@dataclasses.dataclass``\n        decorator. For example, the type checker will synthesize an ``__init__``\n        method.\n\n        The arguments to this decorator can be used to customize this behavior:\n        - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n          True or False if it is omitted by the caller.\n        - ``order_default`` indicates whether the ``order`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``frozen_default`` indicates whether the ``frozen`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``field_specifiers`` specifies a static list of supported classes\n          or functions that describe fields, similar to ``dataclasses.field()``.\n\n        At runtime, this decorator records its arguments in the\n        ``__dataclass_transform__`` attribute on the decorated object.\n\n        See PEP 681 for details.\n\n        \"\"\"\n        def decorator(cls_or_fn):\n            cls_or_fn.__dataclass_transform__ = {\n                \"eq_default\": eq_default,\n                \"order_default\": order_default,\n                \"kw_only_default\": kw_only_default,\n                \"frozen_default\": frozen_default,\n                \"field_specifiers\": field_specifiers,\n                \"kwargs\": kwargs,\n            }\n            return cls_or_fn\n        return decorator\n\n\nif hasattr(typing, \"override\"):\n    override = typing.override\nelse:\n    _F = typing.TypeVar(\"_F\", bound=typing.Callable[..., typing.Any])\n\n    def override(__arg: _F) -> _F:\n        \"\"\"Indicate that a method is intended to override a method in a base class.\n\n        Usage:\n\n            class Base:\n                def method(self) -> None: ...\n                    pass\n\n            class Child(Base):\n                @override\n                def method(self) -> None:\n                    super().method()\n\n        When this decorator is applied to a method, the type checker will\n        validate that it overrides a method with the same name on a base class.\n        This helps prevent bugs that may occur when a base class is changed\n        without an equivalent change to a child class.\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__override__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n\n        See PEP 698 for details.\n\n        \"\"\"\n        try:\n            __arg.__override__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return __arg\n\n\nif hasattr(typing, \"deprecated\"):\n    deprecated = typing.deprecated\nelse:\n    _T = typing.TypeVar(\"_T\")\n\n    def deprecated(\n        __msg: str,\n        *,\n        category: typing.Optional[typing.Type[Warning]] = DeprecationWarning,\n        stacklevel: int = 1,\n    ) -> typing.Callable[[_T], _T]:\n        \"\"\"Indicate that a class, function or overload is deprecated.\n\n        Usage:\n\n            @deprecated(\"Use B instead\")\n            class A:\n                pass\n\n            @deprecated(\"Use g instead\")\n            def f():\n                pass\n\n            @overload\n            @deprecated(\"int support is deprecated\")\n            def g(x: int) -> int: ...\n            @overload\n            def g(x: str) -> int: ...\n\n        When this decorator is applied to an object, the type checker\n        will generate a diagnostic on usage of the deprecated object.\n\n        The warning specified by ``category`` will be emitted on use\n        of deprecated objects. For functions, that happens on calls;\n        for classes, on instantiation. If the ``category`` is ``None``,\n        no warning is emitted. The ``stacklevel`` determines where the\n        warning is emitted. If it is ``1`` (the default), the warning\n        is emitted at the direct caller of the deprecated object; if it\n        is higher, it is emitted further up the stack.\n\n        The decorator sets the ``__deprecated__``\n        attribute on the decorated object to the deprecation message\n        passed to the decorator. If applied to an overload, the decorator\n        must be after the ``@overload`` decorator for the attribute to\n        exist on the overload as returned by ``get_overloads()``.\n\n        See PEP 702 for details.\n\n        \"\"\"\n        def decorator(__arg: _T) -> _T:\n            if category is None:\n                __arg.__deprecated__ = __msg\n                return __arg\n            elif isinstance(__arg, type):\n                original_new = __arg.__new__\n                has_init = __arg.__init__ is not object.__init__\n\n                @functools.wraps(original_new)\n                def __new__(cls, *args, **kwargs):\n                    warnings.warn(__msg, category=category, stacklevel=stacklevel + 1)\n                    if original_new is not object.__new__:\n                        return original_new(cls, *args, **kwargs)\n                    # Mirrors a similar check in object.__new__.\n                    elif not has_init and (args or kwargs):\n                        raise TypeError(f\"{cls.__name__}() takes no arguments\")\n                    else:\n                        return original_new(cls)\n\n                __arg.__new__ = staticmethod(__new__)\n                __arg.__deprecated__ = __new__.__deprecated__ = __msg\n                return __arg\n            elif callable(__arg):\n                @functools.wraps(__arg)\n                def wrapper(*args, **kwargs):\n                    warnings.warn(__msg, category=category, stacklevel=stacklevel + 1)\n                    return __arg(*args, **kwargs)\n\n                __arg.__deprecated__ = wrapper.__deprecated__ = __msg\n                return wrapper\n            else:\n                raise TypeError(\n                    \"@deprecated decorator with non-None category must be applied to \"\n                    f\"a class or callable, not {__arg!r}\"\n                )\n\n        return decorator\n\n\n# We have to do some monkey patching to deal with the dual nature of\n# Unpack/TypeVarTuple:\n# - We want Unpack to be a kind of TypeVar so it gets accepted in\n#   Generic[Unpack[Ts]]\n# - We want it to *not* be treated as a TypeVar for the purposes of\n#   counting generic parameters, so that when we subscript a generic,\n#   the runtime doesn't try to substitute the Unpack with the subscripted type.\nif not hasattr(typing, \"TypeVarTuple\"):\n    typing._collect_type_vars = _collect_type_vars\n    typing._check_generic = _check_generic\n\n\n# Backport typing.NamedTuple as it exists in Python 3.12.\n# In 3.11, the ability to define generic `NamedTuple`s was supported.\n# This was explicitly disallowed in 3.9-3.10, and only half-worked in <=3.8.\n# On 3.12, we added __orig_bases__ to call-based NamedTuples\n# On 3.13, we deprecated kwargs-based NamedTuples\nif sys.version_info >= (3, 13):\n    NamedTuple = typing.NamedTuple\nelse:\n    def _make_nmtuple(name, types, module, defaults=()):\n        fields = [n for n, t in types]\n        annotations = {n: typing._type_check(t, f\"field {n} annotation must be a type\")\n                       for n, t in types}\n        nm_tpl = collections.namedtuple(name, fields,\n                                        defaults=defaults, module=module)\n        nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = annotations\n        # The `_field_types` attribute was removed in 3.9;\n        # in earlier versions, it is the same as the `__annotations__` attribute\n        if sys.version_info < (3, 9):\n            nm_tpl._field_types = annotations\n        return nm_tpl\n\n    _prohibited_namedtuple_fields = typing._prohibited\n    _special_namedtuple_fields = frozenset({'__module__', '__name__', '__annotations__'})\n\n    class _NamedTupleMeta(type):\n        def __new__(cls, typename, bases, ns):\n            assert _NamedTuple in bases\n            for base in bases:\n                if base is not _NamedTuple and base is not typing.Generic:\n                    raise TypeError(\n                        'can only inherit from a NamedTuple type and Generic')\n            bases = tuple(tuple if base is _NamedTuple else base for base in bases)\n            types = ns.get('__annotations__', {})\n            default_names = []\n            for field_name in types:\n                if field_name in ns:\n                    default_names.append(field_name)\n                elif default_names:\n                    raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                    f\"cannot follow default field\"\n                                    f\"{'s' if len(default_names) > 1 else ''} \"\n                                    f\"{', '.join(default_names)}\")\n            nm_tpl = _make_nmtuple(\n                typename, types.items(),\n                defaults=[ns[n] for n in default_names],\n                module=ns['__module__']\n            )\n            nm_tpl.__bases__ = bases\n            if typing.Generic in bases:\n                if hasattr(typing, '_generic_class_getitem'):  # 3.12+\n                    nm_tpl.__class_getitem__ = classmethod(typing._generic_class_getitem)\n                else:\n                    class_getitem = typing.Generic.__class_getitem__.__func__\n                    nm_tpl.__class_getitem__ = classmethod(class_getitem)\n            # update from user namespace without overriding special namedtuple attributes\n            for key in ns:\n                if key in _prohibited_namedtuple_fields:\n                    raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n                elif key not in _special_namedtuple_fields and key not in nm_tpl._fields:\n                    setattr(nm_tpl, key, ns[key])\n            if typing.Generic in bases:\n                nm_tpl.__init_subclass__()\n            return nm_tpl\n\n    _NamedTuple = type.__new__(_NamedTupleMeta, 'NamedTuple', (), {})\n\n    def _namedtuple_mro_entries(bases):\n        assert NamedTuple in bases\n        return (_NamedTuple,)\n\n    @_ensure_subclassable(_namedtuple_mro_entries)\n    def NamedTuple(__typename, __fields=_marker, **kwargs):\n        \"\"\"Typed version of namedtuple.\n\n        Usage::\n\n            class Employee(NamedTuple):\n                name: str\n                id: int\n\n        This is equivalent to::\n\n            Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n        The resulting class has an extra __annotations__ attribute, giving a\n        dict that maps field names to types.  (The field names are also in\n        the _fields attribute, which is part of the namedtuple API.)\n        An alternative equivalent functional syntax is also accepted::\n\n            Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n        \"\"\"\n        if __fields is _marker:\n            if kwargs:\n                deprecated_thing = \"Creating NamedTuple classes using keyword arguments\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"Use the class-based or functional syntax instead.\"\n                )\n            else:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n                example = f\"`{__typename} = NamedTuple({__typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif __fields is None:\n            if kwargs:\n                raise TypeError(\n                    \"Cannot pass `None` as the 'fields' parameter \"\n                    \"and also specify fields using keyword arguments\"\n                )\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n                example = f\"`{__typename} = NamedTuple({__typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif kwargs:\n            raise TypeError(\"Either list of fields or keywords\"\n                            \" can be provided to NamedTuple, not both\")\n        if __fields is _marker or __fields is None:\n            warnings.warn(\n                deprecation_msg.format(name=deprecated_thing, remove=\"3.15\"),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            __fields = kwargs.items()\n        nt = _make_nmtuple(__typename, __fields, module=_caller())\n        nt.__orig_bases__ = (NamedTuple,)\n        return nt\n\n    # On 3.8+, alter the signature so that it matches typing.NamedTuple.\n    # The signature of typing.NamedTuple on >=3.8 is invalid syntax in Python 3.7,\n    # so just leave the signature as it is on 3.7.\n    if sys.version_info >= (3, 8):\n        _new_signature = '(typename, fields=None, /, **kwargs)'\n        if isinstance(NamedTuple, _types.FunctionType):\n            NamedTuple.__text_signature__ = _new_signature\n        else:\n            NamedTuple.__call__.__text_signature__ = _new_signature\n\n\nif hasattr(collections.abc, \"Buffer\"):\n    Buffer = collections.abc.Buffer\nelse:\n    class Buffer(abc.ABC):\n        \"\"\"Base class for classes that implement the buffer protocol.\n\n        The buffer protocol allows Python objects to expose a low-level\n        memory buffer interface. Before Python 3.12, it is not possible\n        to implement the buffer protocol in pure Python code, or even\n        to check whether a class implements the buffer protocol. In\n        Python 3.12 and higher, the ``__buffer__`` method allows access\n        to the buffer protocol from Python code, and the\n        ``collections.abc.Buffer`` ABC allows checking whether a class\n        implements the buffer protocol.\n\n        To indicate support for the buffer protocol in earlier versions,\n        inherit from this ABC, either in a stub file or at runtime,\n        or use ABC registration. This ABC provides no methods, because\n        there is no Python-accessible methods shared by pre-3.12 buffer\n        classes. It is useful primarily for static checks.\n\n        \"\"\"\n\n    # As a courtesy, register the most common stdlib buffer classes.\n    Buffer.register(memoryview)\n    Buffer.register(bytearray)\n    Buffer.register(bytes)\n\n\n# Backport of types.get_original_bases, available on 3.12+ in CPython\nif hasattr(_types, \"get_original_bases\"):\n    get_original_bases = _types.get_original_bases\nelse:\n    def get_original_bases(__cls):\n        \"\"\"Return the class's \"original\" bases prior to modification by `__mro_entries__`.\n\n        Examples::\n\n            from typing import TypeVar, Generic\n            from pip._vendor.typing_extensions import NamedTuple, TypedDict\n\n            T = TypeVar(\"T\")\n            class Foo(Generic[T]): ...\n            class Bar(Foo[int], float): ...\n            class Baz(list[str]): ...\n            Eggs = NamedTuple(\"Eggs\", [(\"a\", int), (\"b\", str)])\n            Spam = TypedDict(\"Spam\", {\"a\": int, \"b\": str})\n\n            assert get_original_bases(Bar) == (Foo[int], float)\n            assert get_original_bases(Baz) == (list[str],)\n            assert get_original_bases(Eggs) == (NamedTuple,)\n            assert get_original_bases(Spam) == (TypedDict,)\n            assert get_original_bases(int) == (object,)\n        \"\"\"\n        try:\n            return __cls.__orig_bases__\n        except AttributeError:\n            try:\n                return __cls.__bases__\n            except AttributeError:\n                raise TypeError(\n                    f'Expected an instance of type, not {type(__cls).__name__!r}'\n                ) from None\n\n\n# NewType is a class on Python 3.10+, making it pickleable\n# The error message for subclassing instances of NewType was improved on 3.11+\nif sys.version_info >= (3, 11):\n    NewType = typing.NewType\nelse:\n    class NewType:\n        \"\"\"NewType creates simple unique types with almost zero\n        runtime overhead. NewType(name, tp) is considered a subtype of tp\n        by static type checkers. At runtime, NewType(name, tp) returns\n        a dummy callable that simply returns its argument. Usage::\n            UserId = NewType('UserId', int)\n            def name_by_id(user_id: UserId) -> str:\n                ...\n            UserId('user')          # Fails type check\n            name_by_id(42)          # Fails type check\n            name_by_id(UserId(42))  # OK\n            num = UserId(5) + 1     # type: int\n        \"\"\"\n\n        def __call__(self, obj):\n            return obj\n\n        def __init__(self, name, tp):\n            self.__qualname__ = name\n            if '.' in name:\n                name = name.rpartition('.')[-1]\n            self.__name__ = name\n            self.__supertype__ = tp\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __mro_entries__(self, bases):\n            # We defined __mro_entries__ to get a better error message\n            # if a user attempts to subclass a NewType instance. bpo-46170\n            supercls_name = self.__name__\n\n            class Dummy:\n                def __init_subclass__(cls):\n                    subcls_name = cls.__name__\n                    raise TypeError(\n                        f\"Cannot subclass an instance of NewType. \"\n                        f\"Perhaps you were looking for: \"\n                        f\"`{subcls_name} = NewType({subcls_name!r}, {supercls_name})`\"\n                    )\n\n            return (Dummy,)\n\n        def __repr__(self):\n            return f'{self.__module__}.{self.__qualname__}'\n\n        def __reduce__(self):\n            return self.__qualname__\n\n        if sys.version_info >= (3, 10):\n            # PEP 604 methods\n            # It doesn't make sense to have these methods on Python <3.10\n\n            def __or__(self, other):\n                return typing.Union[self, other]\n\n            def __ror__(self, other):\n                return typing.Union[other, self]\n\n\nif hasattr(typing, \"TypeAliasType\"):\n    TypeAliasType = typing.TypeAliasType\nelse:\n    def _is_unionable(obj):\n        \"\"\"Corresponds to is_unionable() in unionobject.c in CPython.\"\"\"\n        return obj is None or isinstance(obj, (\n            type,\n            _types.GenericAlias,\n            _types.UnionType,\n            TypeAliasType,\n        ))\n\n    class TypeAliasType:\n        \"\"\"Create named, parameterized type aliases.\n\n        This provides a backport of the new `type` statement in Python 3.12:\n\n            type ListOrSet[T] = list[T] | set[T]\n\n        is equivalent to:\n\n            T = TypeVar(\"T\")\n            ListOrSet = TypeAliasType(\"ListOrSet\", list[T] | set[T], type_params=(T,))\n\n        The name ListOrSet can then be used as an alias for the type it refers to.\n\n        The type_params argument should contain all the type parameters used\n        in the value of the type alias. If the alias is not generic, this\n        argument is omitted.\n\n        Static type checkers should only support type aliases declared using\n        TypeAliasType that follow these rules:\n\n        - The first argument (the name) must be a string literal.\n        - The TypeAliasType instance must be immediately assigned to a variable\n          of the same name. (For example, 'X = TypeAliasType(\"Y\", int)' is invalid,\n          as is 'X, Y = TypeAliasType(\"X\", int), TypeAliasType(\"Y\", int)').\n\n        \"\"\"\n\n        def __init__(self, name: str, value, *, type_params=()):\n            if not isinstance(name, str):\n                raise TypeError(\"TypeAliasType name must be a string\")\n            self.__value__ = value\n            self.__type_params__ = type_params\n\n            parameters = []\n            for type_param in type_params:\n                if isinstance(type_param, TypeVarTuple):\n                    parameters.extend(type_param)\n                else:\n                    parameters.append(type_param)\n            self.__parameters__ = tuple(parameters)\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n            # Setting this attribute closes the TypeAliasType from further modification\n            self.__name__ = name\n\n        def __setattr__(self, __name: str, __value: object) -> None:\n            if hasattr(self, \"__name__\"):\n                self._raise_attribute_error(__name)\n            super().__setattr__(__name, __value)\n\n        def __delattr__(self, __name: str) -> Never:\n            self._raise_attribute_error(__name)\n\n        def _raise_attribute_error(self, name: str) -> Never:\n            # Match the Python 3.12 error messages exactly\n            if name == \"__name__\":\n                raise AttributeError(\"readonly attribute\")\n            elif name in {\"__value__\", \"__type_params__\", \"__parameters__\", \"__module__\"}:\n                raise AttributeError(\n                    f\"attribute '{name}' of 'typing.TypeAliasType' objects \"\n                    \"is not writable\"\n                )\n            else:\n                raise AttributeError(\n                    f\"'typing.TypeAliasType' object has no attribute '{name}'\"\n                )\n\n        def __repr__(self) -> str:\n            return self.__name__\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n            parameters = [\n                typing._type_check(\n                    item, f'Subscripting {self.__name__} requires a type.'\n                )\n                for item in parameters\n            ]\n            return typing._GenericAlias(self, tuple(parameters))\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                \"type 'typing_extensions.TypeAliasType' is not an acceptable base type\"\n            )\n\n        # The presence of this method convinces typing._type_check\n        # that TypeAliasTypes are types.\n        def __call__(self):\n            raise TypeError(\"Type alias is not callable\")\n\n        if sys.version_info >= (3, 10):\n            def __or__(self, right):\n                # For forward compatibility with 3.12, reject Unions\n                # that are not accepted by the built-in Union.\n                if not _is_unionable(right):\n                    return NotImplemented\n                return typing.Union[self, right]\n\n            def __ror__(self, left):\n                if not _is_unionable(left):\n                    return NotImplemented\n                return typing.Union[left, self]\n\n\nif hasattr(typing, \"is_protocol\"):\n    is_protocol = typing.is_protocol\n    get_protocol_members = typing.get_protocol_members\nelse:\n    def is_protocol(__tp: type) -> bool:\n        \"\"\"Return True if the given type is a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, is_protocol\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> is_protocol(P)\n            True\n            >>> is_protocol(int)\n            False\n        \"\"\"\n        return (\n            isinstance(__tp, type)\n            and getattr(__tp, '_is_protocol', False)\n            and __tp is not Protocol\n            and __tp is not getattr(typing, \"Protocol\", object())\n        )\n\n    def get_protocol_members(__tp: type) -> typing.FrozenSet[str]:\n        \"\"\"Return the set of members defined in a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, get_protocol_members\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> get_protocol_members(P)\n            frozenset({'a', 'b'})\n\n        Raise a TypeError for arguments that are not Protocols.\n        \"\"\"\n        if not is_protocol(__tp):\n            raise TypeError(f'{__tp!r} is not a Protocol')\n        if hasattr(__tp, '__protocol_attrs__'):\n            return frozenset(__tp.__protocol_attrs__)\n        return frozenset(_get_protocol_attrs(__tp))\n\n\n# Aliases for items that have always been in typing.\n# Explicitly assign these (rather than using `from typing import *` at the top),\n# so that we get a CI error if one of these is deleted from typing.py\n# in a future version of Python\nAbstractSet = typing.AbstractSet\nAnyStr = typing.AnyStr\nBinaryIO = typing.BinaryIO\nCallable = typing.Callable\nCollection = typing.Collection\nContainer = typing.Container\nDict = typing.Dict\nForwardRef = typing.ForwardRef\nFrozenSet = typing.FrozenSet\nGenerator = typing.Generator\nGeneric = typing.Generic\nHashable = typing.Hashable\nIO = typing.IO\nItemsView = typing.ItemsView\nIterable = typing.Iterable\nIterator = typing.Iterator\nKeysView = typing.KeysView\nList = typing.List\nMapping = typing.Mapping\nMappingView = typing.MappingView\nMatch = typing.Match\nMutableMapping = typing.MutableMapping\nMutableSequence = typing.MutableSequence\nMutableSet = typing.MutableSet\nOptional = typing.Optional\nPattern = typing.Pattern\nReversible = typing.Reversible\nSequence = typing.Sequence\nSet = typing.Set\nSized = typing.Sized\nTextIO = typing.TextIO\nTuple = typing.Tuple\nUnion = typing.Union\nValuesView = typing.ValuesView\ncast = typing.cast\nno_type_check = typing.no_type_check\nno_type_check_decorator = typing.no_type_check_decorator"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/typing_extensions.py",
      "line_number": 1,
      "details": "import abc\nimport collections\nimport collections.abc\nimport functools\nimport inspect\nimport operator\nimport sys\nimport types as _types\nimport typing\nimport warnings\n\n__all__ = [\n    # Super-special typing primitives.\n    'Any',\n    'ClassVar',\n    'Concatenate',\n    'Final',\n    'LiteralString',\n    'ParamSpec',\n    'ParamSpecArgs',\n    'ParamSpecKwargs',\n    'Self',\n    'Type',\n    'TypeVar',\n    'TypeVarTuple',\n    'Unpack',\n\n    # ABCs (from collections.abc).\n    'Awaitable',\n    'AsyncIterator',\n    'AsyncIterable',\n    'Coroutine',\n    'AsyncGenerator',\n    'AsyncContextManager',\n    'Buffer',\n    'ChainMap',\n\n    # Concrete collection types.\n    'ContextManager',\n    'Counter',\n    'Deque',\n    'DefaultDict',\n    'NamedTuple',\n    'OrderedDict',\n    'TypedDict',\n\n    # Structural checks, a.k.a. protocols.\n    'SupportsAbs',\n    'SupportsBytes',\n    'SupportsComplex',\n    'SupportsFloat',\n    'SupportsIndex',\n    'SupportsInt',\n    'SupportsRound',\n\n    # One-off things.\n    'Annotated',\n    'assert_never',\n    'assert_type',\n    'clear_overloads',\n    'dataclass_transform',\n    'deprecated',\n    'get_overloads',\n    'final',\n    'get_args',\n    'get_origin',\n    'get_original_bases',\n    'get_protocol_members',\n    'get_type_hints',\n    'IntVar',\n    'is_protocol',\n    'is_typeddict',\n    'Literal',\n    'NewType',\n    'overload',\n    'override',\n    'Protocol',\n    'reveal_type',\n    'runtime',\n    'runtime_checkable',\n    'Text',\n    'TypeAlias',\n    'TypeAliasType',\n    'TypeGuard',\n    'TYPE_CHECKING',\n    'Never',\n    'NoReturn',\n    'Required',\n    'NotRequired',\n\n    # Pure aliases, have always been in typing\n    'AbstractSet',\n    'AnyStr',\n    'BinaryIO',\n    'Callable',\n    'Collection',\n    'Container',\n    'Dict',\n    'ForwardRef',\n    'FrozenSet',\n    'Generator',\n    'Generic',\n    'Hashable',\n    'IO',\n    'ItemsView',\n    'Iterable',\n    'Iterator',\n    'KeysView',\n    'List',\n    'Mapping',\n    'MappingView',\n    'Match',\n    'MutableMapping',\n    'MutableSequence',\n    'MutableSet',\n    'Optional',\n    'Pattern',\n    'Reversible',\n    'Sequence',\n    'Set',\n    'Sized',\n    'TextIO',\n    'Tuple',\n    'Union',\n    'ValuesView',\n    'cast',\n    'no_type_check',\n    'no_type_check_decorator',\n]\n\n# for backward compatibility\nPEP_560 = True\nGenericMeta = type\n\n# The functions below are modified copies of typing internal helpers.\n# They are needed by _ProtocolMeta and they provide support for PEP 646.\n\n\nclass _Sentinel:\n    def __repr__(self):\n        return \"<sentinel>\"\n\n\n_marker = _Sentinel()\n\n\ndef _check_generic(cls, parameters, elen=_marker):\n    \"\"\"Check correct count for parameters of a generic cls (internal helper).\n    This gives a nice error message in case of count mismatch.\n    \"\"\"\n    if not elen:\n        raise TypeError(f\"{cls} is not a generic class\")\n    if elen is _marker:\n        if not hasattr(cls, \"__parameters__\") or not cls.__parameters__:\n            raise TypeError(f\"{cls} is not a generic class\")\n        elen = len(cls.__parameters__)\n    alen = len(parameters)\n    if alen != elen:\n        if hasattr(cls, \"__parameters__\"):\n            parameters = [p for p in cls.__parameters__ if not _is_unpack(p)]\n            num_tv_tuples = sum(isinstance(p, TypeVarTuple) for p in parameters)\n            if (num_tv_tuples > 0) and (alen >= elen - num_tv_tuples):\n                return\n        raise TypeError(f\"Too {'many' if alen > elen else 'few'} parameters for {cls};\"\n                        f\" actual {alen}, expected {elen}\")\n\n\nif sys.version_info >= (3, 10):\n    def _should_collect_from_parameters(t):\n        return isinstance(\n            t, (typing._GenericAlias, _types.GenericAlias, _types.UnionType)\n        )\nelif sys.version_info >= (3, 9):\n    def _should_collect_from_parameters(t):\n        return isinstance(t, (typing._GenericAlias, _types.GenericAlias))\nelse:\n    def _should_collect_from_parameters(t):\n        return isinstance(t, typing._GenericAlias) and not t._special\n\n\ndef _collect_type_vars(types, typevar_types=None):\n    \"\"\"Collect all type variable contained in types in order of\n    first appearance (lexicographic order). For example::\n\n        _collect_type_vars((T, List[S, T])) == (T, S)\n    \"\"\"\n    if typevar_types is None:\n        typevar_types = typing.TypeVar\n    tvars = []\n    for t in types:\n        if (\n            isinstance(t, typevar_types) and\n            t not in tvars and\n            not _is_unpack(t)\n        ):\n            tvars.append(t)\n        if _should_collect_from_parameters(t):\n            tvars.extend([t for t in t.__parameters__ if t not in tvars])\n    return tuple(tvars)\n\n\nNoReturn = typing.NoReturn\n\n# Some unconstrained type variables.  These are used by the container types.\n# (These are not for export.)\nT = typing.TypeVar('T')  # Any type.\nKT = typing.TypeVar('KT')  # Key type.\nVT = typing.TypeVar('VT')  # Value type.\nT_co = typing.TypeVar('T_co', covariant=True)  # Any type covariant containers.\nT_contra = typing.TypeVar('T_contra', contravariant=True)  # Ditto contravariant.\n\n\nif sys.version_info >= (3, 11):\n    from typing import Any\nelse:\n\n    class _AnyMeta(type):\n        def __instancecheck__(self, obj):\n            if self is Any:\n                raise TypeError(\"typing_extensions.Any cannot be used with isinstance()\")\n            return super().__instancecheck__(obj)\n\n        def __repr__(self):\n            if self is Any:\n                return \"typing_extensions.Any\"\n            return super().__repr__()\n\n    class Any(metaclass=_AnyMeta):\n        \"\"\"Special type indicating an unconstrained type.\n        - Any is compatible with every type.\n        - Any assumed to have all methods.\n        - All values assumed to be instances of Any.\n        Note that all the above statements are true from the point of view of\n        static type checkers. At runtime, Any should not be used with instance\n        checks.\n        \"\"\"\n        def __new__(cls, *args, **kwargs):\n            if cls is Any:\n                raise TypeError(\"Any cannot be instantiated\")\n            return super().__new__(cls, *args, **kwargs)\n\n\nClassVar = typing.ClassVar\n\n\nclass _ExtensionsSpecialForm(typing._SpecialForm, _root=True):\n    def __repr__(self):\n        return 'typing_extensions.' + self._name\n\n\n# On older versions of typing there is an internal class named \"Final\".\n# 3.8+\nif hasattr(typing, 'Final') and sys.version_info[:2] >= (3, 7):\n    Final = typing.Final\n# 3.7\nelse:\n    class _FinalForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    Final = _FinalForm('Final',\n                       doc=\"\"\"A special typing construct to indicate that a name\n                       cannot be re-assigned or overridden in a subclass.\n                       For example:\n\n                           MAX_SIZE: Final = 9000\n                           MAX_SIZE += 1  # Error reported by type checker\n\n                           class Connection:\n                               TIMEOUT: Final[int] = 10\n                           class FastConnector(Connection):\n                               TIMEOUT = 1  # Error reported by type checker\n\n                       There is no runtime checking of these properties.\"\"\")\n\nif sys.version_info >= (3, 11):\n    final = typing.final\nelse:\n    # @final exists in 3.8+, but we backport it for all versions\n    # before 3.11 to keep support for the __final__ attribute.\n    # See https://bugs.python.org/issue46342\n    def final(f):\n        \"\"\"This decorator can be used to indicate to type checkers that\n        the decorated method cannot be overridden, and decorated class\n        cannot be subclassed. For example:\n\n            class Base:\n                @final\n                def done(self) -> None:\n                    ...\n            class Sub(Base):\n                def done(self) -> None:  # Error reported by type checker\n                    ...\n            @final\n            class Leaf:\n                ...\n            class Other(Leaf):  # Error reported by type checker\n                ...\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__final__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n        \"\"\"\n        try:\n            f.__final__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return f\n\n\ndef IntVar(name):\n    return typing.TypeVar(name)\n\n\n# A Literal bug was fixed in 3.11.0, 3.10.1 and 3.9.8\nif sys.version_info >= (3, 10, 1):\n    Literal = typing.Literal\nelse:\n    def _flatten_literal_params(parameters):\n        \"\"\"An internal helper for Literal creation: flatten Literals among parameters\"\"\"\n        params = []\n        for p in parameters:\n            if isinstance(p, _LiteralGenericAlias):\n                params.extend(p.__args__)\n            else:\n                params.append(p)\n        return tuple(params)\n\n    def _value_and_type_iter(params):\n        for p in params:\n            yield p, type(p)\n\n    class _LiteralGenericAlias(typing._GenericAlias, _root=True):\n        def __eq__(self, other):\n            if not isinstance(other, _LiteralGenericAlias):\n                return NotImplemented\n            these_args_deduped = set(_value_and_type_iter(self.__args__))\n            other_args_deduped = set(_value_and_type_iter(other.__args__))\n            return these_args_deduped == other_args_deduped\n\n        def __hash__(self):\n            return hash(frozenset(_value_and_type_iter(self.__args__)))\n\n    class _LiteralForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, doc: str):\n            self._name = 'Literal'\n            self._doc = self.__doc__ = doc\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n\n            parameters = _flatten_literal_params(parameters)\n\n            val_type_pairs = list(_value_and_type_iter(parameters))\n            try:\n                deduped_pairs = set(val_type_pairs)\n            except TypeError:\n                # unhashable parameters\n                pass\n            else:\n                # similar logic to typing._deduplicate on Python 3.9+\n                if len(deduped_pairs) < len(val_type_pairs):\n                    new_parameters = []\n                    for pair in val_type_pairs:\n                        if pair in deduped_pairs:\n                            new_parameters.append(pair[0])\n                            deduped_pairs.remove(pair)\n                    assert not deduped_pairs, deduped_pairs\n                    parameters = tuple(new_parameters)\n\n            return _LiteralGenericAlias(self, parameters)\n\n    Literal = _LiteralForm(doc=\"\"\"\\\n                           A type that can be used to indicate to type checkers\n                           that the corresponding value has a value literally equivalent\n                           to the provided parameter. For example:\n\n                               var: Literal[4] = 4\n\n                           The type checker understands that 'var' is literally equal to\n                           the value 4 and no other value.\n\n                           Literal[...] cannot be subclassed. There is no runtime\n                           checking verifying that the parameter is actually a value\n                           instead of a type.\"\"\")\n\n\n_overload_dummy = typing._overload_dummy\n\n\nif hasattr(typing, \"get_overloads\"):  # 3.11+\n    overload = typing.overload\n    get_overloads = typing.get_overloads\n    clear_overloads = typing.clear_overloads\nelse:\n    # {module: {qualname: {firstlineno: func}}}\n    _overload_registry = collections.defaultdict(\n        functools.partial(collections.defaultdict, dict)\n    )\n\n    def overload(func):\n        \"\"\"Decorator for overloaded functions/methods.\n\n        In a stub file, place two or more stub definitions for the same\n        function in a row, each decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n\n        In a non-stub file (i.e. a regular .py file), do the same but\n        follow it with an implementation.  The implementation should *not*\n        be decorated with @overload.  For example:\n\n        @overload\n        def utf8(value: None) -> None: ...\n        @overload\n        def utf8(value: bytes) -> bytes: ...\n        @overload\n        def utf8(value: str) -> bytes: ...\n        def utf8(value):\n            # implementation goes here\n\n        The overloads for a function can be retrieved at runtime using the\n        get_overloads() function.\n        \"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        try:\n            _overload_registry[f.__module__][f.__qualname__][\n                f.__code__.co_firstlineno\n            ] = func\n        except AttributeError:\n            # Not a normal function; ignore.\n            pass\n        return _overload_dummy\n\n    def get_overloads(func):\n        \"\"\"Return all defined overloads for *func* as a sequence.\"\"\"\n        # classmethod and staticmethod\n        f = getattr(func, \"__func__\", func)\n        if f.__module__ not in _overload_registry:\n            return []\n        mod_dict = _overload_registry[f.__module__]\n        if f.__qualname__ not in mod_dict:\n            return []\n        return list(mod_dict[f.__qualname__].values())\n\n    def clear_overloads():\n        \"\"\"Clear all overloads in the registry.\"\"\"\n        _overload_registry.clear()\n\n\n# This is not a real generic class.  Don't use outside annotations.\nType = typing.Type\n\n# Various ABCs mimicking those in collections.abc.\n# A few are simply re-exported for completeness.\n\n\nAwaitable = typing.Awaitable\nCoroutine = typing.Coroutine\nAsyncIterable = typing.AsyncIterable\nAsyncIterator = typing.AsyncIterator\nDeque = typing.Deque\nContextManager = typing.ContextManager\nAsyncContextManager = typing.AsyncContextManager\nDefaultDict = typing.DefaultDict\n\n# 3.7.2+\nif hasattr(typing, 'OrderedDict'):\n    OrderedDict = typing.OrderedDict\n# 3.7.0-3.7.2\nelse:\n    OrderedDict = typing._alias(collections.OrderedDict, (KT, VT))\n\nCounter = typing.Counter\nChainMap = typing.ChainMap\nAsyncGenerator = typing.AsyncGenerator\nText = typing.Text\nTYPE_CHECKING = typing.TYPE_CHECKING\n\n\n_PROTO_ALLOWLIST = {\n    'collections.abc': [\n        'Callable', 'Awaitable', 'Iterable', 'Iterator', 'AsyncIterable',\n        'Hashable', 'Sized', 'Container', 'Collection', 'Reversible', 'Buffer',\n    ],\n    'contextlib': ['AbstractContextManager', 'AbstractAsyncContextManager'],\n    'typing_extensions': ['Buffer'],\n}\n\n\n_EXCLUDED_ATTRS = {\n    \"__abstractmethods__\", \"__annotations__\", \"__weakref__\", \"_is_protocol\",\n    \"_is_runtime_protocol\", \"__dict__\", \"__slots__\", \"__parameters__\",\n    \"__orig_bases__\", \"__module__\", \"_MutableMapping__marker\", \"__doc__\",\n    \"__subclasshook__\", \"__orig_class__\", \"__init__\", \"__new__\",\n    \"__protocol_attrs__\", \"__callable_proto_members_only__\",\n}\n\nif sys.version_info < (3, 8):\n    _EXCLUDED_ATTRS |= {\n        \"_gorg\", \"__next_in_mro__\", \"__extra__\", \"__tree_hash__\", \"__args__\",\n        \"__origin__\"\n    }\n\nif sys.version_info >= (3, 9):\n    _EXCLUDED_ATTRS.add(\"__class_getitem__\")\n\nif sys.version_info >= (3, 12):\n    _EXCLUDED_ATTRS.add(\"__type_params__\")\n\n_EXCLUDED_ATTRS = frozenset(_EXCLUDED_ATTRS)\n\n\ndef _get_protocol_attrs(cls):\n    attrs = set()\n    for base in cls.__mro__[:-1]:  # without object\n        if base.__name__ in {'Protocol', 'Generic'}:\n            continue\n        annotations = getattr(base, '__annotations__', {})\n        for attr in (*base.__dict__, *annotations):\n            if (not attr.startswith('_abc_') and attr not in _EXCLUDED_ATTRS):\n                attrs.add(attr)\n    return attrs\n\n\ndef _maybe_adjust_parameters(cls):\n    \"\"\"Helper function used in Protocol.__init_subclass__ and _TypedDictMeta.__new__.\n\n    The contents of this function are very similar\n    to logic found in typing.Generic.__init_subclass__\n    on the CPython main branch.\n    \"\"\"\n    tvars = []\n    if '__orig_bases__' in cls.__dict__:\n        tvars = _collect_type_vars(cls.__orig_bases__)\n        # Look for Generic[T1, ..., Tn] or Protocol[T1, ..., Tn].\n        # If found, tvars must be a subset of it.\n        # If not found, tvars is it.\n        # Also check for and reject plain Generic,\n        # and reject multiple Generic[...] and/or Protocol[...].\n        gvars = None\n        for base in cls.__orig_bases__:\n            if (isinstance(base, typing._GenericAlias) and\n                    base.__origin__ in (typing.Generic, Protocol)):\n                # for error messages\n                the_base = base.__origin__.__name__\n                if gvars is not None:\n                    raise TypeError(\n                        \"Cannot inherit from Generic[...]\"\n                        \" and/or Protocol[...] multiple types.\")\n                gvars = base.__parameters__\n        if gvars is None:\n            gvars = tvars\n        else:\n            tvarset = set(tvars)\n            gvarset = set(gvars)\n            if not tvarset <= gvarset:\n                s_vars = ', '.join(str(t) for t in tvars if t not in gvarset)\n                s_args = ', '.join(str(g) for g in gvars)\n                raise TypeError(f\"Some type variables ({s_vars}) are\"\n                                f\" not listed in {the_base}[{s_args}]\")\n            tvars = gvars\n    cls.__parameters__ = tuple(tvars)\n\n\ndef _caller(depth=2):\n    try:\n        return sys._getframe(depth).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):  # For platforms without _getframe()\n        return None\n\n\n# The performance of runtime-checkable protocols is significantly improved on Python 3.12,\n# so we backport the 3.12 version of Protocol to Python <=3.11\nif sys.version_info >= (3, 12):\n    Protocol = typing.Protocol\nelse:\n    def _allow_reckless_class_checks(depth=3):\n        \"\"\"Allow instance and class checks for special stdlib modules.\n        The abc and functools modules indiscriminately call isinstance() and\n        issubclass() on the whole MRO of a user class, which may contain protocols.\n        \"\"\"\n        return _caller(depth) in {'abc', 'functools', None}\n\n    def _no_init(self, *args, **kwargs):\n        if type(self)._is_protocol:\n            raise TypeError('Protocols cannot be instantiated')\n\n    if sys.version_info >= (3, 8):\n        # Inheriting from typing._ProtocolMeta isn't actually desirable,\n        # but is necessary to allow typing.Protocol and typing_extensions.Protocol\n        # to mix without getting TypeErrors about \"metaclass conflict\"\n        _typing_Protocol = typing.Protocol\n        _ProtocolMetaBase = type(_typing_Protocol)\n    else:\n        _typing_Protocol = _marker\n        _ProtocolMetaBase = abc.ABCMeta\n\n    class _ProtocolMeta(_ProtocolMetaBase):\n        # This metaclass is somewhat unfortunate,\n        # but is necessary for several reasons...\n        #\n        # NOTE: DO NOT call super() in any methods in this class\n        # That would call the methods on typing._ProtocolMeta on Python 3.8-3.11\n        # and those are slow\n        def __new__(mcls, name, bases, namespace, **kwargs):\n            if name == \"Protocol\" and len(bases) < 2:\n                pass\n            elif {Protocol, _typing_Protocol} & set(bases):\n                for base in bases:\n                    if not (\n                        base in {object, typing.Generic, Protocol, _typing_Protocol}\n                        or base.__name__ in _PROTO_ALLOWLIST.get(base.__module__, [])\n                        or is_protocol(base)\n                    ):\n                        raise TypeError(\n                            f\"Protocols can only inherit from other protocols, \"\n                            f\"got {base!r}\"\n                        )\n            return abc.ABCMeta.__new__(mcls, name, bases, namespace, **kwargs)\n\n        def __init__(cls, *args, **kwargs):\n            abc.ABCMeta.__init__(cls, *args, **kwargs)\n            if getattr(cls, \"_is_protocol\", False):\n                cls.__protocol_attrs__ = _get_protocol_attrs(cls)\n                # PEP 544 prohibits using issubclass()\n                # with protocols that have non-method members.\n                cls.__callable_proto_members_only__ = all(\n                    callable(getattr(cls, attr, None)) for attr in cls.__protocol_attrs__\n                )\n\n        def __subclasscheck__(cls, other):\n            if cls is Protocol:\n                return type.__subclasscheck__(cls, other)\n            if (\n                getattr(cls, '_is_protocol', False)\n                and not _allow_reckless_class_checks()\n            ):\n                if not isinstance(other, type):\n                    # Same error message as for issubclass(1, int).\n                    raise TypeError('issubclass() arg 1 must be a class')\n                if (\n                    not cls.__callable_proto_members_only__\n                    and cls.__dict__.get(\"__subclasshook__\") is _proto_hook\n                ):\n                    raise TypeError(\n                        \"Protocols with non-method members don't support issubclass()\"\n                    )\n                if not getattr(cls, '_is_runtime_protocol', False):\n                    raise TypeError(\n                        \"Instance and class checks can only be used with \"\n                        \"@runtime_checkable protocols\"\n                    )\n            return abc.ABCMeta.__subclasscheck__(cls, other)\n\n        def __instancecheck__(cls, instance):\n            # We need this method for situations where attributes are\n            # assigned in __init__.\n            if cls is Protocol:\n                return type.__instancecheck__(cls, instance)\n            if not getattr(cls, \"_is_protocol\", False):\n                # i.e., it's a concrete subclass of a protocol\n                return abc.ABCMeta.__instancecheck__(cls, instance)\n\n            if (\n                not getattr(cls, '_is_runtime_protocol', False) and\n                not _allow_reckless_class_checks()\n            ):\n                raise TypeError(\"Instance and class checks can only be used with\"\n                                \" @runtime_checkable protocols\")\n\n            if abc.ABCMeta.__instancecheck__(cls, instance):\n                return True\n\n            for attr in cls.__protocol_attrs__:\n                try:\n                    val = inspect.getattr_static(instance, attr)\n                except AttributeError:\n                    break\n                if val is None and callable(getattr(cls, attr, None)):\n                    break\n            else:\n                return True\n\n            return False\n\n        def __eq__(cls, other):\n            # Hack so that typing.Generic.__class_getitem__\n            # treats typing_extensions.Protocol\n            # as equivalent to typing.Protocol on Python 3.8+\n            if abc.ABCMeta.__eq__(cls, other) is True:\n                return True\n            return (\n                cls is Protocol and other is getattr(typing, \"Protocol\", object())\n            )\n\n        # This has to be defined, or the abc-module cache\n        # complains about classes with this metaclass being unhashable,\n        # if we define only __eq__!\n        def __hash__(cls) -> int:\n            return type.__hash__(cls)\n\n    @classmethod\n    def _proto_hook(cls, other):\n        if not cls.__dict__.get('_is_protocol', False):\n            return NotImplemented\n\n        for attr in cls.__protocol_attrs__:\n            for base in other.__mro__:\n                # Check if the members appears in the class dictionary...\n                if attr in base.__dict__:\n                    if base.__dict__[attr] is None:\n                        return NotImplemented\n                    break\n\n                # ...or in annotations, if it is a sub-protocol.\n                annotations = getattr(base, '__annotations__', {})\n                if (\n                    isinstance(annotations, collections.abc.Mapping)\n                    and attr in annotations\n                    and is_protocol(other)\n                ):\n                    break\n            else:\n                return NotImplemented\n        return True\n\n    if sys.version_info >= (3, 8):\n        class Protocol(typing.Generic, metaclass=_ProtocolMeta):\n            __doc__ = typing.Protocol.__doc__\n            __slots__ = ()\n            _is_protocol = True\n            _is_runtime_protocol = False\n\n            def __init_subclass__(cls, *args, **kwargs):\n                super().__init_subclass__(*args, **kwargs)\n\n                # Determine if this is a protocol or a concrete subclass.\n                if not cls.__dict__.get('_is_protocol', False):\n                    cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n                # Set (or override) the protocol subclass hook.\n                if '__subclasshook__' not in cls.__dict__:\n                    cls.__subclasshook__ = _proto_hook\n\n                # Prohibit instantiation for protocol classes\n                if cls._is_protocol and cls.__init__ is Protocol.__init__:\n                    cls.__init__ = _no_init\n\n    else:\n        class Protocol(metaclass=_ProtocolMeta):\n            # There is quite a lot of overlapping code with typing.Generic.\n            # Unfortunately it is hard to avoid this on Python <3.8,\n            # as the typing module on Python 3.7 doesn't let us subclass typing.Generic!\n            \"\"\"Base class for protocol classes. Protocol classes are defined as::\n\n                class Proto(Protocol):\n                    def meth(self) -> int:\n                        ...\n\n            Such classes are primarily used with static type checkers that recognize\n            structural subtyping (static duck-typing), for example::\n\n                class C:\n                    def meth(self) -> int:\n                        return 0\n\n                def func(x: Proto) -> int:\n                    return x.meth()\n\n                func(C())  # Passes static type check\n\n            See PEP 544 for details. Protocol classes decorated with\n            @typing_extensions.runtime_checkable act\n            as simple-minded runtime-checkable protocols that check\n            only the presence of given attributes, ignoring their type signatures.\n\n            Protocol classes can be generic, they are defined as::\n\n                class GenProto(Protocol[T]):\n                    def meth(self) -> T:\n                        ...\n            \"\"\"\n            __slots__ = ()\n            _is_protocol = True\n            _is_runtime_protocol = False\n\n            def __new__(cls, *args, **kwds):\n                if cls is Protocol:\n                    raise TypeError(\"Type Protocol cannot be instantiated; \"\n                                    \"it can only be used as a base class\")\n                return super().__new__(cls)\n\n            @typing._tp_cache\n            def __class_getitem__(cls, params):\n                if not isinstance(params, tuple):\n                    params = (params,)\n                if not params and cls is not typing.Tuple:\n                    raise TypeError(\n                        f\"Parameter list to {cls.__qualname__}[...] cannot be empty\")\n                msg = \"Parameters to generic types must be types.\"\n                params = tuple(typing._type_check(p, msg) for p in params)\n                if cls is Protocol:\n                    # Generic can only be subscripted with unique type variables.\n                    if not all(isinstance(p, typing.TypeVar) for p in params):\n                        i = 0\n                        while isinstance(params[i], typing.TypeVar):\n                            i += 1\n                        raise TypeError(\n                            \"Parameters to Protocol[...] must all be type variables.\"\n                            f\" Parameter {i + 1} is {params[i]}\")\n                    if len(set(params)) != len(params):\n                        raise TypeError(\n                            \"Parameters to Protocol[...] must all be unique\")\n                else:\n                    # Subscripting a regular Generic subclass.\n                    _check_generic(cls, params, len(cls.__parameters__))\n                return typing._GenericAlias(cls, params)\n\n            def __init_subclass__(cls, *args, **kwargs):\n                if '__orig_bases__' in cls.__dict__:\n                    error = typing.Generic in cls.__orig_bases__\n                else:\n                    error = typing.Generic in cls.__bases__\n                if error:\n                    raise TypeError(\"Cannot inherit from plain Generic\")\n                _maybe_adjust_parameters(cls)\n\n                # Determine if this is a protocol or a concrete subclass.\n                if not cls.__dict__.get('_is_protocol', None):\n                    cls._is_protocol = any(b is Protocol for b in cls.__bases__)\n\n                # Set (or override) the protocol subclass hook.\n                if '__subclasshook__' not in cls.__dict__:\n                    cls.__subclasshook__ = _proto_hook\n\n                # Prohibit instantiation for protocol classes\n                if cls._is_protocol and cls.__init__ is Protocol.__init__:\n                    cls.__init__ = _no_init\n\n\nif sys.version_info >= (3, 8):\n    runtime_checkable = typing.runtime_checkable\nelse:\n    def runtime_checkable(cls):\n        \"\"\"Mark a protocol class as a runtime protocol, so that it\n        can be used with isinstance() and issubclass(). Raise TypeError\n        if applied to a non-protocol class.\n\n        This allows a simple-minded structural check very similar to the\n        one-offs in collections.abc such as Hashable.\n        \"\"\"\n        if not (\n            (isinstance(cls, _ProtocolMeta) or issubclass(cls, typing.Generic))\n            and getattr(cls, \"_is_protocol\", False)\n        ):\n            raise TypeError('@runtime_checkable can be only applied to protocol classes,'\n                            f' got {cls!r}')\n        cls._is_runtime_protocol = True\n        return cls\n\n\n# Exists for backwards compatibility.\nruntime = runtime_checkable\n\n\n# Our version of runtime-checkable protocols is faster on Python 3.7-3.11\nif sys.version_info >= (3, 12):\n    SupportsInt = typing.SupportsInt\n    SupportsFloat = typing.SupportsFloat\n    SupportsComplex = typing.SupportsComplex\n    SupportsBytes = typing.SupportsBytes\n    SupportsIndex = typing.SupportsIndex\n    SupportsAbs = typing.SupportsAbs\n    SupportsRound = typing.SupportsRound\nelse:\n    @runtime_checkable\n    class SupportsInt(Protocol):\n        \"\"\"An ABC with one abstract method __int__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __int__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsFloat(Protocol):\n        \"\"\"An ABC with one abstract method __float__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __float__(self) -> float:\n            pass\n\n    @runtime_checkable\n    class SupportsComplex(Protocol):\n        \"\"\"An ABC with one abstract method __complex__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __complex__(self) -> complex:\n            pass\n\n    @runtime_checkable\n    class SupportsBytes(Protocol):\n        \"\"\"An ABC with one abstract method __bytes__.\"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __bytes__(self) -> bytes:\n            pass\n\n    @runtime_checkable\n    class SupportsIndex(Protocol):\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __index__(self) -> int:\n            pass\n\n    @runtime_checkable\n    class SupportsAbs(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __abs__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __abs__(self) -> T_co:\n            pass\n\n    @runtime_checkable\n    class SupportsRound(Protocol[T_co]):\n        \"\"\"\n        An ABC with one abstract method __round__ that is covariant in its return type.\n        \"\"\"\n        __slots__ = ()\n\n        @abc.abstractmethod\n        def __round__(self, ndigits: int = 0) -> T_co:\n            pass\n\n\ndef _ensure_subclassable(mro_entries):\n    def inner(func):\n        if sys.implementation.name == \"pypy\" and sys.version_info < (3, 9):\n            cls_dict = {\n                \"__call__\": staticmethod(func),\n                \"__mro_entries__\": staticmethod(mro_entries)\n            }\n            t = type(func.__name__, (), cls_dict)\n            return functools.update_wrapper(t(), func)\n        else:\n            func.__mro_entries__ = mro_entries\n            return func\n    return inner\n\n\nif sys.version_info >= (3, 13):\n    # The standard library TypedDict in Python 3.8 does not store runtime information\n    # about which (if any) keys are optional.  See https://bugs.python.org/issue38834\n    # The standard library TypedDict in Python 3.9.0/1 does not honour the \"total\"\n    # keyword with old-style TypedDict().  See https://bugs.python.org/issue42059\n    # The standard library TypedDict below Python 3.11 does not store runtime\n    # information about optional and required keys when using Required or NotRequired.\n    # Generic TypedDicts are also impossible using typing.TypedDict on Python <3.11.\n    # Aaaand on 3.12 we add __orig_bases__ to TypedDict\n    # to enable better runtime introspection.\n    # On 3.13 we deprecate some odd ways of creating TypedDicts.\n    TypedDict = typing.TypedDict\n    _TypedDictMeta = typing._TypedDictMeta\n    is_typeddict = typing.is_typeddict\nelse:\n    # 3.10.0 and later\n    _TAKES_MODULE = \"module\" in inspect.signature(typing._type_check).parameters\n\n    if sys.version_info >= (3, 8):\n        _fake_name = \"Protocol\"\n    else:\n        _fake_name = \"_Protocol\"\n\n    class _TypedDictMeta(type):\n        def __new__(cls, name, bases, ns, total=True):\n            \"\"\"Create new typed dict class object.\n\n            This method is called when TypedDict is subclassed,\n            or when TypedDict is instantiated. This way\n            TypedDict supports all three syntax forms described in its docstring.\n            Subclasses and instances of TypedDict return actual dictionaries.\n            \"\"\"\n            for base in bases:\n                if type(base) is not _TypedDictMeta and base is not typing.Generic:\n                    raise TypeError('cannot inherit from both a TypedDict type '\n                                    'and a non-TypedDict base class')\n\n            if any(issubclass(b, typing.Generic) for b in bases):\n                generic_base = (typing.Generic,)\n            else:\n                generic_base = ()\n\n            # typing.py generally doesn't let you inherit from plain Generic, unless\n            # the name of the class happens to be \"Protocol\" (or \"_Protocol\" on 3.7).\n            tp_dict = type.__new__(_TypedDictMeta, _fake_name, (*generic_base, dict), ns)\n            tp_dict.__name__ = name\n            if tp_dict.__qualname__ == _fake_name:\n                tp_dict.__qualname__ = name\n\n            if not hasattr(tp_dict, '__orig_bases__'):\n                tp_dict.__orig_bases__ = bases\n\n            annotations = {}\n            own_annotations = ns.get('__annotations__', {})\n            msg = \"TypedDict('Name', {f0: t0, f1: t1, ...}); each t must be a type\"\n            if _TAKES_MODULE:\n                own_annotations = {\n                    n: typing._type_check(tp, msg, module=tp_dict.__module__)\n                    for n, tp in own_annotations.items()\n                }\n            else:\n                own_annotations = {\n                    n: typing._type_check(tp, msg)\n                    for n, tp in own_annotations.items()\n                }\n            required_keys = set()\n            optional_keys = set()\n\n            for base in bases:\n                annotations.update(base.__dict__.get('__annotations__', {}))\n                required_keys.update(base.__dict__.get('__required_keys__', ()))\n                optional_keys.update(base.__dict__.get('__optional_keys__', ()))\n\n            annotations.update(own_annotations)\n            for annotation_key, annotation_type in own_annotations.items():\n                annotation_origin = get_origin(annotation_type)\n                if annotation_origin is Annotated:\n                    annotation_args = get_args(annotation_type)\n                    if annotation_args:\n                        annotation_type = annotation_args[0]\n                        annotation_origin = get_origin(annotation_type)\n\n                if annotation_origin is Required:\n                    required_keys.add(annotation_key)\n                elif annotation_origin is NotRequired:\n                    optional_keys.add(annotation_key)\n                elif total:\n                    required_keys.add(annotation_key)\n                else:\n                    optional_keys.add(annotation_key)\n\n            tp_dict.__annotations__ = annotations\n            tp_dict.__required_keys__ = frozenset(required_keys)\n            tp_dict.__optional_keys__ = frozenset(optional_keys)\n            if not hasattr(tp_dict, '__total__'):\n                tp_dict.__total__ = total\n            return tp_dict\n\n        __call__ = dict  # static method\n\n        def __subclasscheck__(cls, other):\n            # Typed dicts are only for static structural subtyping.\n            raise TypeError('TypedDict does not support instance and class checks')\n\n        __instancecheck__ = __subclasscheck__\n\n    _TypedDict = type.__new__(_TypedDictMeta, 'TypedDict', (), {})\n\n    @_ensure_subclassable(lambda bases: (_TypedDict,))\n    def TypedDict(__typename, __fields=_marker, *, total=True, **kwargs):\n        \"\"\"A simple typed namespace. At runtime it is equivalent to a plain dict.\n\n        TypedDict creates a dictionary type such that a type checker will expect all\n        instances to have a certain set of keys, where each key is\n        associated with a value of a consistent type. This expectation\n        is not checked at runtime.\n\n        Usage::\n\n            class Point2D(TypedDict):\n                x: int\n                y: int\n                label: str\n\n            a: Point2D = {'x': 1, 'y': 2, 'label': 'good'}  # OK\n            b: Point2D = {'z': 3, 'label': 'bad'}           # Fails type check\n\n            assert Point2D(x=1, y=2, label='first') == dict(x=1, y=2, label='first')\n\n        The type info can be accessed via the Point2D.__annotations__ dict, and\n        the Point2D.__required_keys__ and Point2D.__optional_keys__ frozensets.\n        TypedDict supports an additional equivalent form::\n\n            Point2D = TypedDict('Point2D', {'x': int, 'y': int, 'label': str})\n\n        By default, all keys must be present in a TypedDict. It is possible\n        to override this by specifying totality::\n\n            class Point2D(TypedDict, total=False):\n                x: int\n                y: int\n\n        This means that a Point2D TypedDict can have any of the keys omitted. A type\n        checker is only expected to support a literal False or True as the value of\n        the total argument. True is the default, and makes all items defined in the\n        class body be required.\n\n        The Required and NotRequired special forms can also be used to mark\n        individual keys as being required or not required::\n\n            class Point2D(TypedDict):\n                x: int  # the \"x\" key must always be present (Required is the default)\n                y: NotRequired[int]  # the \"y\" key can be omitted\n\n        See PEP 655 for more details on Required and NotRequired.\n        \"\"\"\n        if __fields is _marker or __fields is None:\n            if __fields is _marker:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n\n            example = f\"`{__typename} = TypedDict({__typename!r}, {{}})`\"\n            deprecation_msg = (\n                f\"{deprecated_thing} is deprecated and will be disallowed in \"\n                \"Python 3.15. To create a TypedDict class with 0 fields \"\n                \"using the functional syntax, pass an empty dictionary, e.g. \"\n            ) + example + \".\"\n            warnings.warn(deprecation_msg, DeprecationWarning, stacklevel=2)\n            __fields = kwargs\n        elif kwargs:\n            raise TypeError(\"TypedDict takes either a dict or keyword arguments,\"\n                            \" but not both\")\n        if kwargs:\n            warnings.warn(\n                \"The kwargs-based syntax for TypedDict definitions is deprecated \"\n                \"in Python 3.11, will be removed in Python 3.13, and may not be \"\n                \"understood by third-party type checkers.\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        ns = {'__annotations__': dict(__fields)}\n        module = _caller()\n        if module is not None:\n            # Setting correct module is necessary to make typed dict classes pickleable.\n            ns['__module__'] = module\n\n        td = _TypedDictMeta(__typename, (), ns, total=total)\n        td.__orig_bases__ = (TypedDict,)\n        return td\n\n    if hasattr(typing, \"_TypedDictMeta\"):\n        _TYPEDDICT_TYPES = (typing._TypedDictMeta, _TypedDictMeta)\n    else:\n        _TYPEDDICT_TYPES = (_TypedDictMeta,)\n\n    def is_typeddict(tp):\n        \"\"\"Check if an annotation is a TypedDict class\n\n        For example::\n            class Film(TypedDict):\n                title: str\n                year: int\n\n            is_typeddict(Film)  # => True\n            is_typeddict(Union[list, str])  # => False\n        \"\"\"\n        # On 3.8, this would otherwise return True\n        if hasattr(typing, \"TypedDict\") and tp is typing.TypedDict:\n            return False\n        return isinstance(tp, _TYPEDDICT_TYPES)\n\n\nif hasattr(typing, \"assert_type\"):\n    assert_type = typing.assert_type\n\nelse:\n    def assert_type(__val, __typ):\n        \"\"\"Assert (to the type checker) that the value is of the given type.\n\n        When the type checker encounters a call to assert_type(), it\n        emits an error if the value is not of the specified type::\n\n            def greet(name: str) -> None:\n                assert_type(name, str)  # ok\n                assert_type(name, int)  # type checker error\n\n        At runtime this returns the first argument unchanged and otherwise\n        does nothing.\n        \"\"\"\n        return __val\n\n\nif hasattr(typing, \"Required\"):\n    get_type_hints = typing.get_type_hints\nelse:\n    # replaces _strip_annotations()\n    def _strip_extras(t):\n        \"\"\"Strips Annotated, Required and NotRequired from a given type.\"\"\"\n        if isinstance(t, _AnnotatedAlias):\n            return _strip_extras(t.__origin__)\n        if hasattr(t, \"__origin__\") and t.__origin__ in (Required, NotRequired):\n            return _strip_extras(t.__args__[0])\n        if isinstance(t, typing._GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return t.copy_with(stripped_args)\n        if hasattr(_types, \"GenericAlias\") and isinstance(t, _types.GenericAlias):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return _types.GenericAlias(t.__origin__, stripped_args)\n        if hasattr(_types, \"UnionType\") and isinstance(t, _types.UnionType):\n            stripped_args = tuple(_strip_extras(a) for a in t.__args__)\n            if stripped_args == t.__args__:\n                return t\n            return functools.reduce(operator.or_, stripped_args)\n\n        return t\n\n    def get_type_hints(obj, globalns=None, localns=None, include_extras=False):\n        \"\"\"Return type hints for an object.\n\n        This is often the same as obj.__annotations__, but it handles\n        forward references encoded as string literals, adds Optional[t] if a\n        default value equal to None is set and recursively replaces all\n        'Annotated[T, ...]', 'Required[T]' or 'NotRequired[T]' with 'T'\n        (unless 'include_extras=True').\n\n        The argument may be a module, class, method, or function. The annotations\n        are returned as a dictionary. For classes, annotations include also\n        inherited members.\n\n        TypeError is raised if the argument is not of a type that can contain\n        annotations, and an empty dictionary is returned if no annotations are\n        present.\n\n        BEWARE -- the behavior of globalns and localns is counterintuitive\n        (unless you are familiar with how eval() and exec() work).  The\n        search order is locals first, then globals.\n\n        - If no dict arguments are passed, an attempt is made to use the\n          globals from obj (or the respective module's globals for classes),\n          and these are also used as the locals.  If the object does not appear\n          to have globals, an empty dictionary is used.\n\n        - If one dict argument is passed, it is used for both globals and\n          locals.\n\n        - If two dict arguments are passed, they specify globals and\n          locals, respectively.\n        \"\"\"\n        if hasattr(typing, \"Annotated\"):\n            hint = typing.get_type_hints(\n                obj, globalns=globalns, localns=localns, include_extras=True\n            )\n        else:\n            hint = typing.get_type_hints(obj, globalns=globalns, localns=localns)\n        if include_extras:\n            return hint\n        return {k: _strip_extras(t) for k, t in hint.items()}\n\n\n# Python 3.9+ has PEP 593 (Annotated)\nif hasattr(typing, 'Annotated'):\n    Annotated = typing.Annotated\n    # Not exported and not a public API, but needed for get_origin() and get_args()\n    # to work.\n    _AnnotatedAlias = typing._AnnotatedAlias\n# 3.7-3.8\nelse:\n    class _AnnotatedAlias(typing._GenericAlias, _root=True):\n        \"\"\"Runtime representation of an annotated type.\n\n        At its core 'Annotated[t, dec1, dec2, ...]' is an alias for the type 't'\n        with extra annotations. The alias behaves like a normal typing alias,\n        instantiating is the same as instantiating the underlying type, binding\n        it to types is also the same.\n        \"\"\"\n        def __init__(self, origin, metadata):\n            if isinstance(origin, _AnnotatedAlias):\n                metadata = origin.__metadata__ + metadata\n                origin = origin.__origin__\n            super().__init__(origin, origin)\n            self.__metadata__ = metadata\n\n        def copy_with(self, params):\n            assert len(params) == 1\n            new_type = params[0]\n            return _AnnotatedAlias(new_type, self.__metadata__)\n\n        def __repr__(self):\n            return (f\"typing_extensions.Annotated[{typing._type_repr(self.__origin__)}, \"\n                    f\"{', '.join(repr(a) for a in self.__metadata__)}]\")\n\n        def __reduce__(self):\n            return operator.getitem, (\n                Annotated, (self.__origin__,) + self.__metadata__\n            )\n\n        def __eq__(self, other):\n            if not isinstance(other, _AnnotatedAlias):\n                return NotImplemented\n            if self.__origin__ != other.__origin__:\n                return False\n            return self.__metadata__ == other.__metadata__\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__metadata__))\n\n    class Annotated:\n        \"\"\"Add context specific metadata to a type.\n\n        Example: Annotated[int, runtime_check.Unsigned] indicates to the\n        hypothetical runtime_check module that this type is an unsigned int.\n        Every other consumer of this type can ignore this metadata and treat\n        this type as int.\n\n        The first argument to Annotated must be a valid type (and will be in\n        the __origin__ field), the remaining arguments are kept as a tuple in\n        the __extra__ field.\n\n        Details:\n\n        - It's an error to call `Annotated` with less than two arguments.\n        - Nested Annotated are flattened::\n\n            Annotated[Annotated[T, Ann1, Ann2], Ann3] == Annotated[T, Ann1, Ann2, Ann3]\n\n        - Instantiating an annotated type is equivalent to instantiating the\n        underlying type::\n\n            Annotated[C, Ann1](5) == C(5)\n\n        - Annotated can be used as a generic type alias::\n\n            Optimized = Annotated[T, runtime.Optimize()]\n            Optimized[int] == Annotated[int, runtime.Optimize()]\n\n            OptimizedList = Annotated[List[T], runtime.Optimize()]\n            OptimizedList[int] == Annotated[List[int], runtime.Optimize()]\n        \"\"\"\n\n        __slots__ = ()\n\n        def __new__(cls, *args, **kwargs):\n            raise TypeError(\"Type Annotated cannot be instantiated.\")\n\n        @typing._tp_cache\n        def __class_getitem__(cls, params):\n            if not isinstance(params, tuple) or len(params) < 2:\n                raise TypeError(\"Annotated[...] should be used \"\n                                \"with at least two arguments (a type and an \"\n                                \"annotation).\")\n            allowed_special_forms = (ClassVar, Final)\n            if get_origin(params[0]) in allowed_special_forms:\n                origin = params[0]\n            else:\n                msg = \"Annotated[t, ...]: t must be a type.\"\n                origin = typing._type_check(params[0], msg)\n            metadata = tuple(params[1:])\n            return _AnnotatedAlias(origin, metadata)\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                f\"Cannot subclass {cls.__module__}.Annotated\"\n            )\n\n# Python 3.8 has get_origin() and get_args() but those implementations aren't\n# Annotated-aware, so we can't use those. Python 3.9's versions don't support\n# ParamSpecArgs and ParamSpecKwargs, so only Python 3.10's versions will do.\nif sys.version_info[:2] >= (3, 10):\n    get_origin = typing.get_origin\n    get_args = typing.get_args\n# 3.7-3.9\nelse:\n    try:\n        # 3.9+\n        from typing import _BaseGenericAlias\n    except ImportError:\n        _BaseGenericAlias = typing._GenericAlias\n    try:\n        # 3.9+\n        from typing import GenericAlias as _typing_GenericAlias\n    except ImportError:\n        _typing_GenericAlias = typing._GenericAlias\n\n    def get_origin(tp):\n        \"\"\"Get the unsubscripted version of a type.\n\n        This supports generic types, Callable, Tuple, Union, Literal, Final, ClassVar\n        and Annotated. Return None for unsupported types. Examples::\n\n            get_origin(Literal[42]) is Literal\n            get_origin(int) is None\n            get_origin(ClassVar[int]) is ClassVar\n            get_origin(Generic) is Generic\n            get_origin(Generic[T]) is Generic\n            get_origin(Union[T, int]) is Union\n            get_origin(List[Tuple[T, T]][int]) == list\n            get_origin(P.args) is P\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return Annotated\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias, _BaseGenericAlias,\n                           ParamSpecArgs, ParamSpecKwargs)):\n            return tp.__origin__\n        if tp is typing.Generic:\n            return typing.Generic\n        return None\n\n    def get_args(tp):\n        \"\"\"Get type arguments with all substitutions performed.\n\n        For unions, basic simplifications used by Union constructor are performed.\n        Examples::\n            get_args(Dict[str, int]) == (str, int)\n            get_args(int) == ()\n            get_args(Union[int, Union[T, int], str][int]) == (int, str)\n            get_args(Union[int, Tuple[T, int]][str]) == (int, Tuple[str, int])\n            get_args(Callable[[], T][int]) == ([], int)\n        \"\"\"\n        if isinstance(tp, _AnnotatedAlias):\n            return (tp.__origin__,) + tp.__metadata__\n        if isinstance(tp, (typing._GenericAlias, _typing_GenericAlias)):\n            if getattr(tp, \"_special\", False):\n                return ()\n            res = tp.__args__\n            if get_origin(tp) is collections.abc.Callable and res[0] is not Ellipsis:\n                res = (list(res[:-1]), res[-1])\n            return res\n        return ()\n\n\n# 3.10+\nif hasattr(typing, 'TypeAlias'):\n    TypeAlias = typing.TypeAlias\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeAlias(self, parameters):\n        \"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example above.\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n# 3.7-3.8\nelse:\n    TypeAlias = _ExtensionsSpecialForm(\n        'TypeAlias',\n        doc=\"\"\"Special marker indicating that an assignment should\n        be recognized as a proper type alias definition by type\n        checkers.\n\n        For example::\n\n            Predicate: TypeAlias = Callable[..., bool]\n\n        It's invalid when used anywhere except as in the example\n        above.\"\"\"\n    )\n\n\ndef _set_default(type_param, default):\n    if isinstance(default, (tuple, list)):\n        type_param.__default__ = tuple((typing._type_check(d, \"Default must be a type\")\n                                        for d in default))\n    elif default != _marker:\n        type_param.__default__ = typing._type_check(default, \"Default must be a type\")\n    else:\n        type_param.__default__ = None\n\n\ndef _set_module(typevarlike):\n    # for pickling:\n    def_mod = _caller(depth=3)\n    if def_mod != 'typing_extensions':\n        typevarlike.__module__ = def_mod\n\n\nclass _DefaultMixin:\n    \"\"\"Mixin for TypeVarLike defaults.\"\"\"\n\n    __slots__ = ()\n    __init__ = _set_default\n\n\n# Classes using this metaclass must provide a _backported_typevarlike ClassVar\nclass _TypeVarLikeMeta(type):\n    def __instancecheck__(cls, __instance: Any) -> bool:\n        return isinstance(__instance, cls._backported_typevarlike)\n\n\n# Add default and infer_variance parameters from PEP 696 and 695\nclass TypeVar(metaclass=_TypeVarLikeMeta):\n    \"\"\"Type variable.\"\"\"\n\n    _backported_typevarlike = typing.TypeVar\n\n    def __new__(cls, name, *constraints, bound=None,\n                covariant=False, contravariant=False,\n                default=_marker, infer_variance=False):\n        if hasattr(typing, \"TypeAliasType\"):\n            # PEP 695 implemented, can pass infer_variance to typing.TypeVar\n            typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                     covariant=covariant, contravariant=contravariant,\n                                     infer_variance=infer_variance)\n        else:\n            typevar = typing.TypeVar(name, *constraints, bound=bound,\n                                     covariant=covariant, contravariant=contravariant)\n            if infer_variance and (covariant or contravariant):\n                raise ValueError(\"Variance cannot be specified with infer_variance.\")\n            typevar.__infer_variance__ = infer_variance\n        _set_default(typevar, default)\n        _set_module(typevar)\n        return typevar\n\n    def __init_subclass__(cls) -> None:\n        raise TypeError(f\"type '{__name__}.TypeVar' is not an acceptable base type\")\n\n\n# Python 3.10+ has PEP 612\nif hasattr(typing, 'ParamSpecArgs'):\n    ParamSpecArgs = typing.ParamSpecArgs\n    ParamSpecKwargs = typing.ParamSpecKwargs\n# 3.7-3.9\nelse:\n    class _Immutable:\n        \"\"\"Mixin to indicate that object should not be copied.\"\"\"\n        __slots__ = ()\n\n        def __copy__(self):\n            return self\n\n        def __deepcopy__(self, memo):\n            return self\n\n    class ParamSpecArgs(_Immutable):\n        \"\"\"The args for a ParamSpec object.\n\n        Given a ParamSpec object P, P.args is an instance of ParamSpecArgs.\n\n        ParamSpecArgs objects have a reference back to their ParamSpec:\n\n        P.args.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.args\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecArgs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n    class ParamSpecKwargs(_Immutable):\n        \"\"\"The kwargs for a ParamSpec object.\n\n        Given a ParamSpec object P, P.kwargs is an instance of ParamSpecKwargs.\n\n        ParamSpecKwargs objects have a reference back to their ParamSpec:\n\n        P.kwargs.__origin__ is P\n\n        This type is meant for runtime introspection and has no special meaning to\n        static type checkers.\n        \"\"\"\n        def __init__(self, origin):\n            self.__origin__ = origin\n\n        def __repr__(self):\n            return f\"{self.__origin__.__name__}.kwargs\"\n\n        def __eq__(self, other):\n            if not isinstance(other, ParamSpecKwargs):\n                return NotImplemented\n            return self.__origin__ == other.__origin__\n\n# 3.10+\nif hasattr(typing, 'ParamSpec'):\n\n    # Add default parameter - PEP 696\n    class ParamSpec(metaclass=_TypeVarLikeMeta):\n        \"\"\"Parameter specification.\"\"\"\n\n        _backported_typevarlike = typing.ParamSpec\n\n        def __new__(cls, name, *, bound=None,\n                    covariant=False, contravariant=False,\n                    infer_variance=False, default=_marker):\n            if hasattr(typing, \"TypeAliasType\"):\n                # PEP 695 implemented, can pass infer_variance to typing.TypeVar\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant,\n                                             infer_variance=infer_variance)\n            else:\n                paramspec = typing.ParamSpec(name, bound=bound,\n                                             covariant=covariant,\n                                             contravariant=contravariant)\n                paramspec.__infer_variance__ = infer_variance\n\n            _set_default(paramspec, default)\n            _set_module(paramspec)\n            return paramspec\n\n        def __init_subclass__(cls) -> None:\n            raise TypeError(f\"type '{__name__}.ParamSpec' is not an acceptable base type\")\n\n# 3.7-3.9\nelse:\n\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class ParamSpec(list, _DefaultMixin):\n        \"\"\"Parameter specification variable.\n\n        Usage::\n\n           P = ParamSpec('P')\n\n        Parameter specification variables exist primarily for the benefit of static\n        type checkers.  They are used to forward the parameter types of one\n        callable to another callable, a pattern commonly found in higher order\n        functions and decorators.  They are only valid when used in ``Concatenate``,\n        or s the first argument to ``Callable``. In Python 3.10 and higher,\n        they are also supported in user-defined Generics at runtime.\n        See class Generic for more information on generic types.  An\n        example for annotating a decorator::\n\n           T = TypeVar('T')\n           P = ParamSpec('P')\n\n           def add_logging(f: Callable[P, T]) -> Callable[P, T]:\n               '''A type-safe decorator to add logging to a function.'''\n               def inner(*args: P.args, **kwargs: P.kwargs) -> T:\n                   logging.info(f'{f.__name__} was called')\n                   return f(*args, **kwargs)\n               return inner\n\n           @add_logging\n           def add_two(x: float, y: float) -> float:\n               '''Add two numbers together.'''\n               return x + y\n\n        Parameter specification variables defined with covariant=True or\n        contravariant=True can be used to declare covariant or contravariant\n        generic types.  These keyword arguments are valid, but their actual semantics\n        are yet to be decided.  See PEP 612 for details.\n\n        Parameter specification variables can be introspected. e.g.:\n\n           P.__name__ == 'T'\n           P.__bound__ == None\n           P.__covariant__ == False\n           P.__contravariant__ == False\n\n        Note that only parameter specification variables defined in global scope can\n        be pickled.\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        @property\n        def args(self):\n            return ParamSpecArgs(self)\n\n        @property\n        def kwargs(self):\n            return ParamSpecKwargs(self)\n\n        def __init__(self, name, *, bound=None, covariant=False, contravariant=False,\n                     infer_variance=False, default=_marker):\n            super().__init__([self])\n            self.__name__ = name\n            self.__covariant__ = bool(covariant)\n            self.__contravariant__ = bool(contravariant)\n            self.__infer_variance__ = bool(infer_variance)\n            if bound:\n                self.__bound__ = typing._type_check(bound, 'Bound must be a type.')\n            else:\n                self.__bound__ = None\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __repr__(self):\n            if self.__infer_variance__:\n                prefix = ''\n            elif self.__covariant__:\n                prefix = '+'\n            elif self.__contravariant__:\n                prefix = '-'\n            else:\n                prefix = '~'\n            return prefix + self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        # Hack to get typing._type_check to pass.\n        def __call__(self, *args, **kwargs):\n            pass\n\n\n# 3.7-3.9\nif not hasattr(typing, 'Concatenate'):\n    # Inherits from list as a workaround for Callable checks in Python < 3.9.2.\n    class _ConcatenateGenericAlias(list):\n\n        # Trick Generic into looking into this for __parameters__.\n        __class__ = typing._GenericAlias\n\n        # Flag in 3.8.\n        _special = False\n\n        def __init__(self, origin, args):\n            super().__init__(args)\n            self.__origin__ = origin\n            self.__args__ = args\n\n        def __repr__(self):\n            _type_repr = typing._type_repr\n            return (f'{_type_repr(self.__origin__)}'\n                    f'[{\", \".join(_type_repr(arg) for arg in self.__args__)}]')\n\n        def __hash__(self):\n            return hash((self.__origin__, self.__args__))\n\n        # Hack to get typing._type_check to pass in Generic.\n        def __call__(self, *args, **kwargs):\n            pass\n\n        @property\n        def __parameters__(self):\n            return tuple(\n                tp for tp in self.__args__ if isinstance(tp, (typing.TypeVar, ParamSpec))\n            )\n\n\n# 3.7-3.9\n@typing._tp_cache\ndef _concatenate_getitem(self, parameters):\n    if parameters == ():\n        raise TypeError(\"Cannot take a Concatenate of no types.\")\n    if not isinstance(parameters, tuple):\n        parameters = (parameters,)\n    if not isinstance(parameters[-1], ParamSpec):\n        raise TypeError(\"The last parameter to Concatenate should be a \"\n                        \"ParamSpec variable.\")\n    msg = \"Concatenate[arg, ...]: each arg must be a type.\"\n    parameters = tuple(typing._type_check(p, msg) for p in parameters)\n    return _ConcatenateGenericAlias(self, parameters)\n\n\n# 3.10+\nif hasattr(typing, 'Concatenate'):\n    Concatenate = typing.Concatenate\n    _ConcatenateGenericAlias = typing._ConcatenateGenericAlias  # noqa: F811\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def Concatenate(self, parameters):\n        \"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\"\n        return _concatenate_getitem(self, parameters)\n# 3.7-8\nelse:\n    class _ConcatenateForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            return _concatenate_getitem(self, parameters)\n\n    Concatenate = _ConcatenateForm(\n        'Concatenate',\n        doc=\"\"\"Used in conjunction with ``ParamSpec`` and ``Callable`` to represent a\n        higher order function which adds, removes or transforms parameters of a\n        callable.\n\n        For example::\n\n           Callable[Concatenate[int, P], int]\n\n        See PEP 612 for detailed information.\n        \"\"\")\n\n# 3.10+\nif hasattr(typing, 'TypeGuard'):\n    TypeGuard = typing.TypeGuard\n# 3.9\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def TypeGuard(self, parameters):\n        \"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\"\n        item = typing._type_check(parameters, f'{self} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n# 3.7-3.8\nelse:\n    class _TypeGuardForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type')\n            return typing._GenericAlias(self, (item,))\n\n    TypeGuard = _TypeGuardForm(\n        'TypeGuard',\n        doc=\"\"\"Special typing form used to annotate the return type of a user-defined\n        type guard function.  ``TypeGuard`` only accepts a single type argument.\n        At runtime, functions marked this way should return a boolean.\n\n        ``TypeGuard`` aims to benefit *type narrowing* -- a technique used by static\n        type checkers to determine a more precise type of an expression within a\n        program's code flow.  Usually type narrowing is done by analyzing\n        conditional code flow and applying the narrowing to a block of code.  The\n        conditional expression here is sometimes referred to as a \"type guard\".\n\n        Sometimes it would be convenient to use a user-defined boolean function\n        as a type guard.  Such a function should use ``TypeGuard[...]`` as its\n        return type to alert static type checkers to this intention.\n\n        Using  ``-> TypeGuard`` tells the static type checker that for a given\n        function:\n\n        1. The return value is a boolean.\n        2. If the return value is ``True``, the type of its argument\n        is the type inside ``TypeGuard``.\n\n        For example::\n\n            def is_str(val: Union[str, float]):\n                # \"isinstance\" type guard\n                if isinstance(val, str):\n                    # Type of ``val`` is narrowed to ``str``\n                    ...\n                else:\n                    # Else, type of ``val`` is narrowed to ``float``.\n                    ...\n\n        Strict type narrowing is not enforced -- ``TypeB`` need not be a narrower\n        form of ``TypeA`` (it can even be a wider form) and this may lead to\n        type-unsafe results.  The main reason is to allow for things like\n        narrowing ``List[object]`` to ``List[str]`` even though the latter is not\n        a subtype of the former, since ``List`` is invariant.  The responsibility of\n        writing type-safe type guards is left to the user.\n\n        ``TypeGuard`` also works with type variables.  For more information, see\n        PEP 647 (User-Defined Type Guards).\n        \"\"\")\n\n\n# Vendored from cpython typing._SpecialFrom\nclass _SpecialForm(typing._Final, _root=True):\n    __slots__ = ('_name', '__doc__', '_getitem')\n\n    def __init__(self, getitem):\n        self._getitem = getitem\n        self._name = getitem.__name__\n        self.__doc__ = getitem.__doc__\n\n    def __getattr__(self, item):\n        if item in {'__name__', '__qualname__'}:\n            return self._name\n\n        raise AttributeError(item)\n\n    def __mro_entries__(self, bases):\n        raise TypeError(f\"Cannot subclass {self!r}\")\n\n    def __repr__(self):\n        return f'typing_extensions.{self._name}'\n\n    def __reduce__(self):\n        return self._name\n\n    def __call__(self, *args, **kwds):\n        raise TypeError(f\"Cannot instantiate {self!r}\")\n\n    def __or__(self, other):\n        return typing.Union[self, other]\n\n    def __ror__(self, other):\n        return typing.Union[other, self]\n\n    def __instancecheck__(self, obj):\n        raise TypeError(f\"{self} cannot be used with isinstance()\")\n\n    def __subclasscheck__(self, cls):\n        raise TypeError(f\"{self} cannot be used with issubclass()\")\n\n    @typing._tp_cache\n    def __getitem__(self, parameters):\n        return self._getitem(self, parameters)\n\n\nif hasattr(typing, \"LiteralString\"):\n    LiteralString = typing.LiteralString\nelse:\n    @_SpecialForm\n    def LiteralString(self, params):\n        \"\"\"Represents an arbitrary literal string.\n\n        Example::\n\n          from pip._vendor.typing_extensions import LiteralString\n\n          def query(sql: LiteralString) -> ...:\n              ...\n\n          query(\"SELECT * FROM table\")  # ok\n          query(f\"SELECT * FROM {input()}\")  # not ok\n\n        See PEP 675 for details.\n\n        \"\"\"\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Self\"):\n    Self = typing.Self\nelse:\n    @_SpecialForm\n    def Self(self, params):\n        \"\"\"Used to spell the type of \"self\" in classes.\n\n        Example::\n\n          from typing import Self\n\n          class ReturnsSelf:\n              def parse(self, data: bytes) -> Self:\n                  ...\n                  return self\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, \"Never\"):\n    Never = typing.Never\nelse:\n    @_SpecialForm\n    def Never(self, params):\n        \"\"\"The bottom type, a type that has no members.\n\n        This can be used to define a function that should never be\n        called, or a function that never returns::\n\n            from pip._vendor.typing_extensions import Never\n\n            def never_call_me(arg: Never) -> None:\n                pass\n\n            def int_or_str(arg: int | str) -> None:\n                never_call_me(arg)  # type checker error\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        never_call_me(arg)  # ok, arg is of type Never\n\n        \"\"\"\n\n        raise TypeError(f\"{self} is not subscriptable\")\n\n\nif hasattr(typing, 'Required'):\n    Required = typing.Required\n    NotRequired = typing.NotRequired\nelif sys.version_info[:2] >= (3, 9):\n    @_ExtensionsSpecialForm\n    def Required(self, parameters):\n        \"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\n    @_ExtensionsSpecialForm\n    def NotRequired(self, parameters):\n        \"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\"\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return typing._GenericAlias(self, (item,))\n\nelse:\n    class _RequiredForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return typing._GenericAlias(self, (item,))\n\n    Required = _RequiredForm(\n        'Required',\n        doc=\"\"\"A special typing construct to mark a key of a total=False TypedDict\n        as required. For example:\n\n            class Movie(TypedDict, total=False):\n                title: Required[str]\n                year: int\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n\n        There is no runtime checking that a required key is actually provided\n        when instantiating a related TypedDict.\n        \"\"\")\n    NotRequired = _RequiredForm(\n        'NotRequired',\n        doc=\"\"\"A special typing construct to mark a key of a TypedDict as\n        potentially missing. For example:\n\n            class Movie(TypedDict):\n                title: str\n                year: NotRequired[int]\n\n            m = Movie(\n                title='The Matrix',  # typechecker error if key is omitted\n                year=1999,\n            )\n        \"\"\")\n\n\n_UNPACK_DOC = \"\"\"\\\nType unpack operator.\n\nThe type unpack operator takes the child types from some container type,\nsuch as `tuple[int, str]` or a `TypeVarTuple`, and 'pulls them out'. For\nexample:\n\n  # For some generic class `Foo`:\n  Foo[Unpack[tuple[int, str]]]  # Equivalent to Foo[int, str]\n\n  Ts = TypeVarTuple('Ts')\n  # Specifies that `Bar` is generic in an arbitrary number of types.\n  # (Think of `Ts` as a tuple of an arbitrary number of individual\n  #  `TypeVar`s, which the `Unpack` is 'pulling out' directly into the\n  #  `Generic[]`.)\n  class Bar(Generic[Unpack[Ts]]): ...\n  Bar[int]  # Valid\n  Bar[int, str]  # Also valid\n\nFrom Python 3.11, this can also be done using the `*` operator:\n\n    Foo[*tuple[int, str]]\n    class Bar(Generic[*Ts]): ...\n\nThe operator can also be used along with a `TypedDict` to annotate\n`**kwargs` in a function signature. For instance:\n\n  class Movie(TypedDict):\n    name: str\n    year: int\n\n  # This function expects two keyword arguments - *name* of type `str` and\n  # *year* of type `int`.\n  def foo(**kwargs: Unpack[Movie]): ...\n\nNote that there is only some runtime checking of this operator. Not\neverything the runtime allows may be accepted by static type checkers.\n\nFor more information, see PEP 646 and PEP 692.\n\"\"\"\n\n\nif sys.version_info >= (3, 12):  # PEP 692 changed the repr of Unpack[]\n    Unpack = typing.Unpack\n\n    def _is_unpack(obj):\n        return get_origin(obj) is Unpack\n\nelif sys.version_info[:2] >= (3, 9):\n    class _UnpackSpecialForm(_ExtensionsSpecialForm, _root=True):\n        def __init__(self, getitem):\n            super().__init__(getitem)\n            self.__doc__ = _UNPACK_DOC\n\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    @_UnpackSpecialForm\n    def Unpack(self, parameters):\n        item = typing._type_check(parameters, f'{self._name} accepts only a single type.')\n        return _UnpackAlias(self, (item,))\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\nelse:\n    class _UnpackAlias(typing._GenericAlias, _root=True):\n        __class__ = typing.TypeVar\n\n    class _UnpackForm(_ExtensionsSpecialForm, _root=True):\n        def __getitem__(self, parameters):\n            item = typing._type_check(parameters,\n                                      f'{self._name} accepts only a single type.')\n            return _UnpackAlias(self, (item,))\n\n    Unpack = _UnpackForm('Unpack', doc=_UNPACK_DOC)\n\n    def _is_unpack(obj):\n        return isinstance(obj, _UnpackAlias)\n\n\nif hasattr(typing, \"TypeVarTuple\"):  # 3.11+\n\n    # Add default parameter - PEP 696\n    class TypeVarTuple(metaclass=_TypeVarLikeMeta):\n        \"\"\"Type variable tuple.\"\"\"\n\n        _backported_typevarlike = typing.TypeVarTuple\n\n        def __new__(cls, name, *, default=_marker):\n            tvt = typing.TypeVarTuple(name)\n            _set_default(tvt, default)\n            _set_module(tvt)\n            return tvt\n\n        def __init_subclass__(self, *args, **kwds):\n            raise TypeError(\"Cannot subclass special typing classes\")\n\nelse:\n    class TypeVarTuple(_DefaultMixin):\n        \"\"\"Type variable tuple.\n\n        Usage::\n\n            Ts = TypeVarTuple('Ts')\n\n        In the same way that a normal type variable is a stand-in for a single\n        type such as ``int``, a type variable *tuple* is a stand-in for a *tuple*\n        type such as ``Tuple[int, str]``.\n\n        Type variable tuples can be used in ``Generic`` declarations.\n        Consider the following example::\n\n            class Array(Generic[*Ts]): ...\n\n        The ``Ts`` type variable tuple here behaves like ``tuple[T1, T2]``,\n        where ``T1`` and ``T2`` are type variables. To use these type variables\n        as type parameters of ``Array``, we must *unpack* the type variable tuple using\n        the star operator: ``*Ts``. The signature of ``Array`` then behaves\n        as if we had simply written ``class Array(Generic[T1, T2]): ...``.\n        In contrast to ``Generic[T1, T2]``, however, ``Generic[*Shape]`` allows\n        us to parameterise the class with an *arbitrary* number of type parameters.\n\n        Type variable tuples can be used anywhere a normal ``TypeVar`` can.\n        This includes class definitions, as shown above, as well as function\n        signatures and variable annotations::\n\n            class Array(Generic[*Ts]):\n\n                def __init__(self, shape: Tuple[*Ts]):\n                    self._shape: Tuple[*Ts] = shape\n\n                def get_shape(self) -> Tuple[*Ts]:\n                    return self._shape\n\n            shape = (Height(480), Width(640))\n            x: Array[Height, Width] = Array(shape)\n            y = abs(x)  # Inferred type is Array[Height, Width]\n            z = x + x   #        ...    is Array[Height, Width]\n            x.get_shape()  #     ...    is tuple[Height, Width]\n\n        \"\"\"\n\n        # Trick Generic __parameters__.\n        __class__ = typing.TypeVar\n\n        def __iter__(self):\n            yield self.__unpacked__\n\n        def __init__(self, name, *, default=_marker):\n            self.__name__ = name\n            _DefaultMixin.__init__(self, default)\n\n            # for pickling:\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n            self.__unpacked__ = Unpack[self]\n\n        def __repr__(self):\n            return self.__name__\n\n        def __hash__(self):\n            return object.__hash__(self)\n\n        def __eq__(self, other):\n            return self is other\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(self, *args, **kwds):\n            if '_root' not in kwds:\n                raise TypeError(\"Cannot subclass special typing classes\")\n\n\nif hasattr(typing, \"reveal_type\"):\n    reveal_type = typing.reveal_type\nelse:\n    def reveal_type(__obj: T) -> T:\n        \"\"\"Reveal the inferred type of a variable.\n\n        When a static type checker encounters a call to ``reveal_type()``,\n        it will emit the inferred type of the argument::\n\n            x: int = 1\n            reveal_type(x)\n\n        Running a static type checker (e.g., ``mypy``) on this example\n        will produce output similar to 'Revealed type is \"builtins.int\"'.\n\n        At runtime, the function prints the runtime type of the\n        argument and returns it unchanged.\n\n        \"\"\"\n        print(f\"Runtime type is {type(__obj).__name__!r}\", file=sys.stderr)\n        return __obj\n\n\nif hasattr(typing, \"assert_never\"):\n    assert_never = typing.assert_never\nelse:\n    def assert_never(__arg: Never) -> Never:\n        \"\"\"Assert to the type checker that a line of code is unreachable.\n\n        Example::\n\n            def int_or_str(arg: int | str) -> None:\n                match arg:\n                    case int():\n                        print(\"It's an int\")\n                    case str():\n                        print(\"It's a str\")\n                    case _:\n                        assert_never(arg)\n\n        If a type checker finds that a call to assert_never() is\n        reachable, it will emit an error.\n\n        At runtime, this throws an exception when called.\n\n        \"\"\"\n        raise AssertionError(\"Expected code to be unreachable\")\n\n\nif sys.version_info >= (3, 12):\n    # dataclass_transform exists in 3.11 but lacks the frozen_default parameter\n    dataclass_transform = typing.dataclass_transform\nelse:\n    def dataclass_transform(\n        *,\n        eq_default: bool = True,\n        order_default: bool = False,\n        kw_only_default: bool = False,\n        frozen_default: bool = False,\n        field_specifiers: typing.Tuple[\n            typing.Union[typing.Type[typing.Any], typing.Callable[..., typing.Any]],\n            ...\n        ] = (),\n        **kwargs: typing.Any,\n    ) -> typing.Callable[[T], T]:\n        \"\"\"Decorator that marks a function, class, or metaclass as providing\n        dataclass-like behavior.\n\n        Example:\n\n            from pip._vendor.typing_extensions import dataclass_transform\n\n            _T = TypeVar(\"_T\")\n\n            # Used on a decorator function\n            @dataclass_transform()\n            def create_model(cls: type[_T]) -> type[_T]:\n                ...\n                return cls\n\n            @create_model\n            class CustomerModel:\n                id: int\n                name: str\n\n            # Used on a base class\n            @dataclass_transform()\n            class ModelBase: ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n            # Used on a metaclass\n            @dataclass_transform()\n            class ModelMeta(type): ...\n\n            class ModelBase(metaclass=ModelMeta): ...\n\n            class CustomerModel(ModelBase):\n                id: int\n                name: str\n\n        Each of the ``CustomerModel`` classes defined in this example will now\n        behave similarly to a dataclass created with the ``@dataclasses.dataclass``\n        decorator. For example, the type checker will synthesize an ``__init__``\n        method.\n\n        The arguments to this decorator can be used to customize this behavior:\n        - ``eq_default`` indicates whether the ``eq`` parameter is assumed to be\n          True or False if it is omitted by the caller.\n        - ``order_default`` indicates whether the ``order`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``kw_only_default`` indicates whether the ``kw_only`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``frozen_default`` indicates whether the ``frozen`` parameter is\n          assumed to be True or False if it is omitted by the caller.\n        - ``field_specifiers`` specifies a static list of supported classes\n          or functions that describe fields, similar to ``dataclasses.field()``.\n\n        At runtime, this decorator records its arguments in the\n        ``__dataclass_transform__`` attribute on the decorated object.\n\n        See PEP 681 for details.\n\n        \"\"\"\n        def decorator(cls_or_fn):\n            cls_or_fn.__dataclass_transform__ = {\n                \"eq_default\": eq_default,\n                \"order_default\": order_default,\n                \"kw_only_default\": kw_only_default,\n                \"frozen_default\": frozen_default,\n                \"field_specifiers\": field_specifiers,\n                \"kwargs\": kwargs,\n            }\n            return cls_or_fn\n        return decorator\n\n\nif hasattr(typing, \"override\"):\n    override = typing.override\nelse:\n    _F = typing.TypeVar(\"_F\", bound=typing.Callable[..., typing.Any])\n\n    def override(__arg: _F) -> _F:\n        \"\"\"Indicate that a method is intended to override a method in a base class.\n\n        Usage:\n\n            class Base:\n                def method(self) -> None: ...\n                    pass\n\n            class Child(Base):\n                @override\n                def method(self) -> None:\n                    super().method()\n\n        When this decorator is applied to a method, the type checker will\n        validate that it overrides a method with the same name on a base class.\n        This helps prevent bugs that may occur when a base class is changed\n        without an equivalent change to a child class.\n\n        There is no runtime checking of these properties. The decorator\n        sets the ``__override__`` attribute to ``True`` on the decorated object\n        to allow runtime introspection.\n\n        See PEP 698 for details.\n\n        \"\"\"\n        try:\n            __arg.__override__ = True\n        except (AttributeError, TypeError):\n            # Skip the attribute silently if it is not writable.\n            # AttributeError happens if the object has __slots__ or a\n            # read-only property, TypeError if it's a builtin class.\n            pass\n        return __arg\n\n\nif hasattr(typing, \"deprecated\"):\n    deprecated = typing.deprecated\nelse:\n    _T = typing.TypeVar(\"_T\")\n\n    def deprecated(\n        __msg: str,\n        *,\n        category: typing.Optional[typing.Type[Warning]] = DeprecationWarning,\n        stacklevel: int = 1,\n    ) -> typing.Callable[[_T], _T]:\n        \"\"\"Indicate that a class, function or overload is deprecated.\n\n        Usage:\n\n            @deprecated(\"Use B instead\")\n            class A:\n                pass\n\n            @deprecated(\"Use g instead\")\n            def f():\n                pass\n\n            @overload\n            @deprecated(\"int support is deprecated\")\n            def g(x: int) -> int: ...\n            @overload\n            def g(x: str) -> int: ...\n\n        When this decorator is applied to an object, the type checker\n        will generate a diagnostic on usage of the deprecated object.\n\n        The warning specified by ``category`` will be emitted on use\n        of deprecated objects. For functions, that happens on calls;\n        for classes, on instantiation. If the ``category`` is ``None``,\n        no warning is emitted. The ``stacklevel`` determines where the\n        warning is emitted. If it is ``1`` (the default), the warning\n        is emitted at the direct caller of the deprecated object; if it\n        is higher, it is emitted further up the stack.\n\n        The decorator sets the ``__deprecated__``\n        attribute on the decorated object to the deprecation message\n        passed to the decorator. If applied to an overload, the decorator\n        must be after the ``@overload`` decorator for the attribute to\n        exist on the overload as returned by ``get_overloads()``.\n\n        See PEP 702 for details.\n\n        \"\"\"\n        def decorator(__arg: _T) -> _T:\n            if category is None:\n                __arg.__deprecated__ = __msg\n                return __arg\n            elif isinstance(__arg, type):\n                original_new = __arg.__new__\n                has_init = __arg.__init__ is not object.__init__\n\n                @functools.wraps(original_new)\n                def __new__(cls, *args, **kwargs):\n                    warnings.warn(__msg, category=category, stacklevel=stacklevel + 1)\n                    if original_new is not object.__new__:\n                        return original_new(cls, *args, **kwargs)\n                    # Mirrors a similar check in object.__new__.\n                    elif not has_init and (args or kwargs):\n                        raise TypeError(f\"{cls.__name__}() takes no arguments\")\n                    else:\n                        return original_new(cls)\n\n                __arg.__new__ = staticmethod(__new__)\n                __arg.__deprecated__ = __new__.__deprecated__ = __msg\n                return __arg\n            elif callable(__arg):\n                @functools.wraps(__arg)\n                def wrapper(*args, **kwargs):\n                    warnings.warn(__msg, category=category, stacklevel=stacklevel + 1)\n                    return __arg(*args, **kwargs)\n\n                __arg.__deprecated__ = wrapper.__deprecated__ = __msg\n                return wrapper\n            else:\n                raise TypeError(\n                    \"@deprecated decorator with non-None category must be applied to \"\n                    f\"a class or callable, not {__arg!r}\"\n                )\n\n        return decorator\n\n\n# We have to do some monkey patching to deal with the dual nature of\n# Unpack/TypeVarTuple:\n# - We want Unpack to be a kind of TypeVar so it gets accepted in\n#   Generic[Unpack[Ts]]\n# - We want it to *not* be treated as a TypeVar for the purposes of\n#   counting generic parameters, so that when we subscript a generic,\n#   the runtime doesn't try to substitute the Unpack with the subscripted type.\nif not hasattr(typing, \"TypeVarTuple\"):\n    typing._collect_type_vars = _collect_type_vars\n    typing._check_generic = _check_generic\n\n\n# Backport typing.NamedTuple as it exists in Python 3.12.\n# In 3.11, the ability to define generic `NamedTuple`s was supported.\n# This was explicitly disallowed in 3.9-3.10, and only half-worked in <=3.8.\n# On 3.12, we added __orig_bases__ to call-based NamedTuples\n# On 3.13, we deprecated kwargs-based NamedTuples\nif sys.version_info >= (3, 13):\n    NamedTuple = typing.NamedTuple\nelse:\n    def _make_nmtuple(name, types, module, defaults=()):\n        fields = [n for n, t in types]\n        annotations = {n: typing._type_check(t, f\"field {n} annotation must be a type\")\n                       for n, t in types}\n        nm_tpl = collections.namedtuple(name, fields,\n                                        defaults=defaults, module=module)\n        nm_tpl.__annotations__ = nm_tpl.__new__.__annotations__ = annotations\n        # The `_field_types` attribute was removed in 3.9;\n        # in earlier versions, it is the same as the `__annotations__` attribute\n        if sys.version_info < (3, 9):\n            nm_tpl._field_types = annotations\n        return nm_tpl\n\n    _prohibited_namedtuple_fields = typing._prohibited\n    _special_namedtuple_fields = frozenset({'__module__', '__name__', '__annotations__'})\n\n    class _NamedTupleMeta(type):\n        def __new__(cls, typename, bases, ns):\n            assert _NamedTuple in bases\n            for base in bases:\n                if base is not _NamedTuple and base is not typing.Generic:\n                    raise TypeError(\n                        'can only inherit from a NamedTuple type and Generic')\n            bases = tuple(tuple if base is _NamedTuple else base for base in bases)\n            types = ns.get('__annotations__', {})\n            default_names = []\n            for field_name in types:\n                if field_name in ns:\n                    default_names.append(field_name)\n                elif default_names:\n                    raise TypeError(f\"Non-default namedtuple field {field_name} \"\n                                    f\"cannot follow default field\"\n                                    f\"{'s' if len(default_names) > 1 else ''} \"\n                                    f\"{', '.join(default_names)}\")\n            nm_tpl = _make_nmtuple(\n                typename, types.items(),\n                defaults=[ns[n] for n in default_names],\n                module=ns['__module__']\n            )\n            nm_tpl.__bases__ = bases\n            if typing.Generic in bases:\n                if hasattr(typing, '_generic_class_getitem'):  # 3.12+\n                    nm_tpl.__class_getitem__ = classmethod(typing._generic_class_getitem)\n                else:\n                    class_getitem = typing.Generic.__class_getitem__.__func__\n                    nm_tpl.__class_getitem__ = classmethod(class_getitem)\n            # update from user namespace without overriding special namedtuple attributes\n            for key in ns:\n                if key in _prohibited_namedtuple_fields:\n                    raise AttributeError(\"Cannot overwrite NamedTuple attribute \" + key)\n                elif key not in _special_namedtuple_fields and key not in nm_tpl._fields:\n                    setattr(nm_tpl, key, ns[key])\n            if typing.Generic in bases:\n                nm_tpl.__init_subclass__()\n            return nm_tpl\n\n    _NamedTuple = type.__new__(_NamedTupleMeta, 'NamedTuple', (), {})\n\n    def _namedtuple_mro_entries(bases):\n        assert NamedTuple in bases\n        return (_NamedTuple,)\n\n    @_ensure_subclassable(_namedtuple_mro_entries)\n    def NamedTuple(__typename, __fields=_marker, **kwargs):\n        \"\"\"Typed version of namedtuple.\n\n        Usage::\n\n            class Employee(NamedTuple):\n                name: str\n                id: int\n\n        This is equivalent to::\n\n            Employee = collections.namedtuple('Employee', ['name', 'id'])\n\n        The resulting class has an extra __annotations__ attribute, giving a\n        dict that maps field names to types.  (The field names are also in\n        the _fields attribute, which is part of the namedtuple API.)\n        An alternative equivalent functional syntax is also accepted::\n\n            Employee = NamedTuple('Employee', [('name', str), ('id', int)])\n        \"\"\"\n        if __fields is _marker:\n            if kwargs:\n                deprecated_thing = \"Creating NamedTuple classes using keyword arguments\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"Use the class-based or functional syntax instead.\"\n                )\n            else:\n                deprecated_thing = \"Failing to pass a value for the 'fields' parameter\"\n                example = f\"`{__typename} = NamedTuple({__typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif __fields is None:\n            if kwargs:\n                raise TypeError(\n                    \"Cannot pass `None` as the 'fields' parameter \"\n                    \"and also specify fields using keyword arguments\"\n                )\n            else:\n                deprecated_thing = \"Passing `None` as the 'fields' parameter\"\n                example = f\"`{__typename} = NamedTuple({__typename!r}, [])`\"\n                deprecation_msg = (\n                    \"{name} is deprecated and will be disallowed in Python {remove}. \"\n                    \"To create a NamedTuple class with 0 fields \"\n                    \"using the functional syntax, \"\n                    \"pass an empty list, e.g. \"\n                ) + example + \".\"\n        elif kwargs:\n            raise TypeError(\"Either list of fields or keywords\"\n                            \" can be provided to NamedTuple, not both\")\n        if __fields is _marker or __fields is None:\n            warnings.warn(\n                deprecation_msg.format(name=deprecated_thing, remove=\"3.15\"),\n                DeprecationWarning,\n                stacklevel=2,\n            )\n            __fields = kwargs.items()\n        nt = _make_nmtuple(__typename, __fields, module=_caller())\n        nt.__orig_bases__ = (NamedTuple,)\n        return nt\n\n    # On 3.8+, alter the signature so that it matches typing.NamedTuple.\n    # The signature of typing.NamedTuple on >=3.8 is invalid syntax in Python 3.7,\n    # so just leave the signature as it is on 3.7.\n    if sys.version_info >= (3, 8):\n        _new_signature = '(typename, fields=None, /, **kwargs)'\n        if isinstance(NamedTuple, _types.FunctionType):\n            NamedTuple.__text_signature__ = _new_signature\n        else:\n            NamedTuple.__call__.__text_signature__ = _new_signature\n\n\nif hasattr(collections.abc, \"Buffer\"):\n    Buffer = collections.abc.Buffer\nelse:\n    class Buffer(abc.ABC):\n        \"\"\"Base class for classes that implement the buffer protocol.\n\n        The buffer protocol allows Python objects to expose a low-level\n        memory buffer interface. Before Python 3.12, it is not possible\n        to implement the buffer protocol in pure Python code, or even\n        to check whether a class implements the buffer protocol. In\n        Python 3.12 and higher, the ``__buffer__`` method allows access\n        to the buffer protocol from Python code, and the\n        ``collections.abc.Buffer`` ABC allows checking whether a class\n        implements the buffer protocol.\n\n        To indicate support for the buffer protocol in earlier versions,\n        inherit from this ABC, either in a stub file or at runtime,\n        or use ABC registration. This ABC provides no methods, because\n        there is no Python-accessible methods shared by pre-3.12 buffer\n        classes. It is useful primarily for static checks.\n\n        \"\"\"\n\n    # As a courtesy, register the most common stdlib buffer classes.\n    Buffer.register(memoryview)\n    Buffer.register(bytearray)\n    Buffer.register(bytes)\n\n\n# Backport of types.get_original_bases, available on 3.12+ in CPython\nif hasattr(_types, \"get_original_bases\"):\n    get_original_bases = _types.get_original_bases\nelse:\n    def get_original_bases(__cls):\n        \"\"\"Return the class's \"original\" bases prior to modification by `__mro_entries__`.\n\n        Examples::\n\n            from typing import TypeVar, Generic\n            from pip._vendor.typing_extensions import NamedTuple, TypedDict\n\n            T = TypeVar(\"T\")\n            class Foo(Generic[T]): ...\n            class Bar(Foo[int], float): ...\n            class Baz(list[str]): ...\n            Eggs = NamedTuple(\"Eggs\", [(\"a\", int), (\"b\", str)])\n            Spam = TypedDict(\"Spam\", {\"a\": int, \"b\": str})\n\n            assert get_original_bases(Bar) == (Foo[int], float)\n            assert get_original_bases(Baz) == (list[str],)\n            assert get_original_bases(Eggs) == (NamedTuple,)\n            assert get_original_bases(Spam) == (TypedDict,)\n            assert get_original_bases(int) == (object,)\n        \"\"\"\n        try:\n            return __cls.__orig_bases__\n        except AttributeError:\n            try:\n                return __cls.__bases__\n            except AttributeError:\n                raise TypeError(\n                    f'Expected an instance of type, not {type(__cls).__name__!r}'\n                ) from None\n\n\n# NewType is a class on Python 3.10+, making it pickleable\n# The error message for subclassing instances of NewType was improved on 3.11+\nif sys.version_info >= (3, 11):\n    NewType = typing.NewType\nelse:\n    class NewType:\n        \"\"\"NewType creates simple unique types with almost zero\n        runtime overhead. NewType(name, tp) is considered a subtype of tp\n        by static type checkers. At runtime, NewType(name, tp) returns\n        a dummy callable that simply returns its argument. Usage::\n            UserId = NewType('UserId', int)\n            def name_by_id(user_id: UserId) -> str:\n                ...\n            UserId('user')          # Fails type check\n            name_by_id(42)          # Fails type check\n            name_by_id(UserId(42))  # OK\n            num = UserId(5) + 1     # type: int\n        \"\"\"\n\n        def __call__(self, obj):\n            return obj\n\n        def __init__(self, name, tp):\n            self.__qualname__ = name\n            if '.' in name:\n                name = name.rpartition('.')[-1]\n            self.__name__ = name\n            self.__supertype__ = tp\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n\n        def __mro_entries__(self, bases):\n            # We defined __mro_entries__ to get a better error message\n            # if a user attempts to subclass a NewType instance. bpo-46170\n            supercls_name = self.__name__\n\n            class Dummy:\n                def __init_subclass__(cls):\n                    subcls_name = cls.__name__\n                    raise TypeError(\n                        f\"Cannot subclass an instance of NewType. \"\n                        f\"Perhaps you were looking for: \"\n                        f\"`{subcls_name} = NewType({subcls_name!r}, {supercls_name})`\"\n                    )\n\n            return (Dummy,)\n\n        def __repr__(self):\n            return f'{self.__module__}.{self.__qualname__}'\n\n        def __reduce__(self):\n            return self.__qualname__\n\n        if sys.version_info >= (3, 10):\n            # PEP 604 methods\n            # It doesn't make sense to have these methods on Python <3.10\n\n            def __or__(self, other):\n                return typing.Union[self, other]\n\n            def __ror__(self, other):\n                return typing.Union[other, self]\n\n\nif hasattr(typing, \"TypeAliasType\"):\n    TypeAliasType = typing.TypeAliasType\nelse:\n    def _is_unionable(obj):\n        \"\"\"Corresponds to is_unionable() in unionobject.c in CPython.\"\"\"\n        return obj is None or isinstance(obj, (\n            type,\n            _types.GenericAlias,\n            _types.UnionType,\n            TypeAliasType,\n        ))\n\n    class TypeAliasType:\n        \"\"\"Create named, parameterized type aliases.\n\n        This provides a backport of the new `type` statement in Python 3.12:\n\n            type ListOrSet[T] = list[T] | set[T]\n\n        is equivalent to:\n\n            T = TypeVar(\"T\")\n            ListOrSet = TypeAliasType(\"ListOrSet\", list[T] | set[T], type_params=(T,))\n\n        The name ListOrSet can then be used as an alias for the type it refers to.\n\n        The type_params argument should contain all the type parameters used\n        in the value of the type alias. If the alias is not generic, this\n        argument is omitted.\n\n        Static type checkers should only support type aliases declared using\n        TypeAliasType that follow these rules:\n\n        - The first argument (the name) must be a string literal.\n        - The TypeAliasType instance must be immediately assigned to a variable\n          of the same name. (For example, 'X = TypeAliasType(\"Y\", int)' is invalid,\n          as is 'X, Y = TypeAliasType(\"X\", int), TypeAliasType(\"Y\", int)').\n\n        \"\"\"\n\n        def __init__(self, name: str, value, *, type_params=()):\n            if not isinstance(name, str):\n                raise TypeError(\"TypeAliasType name must be a string\")\n            self.__value__ = value\n            self.__type_params__ = type_params\n\n            parameters = []\n            for type_param in type_params:\n                if isinstance(type_param, TypeVarTuple):\n                    parameters.extend(type_param)\n                else:\n                    parameters.append(type_param)\n            self.__parameters__ = tuple(parameters)\n            def_mod = _caller()\n            if def_mod != 'typing_extensions':\n                self.__module__ = def_mod\n            # Setting this attribute closes the TypeAliasType from further modification\n            self.__name__ = name\n\n        def __setattr__(self, __name: str, __value: object) -> None:\n            if hasattr(self, \"__name__\"):\n                self._raise_attribute_error(__name)\n            super().__setattr__(__name, __value)\n\n        def __delattr__(self, __name: str) -> Never:\n            self._raise_attribute_error(__name)\n\n        def _raise_attribute_error(self, name: str) -> Never:\n            # Match the Python 3.12 error messages exactly\n            if name == \"__name__\":\n                raise AttributeError(\"readonly attribute\")\n            elif name in {\"__value__\", \"__type_params__\", \"__parameters__\", \"__module__\"}:\n                raise AttributeError(\n                    f\"attribute '{name}' of 'typing.TypeAliasType' objects \"\n                    \"is not writable\"\n                )\n            else:\n                raise AttributeError(\n                    f\"'typing.TypeAliasType' object has no attribute '{name}'\"\n                )\n\n        def __repr__(self) -> str:\n            return self.__name__\n\n        def __getitem__(self, parameters):\n            if not isinstance(parameters, tuple):\n                parameters = (parameters,)\n            parameters = [\n                typing._type_check(\n                    item, f'Subscripting {self.__name__} requires a type.'\n                )\n                for item in parameters\n            ]\n            return typing._GenericAlias(self, tuple(parameters))\n\n        def __reduce__(self):\n            return self.__name__\n\n        def __init_subclass__(cls, *args, **kwargs):\n            raise TypeError(\n                \"type 'typing_extensions.TypeAliasType' is not an acceptable base type\"\n            )\n\n        # The presence of this method convinces typing._type_check\n        # that TypeAliasTypes are types.\n        def __call__(self):\n            raise TypeError(\"Type alias is not callable\")\n\n        if sys.version_info >= (3, 10):\n            def __or__(self, right):\n                # For forward compatibility with 3.12, reject Unions\n                # that are not accepted by the built-in Union.\n                if not _is_unionable(right):\n                    return NotImplemented\n                return typing.Union[self, right]\n\n            def __ror__(self, left):\n                if not _is_unionable(left):\n                    return NotImplemented\n                return typing.Union[left, self]\n\n\nif hasattr(typing, \"is_protocol\"):\n    is_protocol = typing.is_protocol\n    get_protocol_members = typing.get_protocol_members\nelse:\n    def is_protocol(__tp: type) -> bool:\n        \"\"\"Return True if the given type is a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, is_protocol\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> is_protocol(P)\n            True\n            >>> is_protocol(int)\n            False\n        \"\"\"\n        return (\n            isinstance(__tp, type)\n            and getattr(__tp, '_is_protocol', False)\n            and __tp is not Protocol\n            and __tp is not getattr(typing, \"Protocol\", object())\n        )\n\n    def get_protocol_members(__tp: type) -> typing.FrozenSet[str]:\n        \"\"\"Return the set of members defined in a Protocol.\n\n        Example::\n\n            >>> from typing_extensions import Protocol, get_protocol_members\n            >>> class P(Protocol):\n            ...     def a(self) -> str: ...\n            ...     b: int\n            >>> get_protocol_members(P)\n            frozenset({'a', 'b'})\n\n        Raise a TypeError for arguments that are not Protocols.\n        \"\"\"\n        if not is_protocol(__tp):\n            raise TypeError(f'{__tp!r} is not a Protocol')\n        if hasattr(__tp, '__protocol_attrs__'):\n            return frozenset(__tp.__protocol_attrs__)\n        return frozenset(_get_protocol_attrs(__tp))\n\n\n# Aliases for items that have always been in typing.\n# Explicitly assign these (rather than using `from typing import *` at the top),\n# so that we get a CI error if one of these is deleted from typing.py\n# in a future version of Python\nAbstractSet = typing.AbstractSet\nAnyStr = typing.AnyStr\nBinaryIO = typing.BinaryIO\nCallable = typing.Callable\nCollection = typing.Collection\nContainer = typing.Container\nDict = typing.Dict\nForwardRef = typing.ForwardRef\nFrozenSet = typing.FrozenSet\nGenerator = typing.Generator\nGeneric = typing.Generic\nHashable = typing.Hashable\nIO = typing.IO\nItemsView = typing.ItemsView\nIterable = typing.Iterable\nIterator = typing.Iterator\nKeysView = typing.KeysView\nList = typing.List\nMapping = typing.Mapping\nMappingView = typing.MappingView\nMatch = typing.Match\nMutableMapping = typing.MutableMapping\nMutableSequence = typing.MutableSequence\nMutableSet = typing.MutableSet\nOptional = typing.Optional\nPattern = typing.Pattern\nReversible = typing.Reversible\nSequence = typing.Sequence\nSet = typing.Set\nSized = typing.Sized\nTextIO = typing.TextIO\nTuple = typing.Tuple\nUnion = typing.Union\nValuesView = typing.ValuesView\ncast = typing.cast\nno_type_check = typing.no_type_check\nno_type_check_decorator = typing.no_type_check_decorator"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_internal/utils/setuptools_build.py",
      "line_number": 1,
      "details": "import sys\nimport textwrap\nfrom typing import List, Optional, Sequence\n\n# Shim to wrap setup.py invocation with setuptools\n# Note that __file__ is handled via two {!r} *and* %r, to ensure that paths on\n# Windows are correctly handled (it should be \"C:\\\\Users\" not \"C:\\Users\").\n_SETUPTOOLS_SHIM = textwrap.dedent(\n    \"\"\"\n    exec(compile('''\n    # This is <pip-setuptools-caller> -- a caller that pip uses to run setup.py\n    #\n    # - It imports setuptools before invoking setup.py, to enable projects that directly\n    #   import from `distutils.core` to work with newer packaging standards.\n    # - It provides a clear error message when setuptools is not installed.\n    # - It sets `sys.argv[0]` to the underlying `setup.py`, when invoking `setup.py` so\n    #   setuptools doesn't think the script is `-c`. This avoids the following warning:\n    #     manifest_maker: standard file '-c' not found\".\n    # - It generates a shim setup.py, for handling setup.cfg-only projects.\n    import os, sys, tokenize\n\n    try:\n        import setuptools\n    except ImportError as error:\n        print(\n            \"ERROR: Can not execute `setup.py` since setuptools is not available in \"\n            \"the build environment.\",\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    __file__ = %r\n    sys.argv[0] = __file__\n\n    if os.path.exists(__file__):\n        filename = __file__\n        with tokenize.open(__file__) as f:\n            setup_py_code = f.read()\n    else:\n        filename = \"<auto-generated setuptools caller>\"\n        setup_py_code = \"from setuptools import setup; setup()\"\n\n    exec(compile(setup_py_code, filename, \"exec\"))\n    ''' % ({!r},), \"<pip-setuptools-caller>\", \"exec\"))\n    \"\"\"\n).rstrip()\n\n\ndef make_setuptools_shim_args(\n    setup_py_path: str,\n    global_options: Optional[Sequence[str]] = None,\n    no_user_config: bool = False,\n    unbuffered_output: bool = False,\n) -> List[str]:\n    \"\"\"\n    Get setuptools command arguments with shim wrapped setup file invocation.\n\n    :param setup_py_path: The path to setup.py to be wrapped.\n    :param global_options: Additional global options.\n    :param no_user_config: If True, disables personal user configuration.\n    :param unbuffered_output: If True, adds the unbuffered switch to the\n     argument list.\n    \"\"\"\n    args = [sys.executable]\n    if unbuffered_output:\n        args += [\"-u\"]\n    args += [\"-c\", _SETUPTOOLS_SHIM.format(setup_py_path)]\n    if global_options:\n        args += global_options\n    if no_user_config:\n        args += [\"--no-user-cfg\"]\n    return args\n\n\ndef make_setuptools_bdist_wheel_args(\n    setup_py_path: str,\n    global_options: Sequence[str],\n    build_options: Sequence[str],\n    destination_dir: str,\n) -> List[str]:\n    # NOTE: Eventually, we'd want to also -S to the flags here, when we're\n    # isolating. Currently, it breaks Python in virtualenvs, because it\n    # relies on site.py to find parts of the standard library outside the\n    # virtualenv.\n    args = make_setuptools_shim_args(\n        setup_py_path, global_options=global_options, unbuffered_output=True\n    )\n    args += [\"bdist_wheel\", \"-d\", destination_dir]\n    args += build_options\n    return args\n\n\ndef make_setuptools_clean_args(\n    setup_py_path: str,\n    global_options: Sequence[str],\n) -> List[str]:\n    args = make_setuptools_shim_args(\n        setup_py_path, global_options=global_options, unbuffered_output=True\n    )\n    args += [\"clean\", \"--all\"]\n    return args\n\n\ndef make_setuptools_develop_args(\n    setup_py_path: str,\n    *,\n    global_options: Sequence[str],\n    no_user_config: bool,\n    prefix: Optional[str],\n    home: Optional[str],\n    use_user_site: bool,\n) -> List[str]:\n    assert not (use_user_site and prefix)\n\n    args = make_setuptools_shim_args(\n        setup_py_path,\n        global_options=global_options,\n        no_user_config=no_user_config,\n    )\n\n    args += [\"develop\", \"--no-deps\"]\n\n    if prefix:\n        args += [\"--prefix\", prefix]\n    if home is not None:\n        args += [\"--install-dir\", home]\n\n    if use_user_site:\n        args += [\"--user\", \"--prefix=\"]\n\n    return args\n\n\ndef make_setuptools_egg_info_args(\n    setup_py_path: str,\n    egg_info_dir: Optional[str],\n    no_user_config: bool,\n) -> List[str]:\n    args = make_setuptools_shim_args(setup_py_path, no_user_config=no_user_config)\n\n    args += [\"egg_info\"]\n\n    if egg_info_dir:\n        args += [\"--egg-base\", egg_info_dir]\n\n    return args"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pkg_resources/__init__.py",
      "line_number": 4,
      "details": "')\n        script_filename = self._fn(self.egg_info, script)\n        namespace['__file__'] = script_filename\n        if os.path.exists(script_filename):\n            with open(script_filename) as fid:\n                source = fid.read()\n            code = compile(source, script_filename, 'exec')\n            exec(code, namespace, namespace)\n        else:\n            from linecache import cache\n\n            cache[script_filename] = (\n                len(script_text),\n                0,\n                script_text.split('"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pkg_resources/__init__.py",
      "line_number": 5,
      "details": "'),\n                script_filename,\n            )\n            script_code = compile(script_text, script_filename, 'exec')\n            exec(script_code, namespace, namespace)\n\n    def _has(self, path):\n        raise NotImplementedError(\n            \"Can't perform this operation for unregistered loader type\"\n        )\n\n    def _isdir(self, path):\n        raise NotImplementedError(\n            \"Can't perform this operation for unregistered loader type\"\n        )\n\n    def _listdir(self, path):\n        raise NotImplementedError(\n            \"Can't perform this operation for unregistered loader type\"\n        )\n\n    def _fn(self, base, resource_name):\n        self._validate_resource_path(resource_name)\n        if resource_name:\n            return os.path.join(base, *resource_name.split('/'))\n        return base\n\n    @staticmethod\n    def _validate_resource_path(path):\n        \"\"\"\n        Validate the resource paths according to the docs.\n        https://setuptools.pypa.io/en/latest/pkg_resources.html#basic-resource-access\n\n        >>> warned = getfixture('recwarn')\n        >>> warnings.simplefilter('always')\n        >>> vrp = NullProvider._validate_resource_path\n        >>> vrp('foo/bar.txt')\n        >>> bool(warned)\n        False\n        >>> vrp('../foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('/foo/bar.txt')\n        >>> bool(warned)\n        True\n        >>> vrp('foo/../../bar.txt')\n        >>> bool(warned)\n        True\n        >>> warned.clear()\n        >>> vrp('foo/f../bar.txt')\n        >>> bool(warned)\n        False\n\n        Windows path separators are straight-up disallowed.\n        >>> vrp(r'\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        >>> vrp(r'C:\\\\foo/bar.txt')\n        Traceback (most recent call last):\n        ...\n        ValueError: Use of .. or absolute path in a resource path \\\nis not allowed.\n\n        Blank values are allowed\n\n        >>> vrp('')\n        >>> bool(warned)\n        False\n\n        Non-string values are not.\n\n        >>> vrp(None)\n        Traceback (most recent call last):\n        ...\n        AttributeError: ...\n        \"\"\"\n        invalid = (\n            os.path.pardir in path.split(posixpath.sep)\n            or posixpath.isabs(path)\n            or ntpath.isabs(path)\n        )\n        if not invalid:\n            return\n\n        msg = \"Use of .. or absolute path in a resource path is not allowed.\"\n\n        # Aggressively disallow Windows absolute paths\n        if ntpath.isabs(path) and not posixpath.isabs(path):\n            raise ValueError(msg)\n\n        # for compatibility, warn; in future\n        # raise ValueError(msg)\n        issue_warning(\n            msg[:-1] + \" and will raise exceptions in a future release.\",\n            DeprecationWarning,\n        )\n\n    def _get(self, path):\n        if hasattr(self.loader, 'get_data'):\n            return self.loader.get_data(path)\n        raise NotImplementedError(\n            \"Can't perform this operation for loaders without 'get_data()'\"\n        )\n\n\nregister_loader_type(object, NullProvider)\n\n\ndef _parents(path):\n    \"\"\"\n    yield all parents of path including path\n    \"\"\"\n    last = None\n    while path != last:\n        yield path\n        last = path\n        path, _ = os.path.split(path)\n\n\nclass EggProvider(NullProvider):\n    \"\"\"Provider based on a virtual filesystem\"\"\"\n\n    def __init__(self, module):\n        super().__init__(module)\n        self._setup_prefix()\n\n    def _setup_prefix(self):\n        # Assume that metadata may be nested inside a \"basket\"\n        # of multiple eggs and use module_path instead of .archive.\n        eggs = filter(_is_egg_path, _parents(self.module_path))\n        egg = next(eggs, None)\n        egg and self._set_egg(egg)\n\n    def _set_egg(self, path):\n        self.egg_name = os.path.basename(path)\n        self.egg_info = os.path.join(path, 'EGG-INFO')\n        self.egg_root = path\n\n\nclass DefaultProvider(EggProvider):\n    \"\"\"Provides access to package resources in the filesystem\"\"\"\n\n    def _has(self, path):\n        return os.path.exists(path)\n\n    def _isdir(self, path):\n        return os.path.isdir(path)\n\n    def _listdir(self, path):\n        return os.listdir(path)\n\n    def get_resource_stream(self, manager, resource_name):\n        return open(self._fn(self.module_path, resource_name), 'rb')\n\n    def _get(self, path):\n        with open(path, 'rb') as stream:\n            return stream.read()\n\n    @classmethod\n    def _register(cls):\n        loader_names = (\n            'SourceFileLoader',\n            'SourcelessFileLoader',\n        )\n        for name in loader_names:\n            loader_cls = getattr(importlib_machinery, name, type(None))\n            register_loader_type(loader_cls, cls)\n\n\nDefaultProvider._register()\n\n\nclass EmptyProvider(NullProvider):\n    \"\"\"Provider that returns nothing for all requests\"\"\"\n\n    module_path = None\n\n    _isdir = _has = lambda self, path: False\n\n    def _get(self, path):\n        return ''\n\n    def _listdir(self, path):\n        return []\n\n    def __init__(self):\n        pass\n\n\nempty_provider = EmptyProvider()\n\n\nclass ZipManifests(dict):\n    \"\"\"\n    zip manifest builder\n    \"\"\"\n\n    @classmethod\n    def build(cls, path):\n        \"\"\"\n        Build a dictionary similar to the zipimport directory\n        caches, except instead of tuples, store ZipInfo objects.\n\n        Use a platform-specific path separator (os.sep) for the path keys\n        for compatibility with pypy on Windows.\n        \"\"\"\n        with zipfile.ZipFile(path) as zfile:\n            items = (\n                (\n                    name.replace('/', os.sep),\n                    zfile.getinfo(name),\n                )\n                for name in zfile.namelist()\n            )\n            return dict(items)\n\n    load = build\n\n\nclass MemoizedZipManifests(ZipManifests):\n    \"\"\"\n    Memoized zipfile manifests.\n    \"\"\"\n\n    manifest_mod = collections.namedtuple('manifest_mod', 'manifest mtime')\n\n    def load(self, path):\n        \"\"\"\n        Load a manifest at path or return a suitable manifest already loaded.\n        \"\"\"\n        path = os.path.normpath(path)\n        mtime = os.stat(path).st_mtime\n\n        if path not in self or self[path].mtime != mtime:\n            manifest = self.build(path)\n            self[path] = self.manifest_mod(manifest, mtime)\n\n        return self[path].manifest\n\n\nclass ZipProvider(EggProvider):\n    \"\"\"Resource support for zips and eggs\"\"\"\n\n    eagers = None\n    _zip_manifests = MemoizedZipManifests()\n\n    def __init__(self, module):\n        super().__init__(module)\n        self.zip_pre = self.loader.archive + os.sep\n\n    def _zipinfo_name(self, fspath):\n        # Convert a virtual filename (full path to file) into a zipfile subpath\n        # usable with the zipimport directory cache for our target archive\n        fspath = fspath.rstrip(os.sep)\n        if fspath == self.loader.archive:\n            return ''\n        if fspath.startswith(self.zip_pre):\n            return fspath[len(self.zip_pre) :]\n        raise AssertionError(\"%s is not a subpath of %s\" % (fspath, self.zip_pre))\n\n    def _parts(self, zip_path):\n        # Convert a zipfile subpath into an egg-relative path part list.\n        # pseudo-fs path\n        fspath = self.zip_pre + zip_path\n        if fspath.startswith(self.egg_root + os.sep):\n            return fspath[len(self.egg_root) + 1 :].split(os.sep)\n        raise AssertionError(\"%s is not a subpath of %s\" % (fspath, self.egg_root))\n\n    @property\n    def zipinfo(self):\n        return self._zip_manifests.load(self.loader.archive)\n\n    def get_resource_filename(self, manager, resource_name):\n        if not self.egg_name:\n            raise NotImplementedError(\n                \"resource_filename() only supported for .egg, not .zip\"\n            )\n        # no need to lock for extraction, since we use temp names\n        zip_path = self._resource_to_zip(resource_name)\n        eagers = self._get_eager_resources()\n        if '/'.join(self._parts(zip_path)) in eagers:\n            for name in eagers:\n                self._extract_resource(manager, self._eager_to_zip(name))\n        return self._extract_resource(manager, zip_path)\n\n    @staticmethod\n    def _get_date_and_size(zip_stat):\n        size = zip_stat.file_size\n        # ymdhms+wday, yday, dst\n        date_time = zip_stat.date_time + (0, 0, -1)\n        # 1980 offset already done\n        timestamp = time.mktime(date_time)\n        return timestamp, size\n\n    # FIXME: 'ZipProvider._extract_resource' is too complex (12)\n    def _extract_resource(self, manager, zip_path):  # noqa: C901\n        if zip_path in self._index():\n            for name in self._index()[zip_path]:\n                last = self._extract_resource(manager, os.path.join(zip_path, name))\n            # return the extracted directory name\n            return os.path.dirname(last)\n\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n\n        if not WRITE_SUPPORT:\n            raise IOError(\n                '\"os.rename\" and \"os.unlink\" are not supported ' 'on this platform'\n            )\n        try:\n            real_path = manager.get_cache_path(self.egg_name, self._parts(zip_path))\n\n            if self._is_current(real_path, zip_path):\n                return real_path\n\n            outf, tmpnam = _mkstemp(\n                \".$extract\",\n                dir=os.path.dirname(real_path),\n            )\n            os.write(outf, self.loader.get_data(zip_path))\n            os.close(outf)\n            utime(tmpnam, (timestamp, timestamp))\n            manager.postprocess(tmpnam, real_path)\n\n            try:\n                rename(tmpnam, real_path)\n\n            except os.error:\n                if os.path.isfile(real_path):\n                    if self._is_current(real_path, zip_path):\n                        # the file became current since it was checked above,\n                        #  so proceed.\n                        return real_path\n                    # Windows, del old file and retry\n                    elif os.name == 'nt':\n                        unlink(real_path)\n                        rename(tmpnam, real_path)\n                        return real_path\n                raise\n\n        except os.error:\n            # report a user-friendly error\n            manager.extraction_error()\n\n        return real_path\n\n    def _is_current(self, file_path, zip_path):\n        \"\"\"\n        Return True if the file_path is current for this zip_path\n        \"\"\"\n        timestamp, size = self._get_date_and_size(self.zipinfo[zip_path])\n        if not os.path.isfile(file_path):\n            return False\n        stat = os.stat(file_path)\n        if stat.st_size != size or stat.st_mtime != timestamp:\n            return False\n        # check that the contents match\n        zip_contents = self.loader.get_data(zip_path)\n        with open(file_path, 'rb') as f:\n            file_contents = f.read()\n        return zip_contents == file_contents\n\n    def _get_eager_resources(self):\n        if self.eagers is None:\n            eagers = []\n            for name in ('native_libs.txt', 'eager_resources.txt'):\n                if self.has_metadata(name):\n                    eagers.extend(self.get_metadata_lines(name))\n            self.eagers = eagers\n        return self.eagers\n\n    def _index(self):\n        try:\n            return self._dirindex\n        except AttributeError:\n            ind = {}\n            for path in self.zipinfo:\n                parts = path.split(os.sep)\n                while parts:\n                    parent = os.sep.join(parts[:-1])\n                    if parent in ind:\n                        ind[parent].append(parts[-1])\n                        break\n                    else:\n                        ind[parent] = [parts.pop()]\n            self._dirindex = ind\n            return ind\n\n    def _has(self, fspath):\n        zip_path = self._zipinfo_name(fspath)\n        return zip_path in self.zipinfo or zip_path in self._index()\n\n    def _isdir(self, fspath):\n        return self._zipinfo_name(fspath) in self._index()\n\n    def _listdir(self, fspath):\n        return list(self._index().get(self._zipinfo_name(fspath), ()))\n\n    def _eager_to_zip(self, resource_name):\n        return self._zipinfo_name(self._fn(self.egg_root, resource_name))\n\n    def _resource_to_zip(self, resource_name):\n        return self._zipinfo_name(self._fn(self.module_path, resource_name))\n\n\nregister_loader_type(zipimport.zipimporter, ZipProvider)\n\n\nclass FileMetadata(EmptyProvider):\n    \"\"\"Metadata handler for standalone PKG-INFO files\n\n    Usage::\n\n        metadata = FileMetadata(\"/path/to/PKG-INFO\")\n\n    This provider rejects all data and metadata requests except for PKG-INFO,\n    which is treated as existing, and will be the contents of the file at\n    the provided location.\n    \"\"\"\n\n    def __init__(self, path):\n        self.path = path\n\n    def _get_metadata_path(self, name):\n        return self.path\n\n    def has_metadata(self, name):\n        return name == 'PKG-INFO' and os.path.isfile(self.path)\n\n    def get_metadata(self, name):\n        if name != 'PKG-INFO':\n            raise KeyError(\"No metadata except PKG-INFO is available\")\n\n        with io.open(self.path, encoding='utf-8', errors=\"replace\") as f:\n            metadata = f.read()\n        self._warn_on_replacement(metadata)\n        return metadata\n\n    def _warn_on_replacement(self, metadata):\n        replacement_char = '\ufffd'\n        if replacement_char in metadata:\n            tmpl = \"{self.path} could not be properly decoded in UTF-8\"\n            msg = tmpl.format(**locals())\n            warnings.warn(msg)\n\n    def get_metadata_lines(self, name):\n        return yield_lines(self.get_metadata(name))\n\n\nclass PathMetadata(DefaultProvider):\n    \"\"\"Metadata provider for egg directories\n\n    Usage::\n\n        # Development eggs:\n\n        egg_info = \"/path/to/PackageName.egg-info\"\n        base_dir = os.path.dirname(egg_info)\n        metadata = PathMetadata(base_dir, egg_info)\n        dist_name = os.path.splitext(os.path.basename(egg_info))[0]\n        dist = Distribution(basedir, project_name=dist_name, metadata=metadata)\n\n        # Unpacked egg directories:\n\n        egg_path = \"/path/to/PackageName-ver-pyver-etc.egg\"\n        metadata = PathMetadata(egg_path, os.path.join(egg_path,'EGG-INFO'))\n        dist = Distribution.from_filename(egg_path, metadata=metadata)\n    \"\"\"\n\n    def __init__(self, path, egg_info):\n        self.module_path = path\n        self.egg_info = egg_info\n\n\nclass EggMetadata(ZipProvider):\n    \"\"\"Metadata provider for .egg files\"\"\"\n\n    def __init__(self, importer):\n        \"\"\"Create a metadata provider from a zipimporter\"\"\"\n\n        self.zip_pre = importer.archive + os.sep\n        self.loader = importer\n        if importer.prefix:\n            self.module_path = os.path.join(importer.archive, importer.prefix)\n        else:\n            self.module_path = importer.archive\n        self._setup_prefix()\n\n\n_declare_state('dict', _distribution_finders={})\n\n\ndef register_finder(importer_type, distribution_finder):\n    \"\"\"Register `distribution_finder` to find distributions in sys.path items\n\n    `importer_type` is the type or class of a PEP 302 \"Importer\" (sys.path item\n    handler), and `distribution_finder` is a callable that, passed a path\n    item and the importer instance, yields ``Distribution`` instances found on\n    that path item.  See ``pkg_resources.find_on_path`` for an example.\"\"\"\n    _distribution_finders[importer_type] = distribution_finder\n\n\ndef find_distributions(path_item, only=False):\n    \"\"\"Yield distributions accessible via `path_item`\"\"\"\n    importer = get_importer(path_item)\n    finder = _find_adapter(_distribution_finders, importer)\n    return finder(importer, path_item, only)\n\n\ndef find_eggs_in_zip(importer, path_item, only=False):\n    \"\"\"\n    Find eggs in zip files; possibly multiple nested eggs.\n    \"\"\"\n    if importer.archive.endswith('.whl'):\n        # wheels are not supported with this finder\n        # they don't have PKG-INFO metadata, and won't ever contain eggs\n        return\n    metadata = EggMetadata(importer)\n    if metadata.has_metadata('PKG-INFO'):\n        yield Distribution.from_filename(path_item, metadata=metadata)\n    if only:\n        # don't yield nested distros\n        return\n    for subitem in metadata.resource_listdir(''):\n        if _is_egg_path(subitem):\n            subpath = os.path.join(path_item, subitem)\n            dists = find_eggs_in_zip(zipimport.zipimporter(subpath), subpath)\n            for dist in dists:\n                yield dist\n        elif subitem.lower().endswith(('.dist-info', '.egg-info')):\n            subpath = os.path.join(path_item, subitem)\n            submeta = EggMetadata(zipimport.zipimporter(subpath))\n            submeta.egg_info = subpath\n            yield Distribution.from_location(path_item, subitem, submeta)\n\n\nregister_finder(zipimport.zipimporter, find_eggs_in_zip)\n\n\ndef find_nothing(importer, path_item, only=False):\n    return ()\n\n\nregister_finder(object, find_nothing)\n\n\ndef find_on_path(importer, path_item, only=False):\n    \"\"\"Yield distributions accessible on a sys.path directory\"\"\"\n    path_item = _normalize_cached(path_item)\n\n    if _is_unpacked_egg(path_item):\n        yield Distribution.from_filename(\n            path_item,\n            metadata=PathMetadata(path_item, os.path.join(path_item, 'EGG-INFO')),\n        )\n        return\n\n    entries = (os.path.join(path_item, child) for child in safe_listdir(path_item))\n\n    # scan for .egg and .egg-info in directory\n    for entry in sorted(entries):\n        fullpath = os.path.join(path_item, entry)\n        factory = dist_factory(path_item, entry, only)\n        for dist in factory(fullpath):\n            yield dist\n\n\ndef dist_factory(path_item, entry, only):\n    \"\"\"Return a dist_factory for the given entry.\"\"\"\n    lower = entry.lower()\n    is_egg_info = lower.endswith('.egg-info')\n    is_dist_info = lower.endswith('.dist-info') and os.path.isdir(\n        os.path.join(path_item, entry)\n    )\n    is_meta = is_egg_info or is_dist_info\n    return (\n        distributions_from_metadata\n        if is_meta\n        else find_distributions\n        if not only and _is_egg_path(entry)\n        else resolve_egg_link\n        if not only and lower.endswith('.egg-link')\n        else NoDists()\n    )\n\n\nclass NoDists:\n    \"\"\"\n    >>> bool(NoDists())\n    False\n\n    >>> list(NoDists()('anything'))\n    []\n    \"\"\"\n\n    def __bool__(self):\n        return False\n\n    def __call__(self, fullpath):\n        return iter(())\n\n\ndef safe_listdir(path):\n    \"\"\"\n    Attempt to list contents of path, but suppress some exceptions.\n    \"\"\"\n    try:\n        return os.listdir(path)\n    except (PermissionError, NotADirectoryError):\n        pass\n    except OSError as e:\n        # Ignore the directory if does not exist, not a directory or\n        # permission denied\n        if e.errno not in (errno.ENOTDIR, errno.EACCES, errno.ENOENT):\n            raise\n    return ()\n\n\ndef distributions_from_metadata(path):\n    root = os.path.dirname(path)\n    if os.path.isdir(path):\n        if len(os.listdir(path)) == 0:\n            # empty metadata dir; skip\n            return\n        metadata = PathMetadata(root, path)\n    else:\n        metadata = FileMetadata(path)\n    entry = os.path.basename(path)\n    yield Distribution.from_location(\n        root,\n        entry,\n        metadata,\n        precedence=DEVELOP_DIST,\n    )\n\n\ndef non_empty_lines(path):\n    \"\"\"\n    Yield non-empty lines from file at path\n    \"\"\"\n    with open(path) as f:\n        for line in f:\n            line = line.strip()\n            if line:\n                yield line\n\n\ndef resolve_egg_link(path):\n    \"\"\"\n    Given a path to an .egg-link, resolve distributions\n    present in the referenced path.\n    \"\"\"\n    referenced_paths = non_empty_lines(path)\n    resolved_paths = (\n        os.path.join(os.path.dirname(path), ref) for ref in referenced_paths\n    )\n    dist_groups = map(find_distributions, resolved_paths)\n    return next(dist_groups, ())\n\n\nif hasattr(pkgutil, 'ImpImporter'):\n    register_finder(pkgutil.ImpImporter, find_on_path)\n\nregister_finder(importlib_machinery.FileFinder, find_on_path)\n\n_declare_state('dict', _namespace_handlers={})\n_declare_state('dict', _namespace_packages={})\n\n\ndef register_namespace_handler(importer_type, namespace_handler):\n    \"\"\"Register `namespace_handler` to declare namespace packages\n\n    `importer_type` is the type or class of a PEP 302 \"Importer\" (sys.path item\n    handler), and `namespace_handler` is a callable like this::\n\n        def namespace_handler(importer, path_entry, moduleName, module):\n            # return a path_entry to use for child packages\n\n    Namespace handlers are only called if the importer object has already\n    agreed that it can handle the relevant path item, and they should only\n    return a subpath if the module __path__ does not already contain an\n    equivalent subpath.  For an example namespace handler, see\n    ``pkg_resources.file_ns_handler``.\n    \"\"\"\n    _namespace_handlers[importer_type] = namespace_handler\n\n\ndef _handle_ns(packageName, path_item):\n    \"\"\"Ensure that named package includes a subpath of path_item (if needed)\"\"\"\n\n    importer = get_importer(path_item)\n    if importer is None:\n        return None\n\n    # use find_spec (PEP 451) and fall-back to find_module (PEP 302)\n    try:\n        spec = importer.find_spec(packageName)\n    except AttributeError:\n        # capture warnings due to #1111\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            loader = importer.find_module(packageName)\n    else:\n        loader = spec.loader if spec else None\n\n    if loader is None:\n        return None\n    module = sys.modules.get(packageName)\n    if module is None:\n        module = sys.modules[packageName] = types.ModuleType(packageName)\n        module.__path__ = []\n        _set_parent_ns(packageName)\n    elif not hasattr(module, '__path__'):\n        raise TypeError(\"Not a package:\", packageName)\n    handler = _find_adapter(_namespace_handlers, importer)\n    subpath = handler(importer, path_item, packageName, module)\n    if subpath is not None:\n        path = module.__path__\n        path.append(subpath)\n        importlib.import_module(packageName)\n        _rebuild_mod_path(path, packageName, module)\n    return subpath\n\n\ndef _rebuild_mod_path(orig_path, package_name, module):\n    \"\"\"\n    Rebuild module.__path__ ensuring that all entries are ordered\n    corresponding to their sys.path order\n    \"\"\"\n    sys_path = [_normalize_cached(p) for p in sys.path]\n\n    def safe_sys_path_index(entry):\n        \"\"\"\n        Workaround for #520 and #513.\n        \"\"\"\n        try:\n            return sys_path.index(entry)\n        except ValueError:\n            return float('inf')\n\n    def position_in_sys_path(path):\n        \"\"\"\n        Return the ordinal of the path based on its position in sys.path\n        \"\"\"\n        path_parts = path.split(os.sep)\n        module_parts = package_name.count('.') + 1\n        parts = path_parts[:-module_parts]\n        return safe_sys_path_index(_normalize_cached(os.sep.join(parts)))\n\n    new_path = sorted(orig_path, key=position_in_sys_path)\n    new_path = [_normalize_cached(p) for p in new_path]\n\n    if isinstance(module.__path__, list):\n        module.__path__[:] = new_path\n    else:\n        module.__path__ = new_path\n\n\ndef declare_namespace(packageName):\n    \"\"\"Declare that package 'packageName' is a namespace package\"\"\"\n\n    msg = (\n        f\"Deprecated call to `pkg_resources.declare_namespace({packageName!r})`."
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pyparsing/results.py",
      "line_number": 1,
      "details": "# results.py\nfrom collections.abc import (\n    MutableMapping,\n    Mapping,\n    MutableSequence,\n    Iterator,\n    Sequence,\n    Container,\n)\nimport pprint\nfrom typing import Tuple, Any, Dict, Set, List\n\nstr_type: Tuple[type, ...] = (str, bytes)\n_generator_type = type((_ for _ in ()))\n\n\nclass _ParseResultsWithOffset:\n    tup: Tuple[\"ParseResults\", int]\n    __slots__ = [\"tup\"]\n\n    def __init__(self, p1: \"ParseResults\", p2: int):\n        self.tup: Tuple[ParseResults, int] = (p1, p2)\n\n    def __getitem__(self, i):\n        return self.tup[i]\n\n    def __getstate__(self):\n        return self.tup\n\n    def __setstate__(self, *args):\n        self.tup = args[0]\n\n\nclass ParseResults:\n    \"\"\"Structured parse results, to provide multiple means of access to\n    the parsed data:\n\n    - as a list (``len(results)``)\n    - by list index (``results[0], results[1]``, etc.)\n    - by attribute (``results.<results_name>`` - see :class:`ParserElement.set_results_name`)\n\n    Example::\n\n        integer = Word(nums)\n        date_str = (integer.set_results_name(\"year\") + '/'\n                    + integer.set_results_name(\"month\") + '/'\n                    + integer.set_results_name(\"day\"))\n        # equivalent form:\n        # date_str = (integer(\"year\") + '/'\n        #             + integer(\"month\") + '/'\n        #             + integer(\"day\"))\n\n        # parse_string returns a ParseResults object\n        result = date_str.parse_string(\"1999/12/31\")\n\n        def test(s, fn=repr):\n            print(f\"{s} -> {fn(eval(s))}\")\n        test(\"list(result)\")\n        test(\"result[0]\")\n        test(\"result['month']\")\n        test(\"result.day\")\n        test(\"'month' in result\")\n        test(\"'minutes' in result\")\n        test(\"result.dump()\", str)\n\n    prints::\n\n        list(result) -> ['1999', '/', '12', '/', '31']\n        result[0] -> '1999'\n        result['month'] -> '12'\n        result.day -> '31'\n        'month' in result -> True\n        'minutes' in result -> False\n        result.dump() -> ['1999', '/', '12', '/', '31']\n        - day: '31'\n        - month: '12'\n        - year: '1999'\n    \"\"\"\n\n    _null_values: Tuple[Any, ...] = (None, [], ())\n\n    _name: str\n    _parent: \"ParseResults\"\n    _all_names: Set[str]\n    _modal: bool\n    _toklist: List[Any]\n    _tokdict: Dict[str, Any]\n\n    __slots__ = (\n        \"_name\",\n        \"_parent\",\n        \"_all_names\",\n        \"_modal\",\n        \"_toklist\",\n        \"_tokdict\",\n    )\n\n    class List(list):\n        \"\"\"\n        Simple wrapper class to distinguish parsed list results that should be preserved\n        as actual Python lists, instead of being converted to :class:`ParseResults`::\n\n            LBRACK, RBRACK = map(pp.Suppress, \"[]\")\n            element = pp.Forward()\n            item = ppc.integer\n            element_list = LBRACK + pp.DelimitedList(element) + RBRACK\n\n            # add parse actions to convert from ParseResults to actual Python collection types\n            def as_python_list(t):\n                return pp.ParseResults.List(t.as_list())\n            element_list.add_parse_action(as_python_list)\n\n            element <<= item | element_list\n\n            element.run_tests('''\n                100\n                [2,3,4]\n                [[2, 1],3,4]\n                [(2, 1),3,4]\n                (2,3,4)\n                ''', post_parse=lambda s, r: (r[0], type(r[0])))\n\n        prints::\n\n            100\n            (100, <class 'int'>)\n\n            [2,3,4]\n            ([2, 3, 4], <class 'list'>)\n\n            [[2, 1],3,4]\n            ([[2, 1], 3, 4], <class 'list'>)\n\n        (Used internally by :class:`Group` when `aslist=True`.)\n        \"\"\"\n\n        def __new__(cls, contained=None):\n            if contained is None:\n                contained = []\n\n            if not isinstance(contained, list):\n                raise TypeError(\n                    f\"{cls.__name__} may only be constructed with a list, not {type(contained).__name__}\"\n                )\n\n            return list.__new__(cls)\n\n    def __new__(cls, toklist=None, name=None, **kwargs):\n        if isinstance(toklist, ParseResults):\n            return toklist\n        self = object.__new__(cls)\n        self._name = None\n        self._parent = None\n        self._all_names = set()\n\n        if toklist is None:\n            self._toklist = []\n        elif isinstance(toklist, (list, _generator_type)):\n            self._toklist = (\n                [toklist[:]]\n                if isinstance(toklist, ParseResults.List)\n                else list(toklist)\n            )\n        else:\n            self._toklist = [toklist]\n        self._tokdict = dict()\n        return self\n\n    # Performance tuning: we construct a *lot* of these, so keep this\n    # constructor as small and fast as possible\n    def __init__(\n        self, toklist=None, name=None, asList=True, modal=True, isinstance=isinstance\n    ):\n        self._tokdict: Dict[str, _ParseResultsWithOffset]\n        self._modal = modal\n        if name is not None and name != \"\":\n            if isinstance(name, int):\n                name = str(name)\n            if not modal:\n                self._all_names = {name}\n            self._name = name\n            if toklist not in self._null_values:\n                if isinstance(toklist, (str_type, type)):\n                    toklist = [toklist]\n                if asList:\n                    if isinstance(toklist, ParseResults):\n                        self[name] = _ParseResultsWithOffset(\n                            ParseResults(toklist._toklist), 0\n                        )\n                    else:\n                        self[name] = _ParseResultsWithOffset(\n                            ParseResults(toklist[0]), 0\n                        )\n                    self[name]._name = name\n                else:\n                    try:\n                        self[name] = toklist[0]\n                    except (KeyError, TypeError, IndexError):\n                        if toklist is not self:\n                            self[name] = toklist\n                        else:\n                            self._name = name\n\n    def __getitem__(self, i):\n        if isinstance(i, (int, slice)):\n            return self._toklist[i]\n        else:\n            if i not in self._all_names:\n                return self._tokdict[i][-1][0]\n            else:\n                return ParseResults([v[0] for v in self._tokdict[i]])\n\n    def __setitem__(self, k, v, isinstance=isinstance):\n        if isinstance(v, _ParseResultsWithOffset):\n            self._tokdict[k] = self._tokdict.get(k, list()) + [v]\n            sub = v[0]\n        elif isinstance(k, (int, slice)):\n            self._toklist[k] = v\n            sub = v\n        else:\n            self._tokdict[k] = self._tokdict.get(k, list()) + [\n                _ParseResultsWithOffset(v, 0)\n            ]\n            sub = v\n        if isinstance(sub, ParseResults):\n            sub._parent = self\n\n    def __delitem__(self, i):\n        if isinstance(i, (int, slice)):\n            mylen = len(self._toklist)\n            del self._toklist[i]\n\n            # convert int to slice\n            if isinstance(i, int):\n                if i < 0:\n                    i += mylen\n                i = slice(i, i + 1)\n            # get removed indices\n            removed = list(range(*i.indices(mylen)))\n            removed.reverse()\n            # fixup indices in token dictionary\n            for name, occurrences in self._tokdict.items():\n                for j in removed:\n                    for k, (value, position) in enumerate(occurrences):\n                        occurrences[k] = _ParseResultsWithOffset(\n                            value, position - (position > j)\n                        )\n        else:\n            del self._tokdict[i]\n\n    def __contains__(self, k) -> bool:\n        return k in self._tokdict\n\n    def __len__(self) -> int:\n        return len(self._toklist)\n\n    def __bool__(self) -> bool:\n        return not not (self._toklist or self._tokdict)\n\n    def __iter__(self) -> Iterator:\n        return iter(self._toklist)\n\n    def __reversed__(self) -> Iterator:\n        return iter(self._toklist[::-1])\n\n    def keys(self):\n        return iter(self._tokdict)\n\n    def values(self):\n        return (self[k] for k in self.keys())\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n\n    def haskeys(self) -> bool:\n        \"\"\"\n        Since ``keys()`` returns an iterator, this method is helpful in bypassing\n        code that looks for the existence of any defined results names.\"\"\"\n        return not not self._tokdict\n\n    def pop(self, *args, **kwargs):\n        \"\"\"\n        Removes and returns item at specified index (default= ``last``).\n        Supports both ``list`` and ``dict`` semantics for ``pop()``. If\n        passed no argument or an integer argument, it will use ``list``\n        semantics and pop tokens from the list of parsed tokens. If passed\n        a non-integer argument (most likely a string), it will use ``dict``\n        semantics and pop the corresponding value from any defined results\n        names. A second default return value argument is supported, just as in\n        ``dict.pop()``.\n\n        Example::\n\n            numlist = Word(nums)[...]\n            print(numlist.parse_string(\"0 123 321\")) # -> ['0', '123', '321']\n\n            def remove_first(tokens):\n                tokens.pop(0)\n            numlist.add_parse_action(remove_first)\n            print(numlist.parse_string(\"0 123 321\")) # -> ['123', '321']\n\n            label = Word(alphas)\n            patt = label(\"LABEL\") + Word(nums)[1, ...]\n            print(patt.parse_string(\"AAB 123 321\").dump())\n\n            # Use pop() in a parse action to remove named result (note that corresponding value is not\n            # removed from list form of results)\n            def remove_LABEL(tokens):\n                tokens.pop(\"LABEL\")\n                return tokens\n            patt.add_parse_action(remove_LABEL)\n            print(patt.parse_string(\"AAB 123 321\").dump())\n\n        prints::\n\n            ['AAB', '123', '321']\n            - LABEL: 'AAB'\n\n            ['AAB', '123', '321']\n        \"\"\"\n        if not args:\n            args = [-1]\n        for k, v in kwargs.items():\n            if k == \"default\":\n                args = (args[0], v)\n            else:\n                raise TypeError(f\"pop() got an unexpected keyword argument {k!r}\")\n        if isinstance(args[0], int) or len(args) == 1 or args[0] in self:\n            index = args[0]\n            ret = self[index]\n            del self[index]\n            return ret\n        else:\n            defaultvalue = args[1]\n            return defaultvalue\n\n    def get(self, key, default_value=None):\n        \"\"\"\n        Returns named result matching the given key, or if there is no\n        such name, then returns the given ``default_value`` or ``None`` if no\n        ``default_value`` is specified.\n\n        Similar to ``dict.get()``.\n\n        Example::\n\n            integer = Word(nums)\n            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n\n            result = date_str.parse_string(\"1999/12/31\")\n            print(result.get(\"year\")) # -> '1999'\n            print(result.get(\"hour\", \"not specified\")) # -> 'not specified'\n            print(result.get(\"hour\")) # -> None\n        \"\"\"\n        if key in self:\n            return self[key]\n        else:\n            return default_value\n\n    def insert(self, index, ins_string):\n        \"\"\"\n        Inserts new element at location index in the list of parsed tokens.\n\n        Similar to ``list.insert()``.\n\n        Example::\n\n            numlist = Word(nums)[...]\n            print(numlist.parse_string(\"0 123 321\")) # -> ['0', '123', '321']\n\n            # use a parse action to insert the parse location in the front of the parsed results\n            def insert_locn(locn, tokens):\n                tokens.insert(0, locn)\n            numlist.add_parse_action(insert_locn)\n            print(numlist.parse_string(\"0 123 321\")) # -> [0, '0', '123', '321']\n        \"\"\"\n        self._toklist.insert(index, ins_string)\n        # fixup indices in token dictionary\n        for name, occurrences in self._tokdict.items():\n            for k, (value, position) in enumerate(occurrences):\n                occurrences[k] = _ParseResultsWithOffset(\n                    value, position + (position > index)\n                )\n\n    def append(self, item):\n        \"\"\"\n        Add single element to end of ``ParseResults`` list of elements.\n\n        Example::\n\n            numlist = Word(nums)[...]\n            print(numlist.parse_string(\"0 123 321\")) # -> ['0', '123', '321']\n\n            # use a parse action to compute the sum of the parsed integers, and add it to the end\n            def append_sum(tokens):\n                tokens.append(sum(map(int, tokens)))\n            numlist.add_parse_action(append_sum)\n            print(numlist.parse_string(\"0 123 321\")) # -> ['0', '123', '321', 444]\n        \"\"\"\n        self._toklist.append(item)\n\n    def extend(self, itemseq):\n        \"\"\"\n        Add sequence of elements to end of ``ParseResults`` list of elements.\n\n        Example::\n\n            patt = Word(alphas)[1, ...]\n\n            # use a parse action to append the reverse of the matched strings, to make a palindrome\n            def make_palindrome(tokens):\n                tokens.extend(reversed([t[::-1] for t in tokens]))\n                return ''.join(tokens)\n            patt.add_parse_action(make_palindrome)\n            print(patt.parse_string(\"lskdj sdlkjf lksd\")) # -> 'lskdjsdlkjflksddsklfjkldsjdksl'\n        \"\"\"\n        if isinstance(itemseq, ParseResults):\n            self.__iadd__(itemseq)\n        else:\n            self._toklist.extend(itemseq)\n\n    def clear(self):\n        \"\"\"\n        Clear all elements and results names.\n        \"\"\"\n        del self._toklist[:]\n        self._tokdict.clear()\n\n    def __getattr__(self, name):\n        try:\n            return self[name]\n        except KeyError:\n            if name.startswith(\"__\"):\n                raise AttributeError(name)\n            return \"\"\n\n    def __add__(self, other: \"ParseResults\") -> \"ParseResults\":\n        ret = self.copy()\n        ret += other\n        return ret\n\n    def __iadd__(self, other: \"ParseResults\") -> \"ParseResults\":\n        if not other:\n            return self\n\n        if other._tokdict:\n            offset = len(self._toklist)\n            addoffset = lambda a: offset if a < 0 else a + offset\n            otheritems = other._tokdict.items()\n            otherdictitems = [\n                (k, _ParseResultsWithOffset(v[0], addoffset(v[1])))\n                for k, vlist in otheritems\n                for v in vlist\n            ]\n            for k, v in otherdictitems:\n                self[k] = v\n                if isinstance(v[0], ParseResults):\n                    v[0]._parent = self\n\n        self._toklist += other._toklist\n        self._all_names |= other._all_names\n        return self\n\n    def __radd__(self, other) -> \"ParseResults\":\n        if isinstance(other, int) and other == 0:\n            # useful for merging many ParseResults using sum() builtin\n            return self.copy()\n        else:\n            # this may raise a TypeError - so be it\n            return other + self\n\n    def __repr__(self) -> str:\n        return f\"{type(self).__name__}({self._toklist!r}, {self.as_dict()})\"\n\n    def __str__(self) -> str:\n        return (\n            \"[\"\n            + \", \".join(\n                [\n                    str(i) if isinstance(i, ParseResults) else repr(i)\n                    for i in self._toklist\n                ]\n            )\n            + \"]\"\n        )\n\n    def _asStringList(self, sep=\"\"):\n        out = []\n        for item in self._toklist:\n            if out and sep:\n                out.append(sep)\n            if isinstance(item, ParseResults):\n                out += item._asStringList()\n            else:\n                out.append(str(item))\n        return out\n\n    def as_list(self) -> list:\n        \"\"\"\n        Returns the parse results as a nested list of matching tokens, all converted to strings.\n\n        Example::\n\n            patt = Word(alphas)[1, ...]\n            result = patt.parse_string(\"sldkj lsdkj sldkj\")\n            # even though the result prints in string-like form, it is actually a pyparsing ParseResults\n            print(type(result), result) # -> <class 'pyparsing.ParseResults'> ['sldkj', 'lsdkj', 'sldkj']\n\n            # Use as_list() to create an actual list\n            result_list = result.as_list()\n            print(type(result_list), result_list) # -> <class 'list'> ['sldkj', 'lsdkj', 'sldkj']\n        \"\"\"\n        return [\n            res.as_list() if isinstance(res, ParseResults) else res\n            for res in self._toklist\n        ]\n\n    def as_dict(self) -> dict:\n        \"\"\"\n        Returns the named parse results as a nested dictionary.\n\n        Example::\n\n            integer = Word(nums)\n            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n\n            result = date_str.parse_string('12/31/1999')\n            print(type(result), repr(result)) # -> <class 'pyparsing.ParseResults'> (['12', '/', '31', '/', '1999'], {'day': [('1999', 4)], 'year': [('12', 0)], 'month': [('31', 2)]})\n\n            result_dict = result.as_dict()\n            print(type(result_dict), repr(result_dict)) # -> <class 'dict'> {'day': '1999', 'year': '12', 'month': '31'}\n\n            # even though a ParseResults supports dict-like access, sometime you just need to have a dict\n            import json\n            print(json.dumps(result)) # -> Exception: TypeError: ... is not JSON serializable\n            print(json.dumps(result.as_dict())) # -> {\"month\": \"31\", \"day\": \"1999\", \"year\": \"12\"}\n        \"\"\"\n\n        def to_item(obj):\n            if isinstance(obj, ParseResults):\n                return obj.as_dict() if obj.haskeys() else [to_item(v) for v in obj]\n            else:\n                return obj\n\n        return dict((k, to_item(v)) for k, v in self.items())\n\n    def copy(self) -> \"ParseResults\":\n        \"\"\"\n        Returns a new shallow copy of a :class:`ParseResults` object. `ParseResults`\n        items contained within the source are shared with the copy. Use\n        :class:`ParseResults.deepcopy()` to create a copy with its own separate\n        content values.\n        \"\"\"\n        ret = ParseResults(self._toklist)\n        ret._tokdict = self._tokdict.copy()\n        ret._parent = self._parent\n        ret._all_names |= self._all_names\n        ret._name = self._name\n        return ret\n\n    def deepcopy(self) -> \"ParseResults\":\n        \"\"\"\n        Returns a new deep copy of a :class:`ParseResults` object.\n        \"\"\"\n        ret = self.copy()\n        # replace values with copies if they are of known mutable types\n        for i, obj in enumerate(self._toklist):\n            if isinstance(obj, ParseResults):\n                self._toklist[i] = obj.deepcopy()\n            elif isinstance(obj, (str, bytes)):\n                pass\n            elif isinstance(obj, MutableMapping):\n                self._toklist[i] = dest = type(obj)()\n                for k, v in obj.items():\n                    dest[k] = v.deepcopy() if isinstance(v, ParseResults) else v\n            elif isinstance(obj, Container):\n                self._toklist[i] = type(obj)(\n                    v.deepcopy() if isinstance(v, ParseResults) else v for v in obj\n                )\n        return ret\n\n    def get_name(self):\n        r\"\"\"\n        Returns the results name for this token expression. Useful when several\n        different expressions might match at a particular location.\n\n        Example::\n\n            integer = Word(nums)\n            ssn_expr = Regex(r\"\\d\\d\\d-\\d\\d-\\d\\d\\d\\d\")\n            house_number_expr = Suppress('#') + Word(nums, alphanums)\n            user_data = (Group(house_number_expr)(\"house_number\")\n                        | Group(ssn_expr)(\"ssn\")\n                        | Group(integer)(\"age\"))\n            user_info = user_data[1, ...]\n\n            result = user_info.parse_string(\"22 111-22-3333 #221B\")\n            for item in result:\n                print(item.get_name(), ':', item[0])\n\n        prints::\n\n            age : 22\n            ssn : 111-22-3333\n            house_number : 221B\n        \"\"\"\n        if self._name:\n            return self._name\n        elif self._parent:\n            par: \"ParseResults\" = self._parent\n            parent_tokdict_items = par._tokdict.items()\n            return next(\n                (\n                    k\n                    for k, vlist in parent_tokdict_items\n                    for v, loc in vlist\n                    if v is self\n                ),\n                None,\n            )\n        elif (\n            len(self) == 1\n            and len(self._tokdict) == 1\n            and next(iter(self._tokdict.values()))[0][1] in (0, -1)\n        ):\n            return next(iter(self._tokdict.keys()))\n        else:\n            return None\n\n    def dump(self, indent=\"\", full=True, include_list=True, _depth=0) -> str:\n        \"\"\"\n        Diagnostic method for listing out the contents of\n        a :class:`ParseResults`. Accepts an optional ``indent`` argument so\n        that this string can be embedded in a nested display of other data.\n\n        Example::\n\n            integer = Word(nums)\n            date_str = integer(\"year\") + '/' + integer(\"month\") + '/' + integer(\"day\")\n\n            result = date_str.parse_string('1999/12/31')\n            print(result.dump())\n\n        prints::\n\n            ['1999', '/', '12', '/', '31']\n            - day: '31'\n            - month: '12'\n            - year: '1999'\n        \"\"\"\n        out = []\n        NL = \""
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/markup.py",
      "line_number": 1,
      "details": "import re\nfrom ast import literal_eval\nfrom operator import attrgetter\nfrom typing import Callable, Iterable, List, Match, NamedTuple, Optional, Tuple, Union\n\nfrom ._emoji_replace import _emoji_replace\nfrom .emoji import EmojiVariant\nfrom .errors import MarkupError\nfrom .style import Style\nfrom .text import Span, Text\n\nRE_TAGS = re.compile(\n    r\"\"\"((\\\\*)\\[([a-z#/@][^[]*?)])\"\"\",\n    re.VERBOSE,\n)\n\nRE_HANDLER = re.compile(r\"^([\\w.]*?)(\\(.*?\\))?$\")\n\n\nclass Tag(NamedTuple):\n    \"\"\"A tag in console markup.\"\"\"\n\n    name: str\n    \"\"\"The tag name. e.g. 'bold'.\"\"\"\n    parameters: Optional[str]\n    \"\"\"Any additional parameters after the name.\"\"\"\n\n    def __str__(self) -> str:\n        return (\n            self.name if self.parameters is None else f\"{self.name} {self.parameters}\"\n        )\n\n    @property\n    def markup(self) -> str:\n        \"\"\"Get the string representation of this tag.\"\"\"\n        return (\n            f\"[{self.name}]\"\n            if self.parameters is None\n            else f\"[{self.name}={self.parameters}]\"\n        )\n\n\n_ReStringMatch = Match[str]  # regex match object\n_ReSubCallable = Callable[[_ReStringMatch], str]  # Callable invoked by re.sub\n_EscapeSubMethod = Callable[[_ReSubCallable, str], str]  # Sub method of a compiled re\n\n\ndef escape(\n    markup: str,\n    _escape: _EscapeSubMethod = re.compile(r\"(\\\\*)(\\[[a-z#/@][^[]*?])\").sub,\n) -> str:\n    \"\"\"Escapes text so that it won't be interpreted as markup.\n\n    Args:\n        markup (str): Content to be inserted in to markup.\n\n    Returns:\n        str: Markup with square brackets escaped.\n    \"\"\"\n\n    def escape_backslashes(match: Match[str]) -> str:\n        \"\"\"Called by re.sub replace matches.\"\"\"\n        backslashes, text = match.groups()\n        return f\"{backslashes}{backslashes}\\\\{text}\"\n\n    markup = _escape(escape_backslashes, markup)\n    return markup\n\n\ndef _parse(markup: str) -> Iterable[Tuple[int, Optional[str], Optional[Tag]]]:\n    \"\"\"Parse markup in to an iterable of tuples of (position, text, tag).\n\n    Args:\n        markup (str): A string containing console markup\n\n    \"\"\"\n    position = 0\n    _divmod = divmod\n    _Tag = Tag\n    for match in RE_TAGS.finditer(markup):\n        full_text, escapes, tag_text = match.groups()\n        start, end = match.span()\n        if start > position:\n            yield start, markup[position:start], None\n        if escapes:\n            backslashes, escaped = _divmod(len(escapes), 2)\n            if backslashes:\n                # Literal backslashes\n                yield start, \"\\\\\" * backslashes, None\n                start += backslashes * 2\n            if escaped:\n                # Escape of tag\n                yield start, full_text[len(escapes) :], None\n                position = end\n                continue\n        text, equals, parameters = tag_text.partition(\"=\")\n        yield start, None, _Tag(text, parameters if equals else None)\n        position = end\n    if position < len(markup):\n        yield position, markup[position:], None\n\n\ndef render(\n    markup: str,\n    style: Union[str, Style] = \"\",\n    emoji: bool = True,\n    emoji_variant: Optional[EmojiVariant] = None,\n) -> Text:\n    \"\"\"Render console markup in to a Text instance.\n\n    Args:\n        markup (str): A string containing console markup.\n        emoji (bool, optional): Also render emoji code. Defaults to True.\n\n    Raises:\n        MarkupError: If there is a syntax error in the markup.\n\n    Returns:\n        Text: A test instance.\n    \"\"\"\n    emoji_replace = _emoji_replace\n    if \"[\" not in markup:\n        return Text(\n            emoji_replace(markup, default_variant=emoji_variant) if emoji else markup,\n            style=style,\n        )\n    text = Text(style=style)\n    append = text.append\n    normalize = Style.normalize\n\n    style_stack: List[Tuple[int, Tag]] = []\n    pop = style_stack.pop\n\n    spans: List[Span] = []\n    append_span = spans.append\n\n    _Span = Span\n    _Tag = Tag\n\n    def pop_style(style_name: str) -> Tuple[int, Tag]:\n        \"\"\"Pop tag matching given style name.\"\"\"\n        for index, (_, tag) in enumerate(reversed(style_stack), 1):\n            if tag.name == style_name:\n                return pop(-index)\n        raise KeyError(style_name)\n\n    for position, plain_text, tag in _parse(markup):\n        if plain_text is not None:\n            # Handle open brace escapes, where the brace is not part of a tag.\n            plain_text = plain_text.replace(\"\\\\[\", \"[\")\n            append(emoji_replace(plain_text) if emoji else plain_text)\n        elif tag is not None:\n            if tag.name.startswith(\"/\"):  # Closing tag\n                style_name = tag.name[1:].strip()\n\n                if style_name:  # explicit close\n                    style_name = normalize(style_name)\n                    try:\n                        start, open_tag = pop_style(style_name)\n                    except KeyError:\n                        raise MarkupError(\n                            f\"closing tag '{tag.markup}' at position {position} doesn't match any open tag\"\n                        ) from None\n                else:  # implicit close\n                    try:\n                        start, open_tag = pop()\n                    except IndexError:\n                        raise MarkupError(\n                            f\"closing tag '[/]' at position {position} has nothing to close\"\n                        ) from None\n\n                if open_tag.name.startswith(\"@\"):\n                    if open_tag.parameters:\n                        handler_name = \"\"\n                        parameters = open_tag.parameters.strip()\n                        handler_match = RE_HANDLER.match(parameters)\n                        if handler_match is not None:\n                            handler_name, match_parameters = handler_match.groups()\n                            parameters = (\n                                \"()\" if match_parameters is None else match_parameters\n                            )\n\n                        try:\n                            meta_params = literal_eval(parameters)\n                        except SyntaxError as error:\n                            raise MarkupError(\n                                f\"error parsing {parameters!r} in {open_tag.parameters!r}; {error.msg}\"\n                            )\n                        except Exception as error:\n                            raise MarkupError(\n                                f\"error parsing {open_tag.parameters!r}; {error}\"\n                            ) from None\n\n                        if handler_name:\n                            meta_params = (\n                                handler_name,\n                                meta_params\n                                if isinstance(meta_params, tuple)\n                                else (meta_params,),\n                            )\n\n                    else:\n                        meta_params = ()\n\n                    append_span(\n                        _Span(\n                            start, len(text), Style(meta={open_tag.name: meta_params})\n                        )\n                    )\n                else:\n                    append_span(_Span(start, len(text), str(open_tag)))\n\n            else:  # Opening tag\n                normalized_tag = _Tag(normalize(tag.name), tag.parameters)\n                style_stack.append((len(text), normalized_tag))\n\n    text_length = len(text)\n    while style_stack:\n        start, tag = style_stack.pop()\n        style = str(tag)\n        if style:\n            append_span(_Span(start, text_length, style))\n\n    text.spans = sorted(spans[::-1], key=attrgetter(\"start\"))\n    return text\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n\n    MARKUP = [\n        \"[red]Hello World[/red]\",\n        \"[magenta]Hello [b]World[/b]\",\n        \"[bold]Bold[italic] bold and italic [/bold]italic[/italic]\",\n        \"Click [link=https://www.willmcgugan.com]here[/link] to visit my Blog\",\n        \":warning-emoji: [bold red blink] DANGER![/]\",\n    ]\n\n    from pip._vendor.rich import print\n    from pip._vendor.rich.table import Table\n\n    grid = Table(\"Markup\", \"Result\", padding=(0, 1))\n\n    for markup in MARKUP:\n        grid.add_row(Text(markup), markup)\n\n    print(grid)"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/__init__.py",
      "line_number": 1,
      "details": "\"\"\"\n    pygments.formatters\n    ~~~~~~~~~~~~~~~~~~~\n\n    Pygments formatters.\n\n    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nimport sys\nimport types\nimport fnmatch\nfrom os.path import basename\n\nfrom pip._vendor.pygments.formatters._mapping import FORMATTERS\nfrom pip._vendor.pygments.plugin import find_plugin_formatters\nfrom pip._vendor.pygments.util import ClassNotFound\n\n__all__ = ['get_formatter_by_name', 'get_formatter_for_filename',\n           'get_all_formatters', 'load_formatter_from_file'] + list(FORMATTERS)\n\n_formatter_cache = {}  # classes by name\n_pattern_cache = {}\n\n\ndef _fn_matches(fn, glob):\n    \"\"\"Return whether the supplied file name fn matches pattern filename.\"\"\"\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)\n\n\ndef _load_formatters(module_name):\n    \"\"\"Load a formatter (and all others in the module too).\"\"\"\n    mod = __import__(module_name, None, None, ['__all__'])\n    for formatter_name in mod.__all__:\n        cls = getattr(mod, formatter_name)\n        _formatter_cache[cls.name] = cls\n\n\ndef get_all_formatters():\n    \"\"\"Return a generator for all formatter classes.\"\"\"\n    # NB: this returns formatter classes, not info like get_all_lexers().\n    for info in FORMATTERS.values():\n        if info[1] not in _formatter_cache:\n            _load_formatters(info[0])\n        yield _formatter_cache[info[1]]\n    for _, formatter in find_plugin_formatters():\n        yield formatter\n\n\ndef find_formatter_class(alias):\n    \"\"\"Lookup a formatter by alias.\n\n    Returns None if not found.\n    \"\"\"\n    for module_name, name, aliases, _, _ in FORMATTERS.values():\n        if alias in aliases:\n            if name not in _formatter_cache:\n                _load_formatters(module_name)\n            return _formatter_cache[name]\n    for _, cls in find_plugin_formatters():\n        if alias in cls.aliases:\n            return cls\n\n\ndef get_formatter_by_name(_alias, **options):\n    \"\"\"\n    Return an instance of a :class:`.Formatter` subclass that has `alias` in its\n    aliases list. The formatter is given the `options` at its instantiation.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if no formatter with that\n    alias is found.\n    \"\"\"\n    cls = find_formatter_class(_alias)\n    if cls is None:\n        raise ClassNotFound(\"no formatter found for name %r\" % _alias)\n    return cls(**options)\n\n\ndef load_formatter_from_file(filename, formattername=\"CustomFormatter\", **options):\n    \"\"\"\n    Return a `Formatter` subclass instance loaded from the provided file, relative\n    to the current directory.\n\n    The file is expected to contain a Formatter class named ``formattername``\n    (by default, CustomFormatter). Users should be very careful with the input, because\n    this method is equivalent to running ``eval()`` on the input file. The formatter is\n    given the `options` at its instantiation.\n\n    :exc:`pygments.util.ClassNotFound` is raised if there are any errors loading\n    the formatter.\n\n    .. versionadded:: 2.2\n    \"\"\"\n    try:\n        # This empty dict will contain the namespace for the exec'd file\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        # Retrieve the class `formattername` from that namespace\n        if formattername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' %\n                                (formattername, filename))\n        formatter_class = custom_namespace[formattername]\n        # And finally instantiate it with the options\n        return formatter_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom formatter: %s' % err)\n\n\ndef get_formatter_for_filename(fn, **options):\n    \"\"\"\n    Return a :class:`.Formatter` subclass instance that has a filename pattern\n    matching `fn`. The formatter is given the `options` at its instantiation.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if no formatter for that filename\n    is found.\n    \"\"\"\n    fn = basename(fn)\n    for modname, name, _, filenames, _ in FORMATTERS.values():\n        for filename in filenames:\n            if _fn_matches(fn, filename):\n                if name not in _formatter_cache:\n                    _load_formatters(modname)\n                return _formatter_cache[name](**options)\n    for cls in find_plugin_formatters():\n        for filename in cls.filenames:\n            if _fn_matches(fn, filename):\n                return cls(**options)\n    raise ClassNotFound(\"no formatter found for file name %r\" % fn)\n\n\nclass _automodule(types.ModuleType):\n    \"\"\"Automatically import formatters.\"\"\"\n\n    def __getattr__(self, name):\n        info = FORMATTERS.get(name)\n        if info:\n            _load_formatters(info[0])\n            cls = _formatter_cache[info[1]]\n            setattr(self, name, cls)\n            return cls\n        raise AttributeError(name)\n\n\noldmod = sys.modules[__name__]\nnewmod = _automodule(__name__)\nnewmod.__dict__.update(oldmod.__dict__)\nsys.modules[__name__] = newmod\ndel newmod.newmod, newmod.oldmod, newmod.sys, newmod.types"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/__init__.py",
      "line_number": 1,
      "details": "\"\"\"\n    pygments.formatters\n    ~~~~~~~~~~~~~~~~~~~\n\n    Pygments formatters.\n\n    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nimport sys\nimport types\nimport fnmatch\nfrom os.path import basename\n\nfrom pip._vendor.pygments.formatters._mapping import FORMATTERS\nfrom pip._vendor.pygments.plugin import find_plugin_formatters\nfrom pip._vendor.pygments.util import ClassNotFound\n\n__all__ = ['get_formatter_by_name', 'get_formatter_for_filename',\n           'get_all_formatters', 'load_formatter_from_file'] + list(FORMATTERS)\n\n_formatter_cache = {}  # classes by name\n_pattern_cache = {}\n\n\ndef _fn_matches(fn, glob):\n    \"\"\"Return whether the supplied file name fn matches pattern filename.\"\"\"\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)\n\n\ndef _load_formatters(module_name):\n    \"\"\"Load a formatter (and all others in the module too).\"\"\"\n    mod = __import__(module_name, None, None, ['__all__'])\n    for formatter_name in mod.__all__:\n        cls = getattr(mod, formatter_name)\n        _formatter_cache[cls.name] = cls\n\n\ndef get_all_formatters():\n    \"\"\"Return a generator for all formatter classes.\"\"\"\n    # NB: this returns formatter classes, not info like get_all_lexers().\n    for info in FORMATTERS.values():\n        if info[1] not in _formatter_cache:\n            _load_formatters(info[0])\n        yield _formatter_cache[info[1]]\n    for _, formatter in find_plugin_formatters():\n        yield formatter\n\n\ndef find_formatter_class(alias):\n    \"\"\"Lookup a formatter by alias.\n\n    Returns None if not found.\n    \"\"\"\n    for module_name, name, aliases, _, _ in FORMATTERS.values():\n        if alias in aliases:\n            if name not in _formatter_cache:\n                _load_formatters(module_name)\n            return _formatter_cache[name]\n    for _, cls in find_plugin_formatters():\n        if alias in cls.aliases:\n            return cls\n\n\ndef get_formatter_by_name(_alias, **options):\n    \"\"\"\n    Return an instance of a :class:`.Formatter` subclass that has `alias` in its\n    aliases list. The formatter is given the `options` at its instantiation.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if no formatter with that\n    alias is found.\n    \"\"\"\n    cls = find_formatter_class(_alias)\n    if cls is None:\n        raise ClassNotFound(\"no formatter found for name %r\" % _alias)\n    return cls(**options)\n\n\ndef load_formatter_from_file(filename, formattername=\"CustomFormatter\", **options):\n    \"\"\"\n    Return a `Formatter` subclass instance loaded from the provided file, relative\n    to the current directory.\n\n    The file is expected to contain a Formatter class named ``formattername``\n    (by default, CustomFormatter). Users should be very careful with the input, because\n    this method is equivalent to running ``eval()`` on the input file. The formatter is\n    given the `options` at its instantiation.\n\n    :exc:`pygments.util.ClassNotFound` is raised if there are any errors loading\n    the formatter.\n\n    .. versionadded:: 2.2\n    \"\"\"\n    try:\n        # This empty dict will contain the namespace for the exec'd file\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        # Retrieve the class `formattername` from that namespace\n        if formattername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' %\n                                (formattername, filename))\n        formatter_class = custom_namespace[formattername]\n        # And finally instantiate it with the options\n        return formatter_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom formatter: %s' % err)\n\n\ndef get_formatter_for_filename(fn, **options):\n    \"\"\"\n    Return a :class:`.Formatter` subclass instance that has a filename pattern\n    matching `fn`. The formatter is given the `options` at its instantiation.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if no formatter for that filename\n    is found.\n    \"\"\"\n    fn = basename(fn)\n    for modname, name, _, filenames, _ in FORMATTERS.values():\n        for filename in filenames:\n            if _fn_matches(fn, filename):\n                if name not in _formatter_cache:\n                    _load_formatters(modname)\n                return _formatter_cache[name](**options)\n    for cls in find_plugin_formatters():\n        for filename in cls.filenames:\n            if _fn_matches(fn, filename):\n                return cls(**options)\n    raise ClassNotFound(\"no formatter found for file name %r\" % fn)\n\n\nclass _automodule(types.ModuleType):\n    \"\"\"Automatically import formatters.\"\"\"\n\n    def __getattr__(self, name):\n        info = FORMATTERS.get(name)\n        if info:\n            _load_formatters(info[0])\n            cls = _formatter_cache[info[1]]\n            setattr(self, name, cls)\n            return cls\n        raise AttributeError(name)\n\n\noldmod = sys.modules[__name__]\nnewmod = _automodule(__name__)\nnewmod.__dict__.update(oldmod.__dict__)\nsys.modules[__name__] = newmod\ndel newmod.newmod, newmod.oldmod, newmod.sys, newmod.types"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/__init__.py",
      "line_number": 1,
      "details": "\"\"\"\n    pygments.lexers\n    ~~~~~~~~~~~~~~~\n\n    Pygments lexers.\n\n    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nimport sys\nimport types\nimport fnmatch\nfrom os.path import basename\n\nfrom pip._vendor.pygments.lexers._mapping import LEXERS\nfrom pip._vendor.pygments.modeline import get_filetype_from_buffer\nfrom pip._vendor.pygments.plugin import find_plugin_lexers\nfrom pip._vendor.pygments.util import ClassNotFound, guess_decode\n\nCOMPAT = {\n    'Python3Lexer': 'PythonLexer',\n    'Python3TracebackLexer': 'PythonTracebackLexer',\n}\n\n__all__ = ['get_lexer_by_name', 'get_lexer_for_filename', 'find_lexer_class',\n           'guess_lexer', 'load_lexer_from_file'] + list(LEXERS) + list(COMPAT)\n\n_lexer_cache = {}\n_pattern_cache = {}\n\n\ndef _fn_matches(fn, glob):\n    \"\"\"Return whether the supplied file name fn matches pattern filename.\"\"\"\n    if glob not in _pattern_cache:\n        pattern = _pattern_cache[glob] = re.compile(fnmatch.translate(glob))\n        return pattern.match(fn)\n    return _pattern_cache[glob].match(fn)\n\n\ndef _load_lexers(module_name):\n    \"\"\"Load a lexer (and all others in the module too).\"\"\"\n    mod = __import__(module_name, None, None, ['__all__'])\n    for lexer_name in mod.__all__:\n        cls = getattr(mod, lexer_name)\n        _lexer_cache[cls.name] = cls\n\n\ndef get_all_lexers(plugins=True):\n    \"\"\"Return a generator of tuples in the form ``(name, aliases,\n    filenames, mimetypes)`` of all know lexers.\n\n    If *plugins* is true (the default), plugin lexers supplied by entrypoints\n    are also returned.  Otherwise, only builtin ones are considered.\n    \"\"\"\n    for item in LEXERS.values():\n        yield item[1:]\n    if plugins:\n        for lexer in find_plugin_lexers():\n            yield lexer.name, lexer.aliases, lexer.filenames, lexer.mimetypes\n\n\ndef find_lexer_class(name):\n    \"\"\"\n    Return the `Lexer` subclass that with the *name* attribute as given by\n    the *name* argument.\n    \"\"\"\n    if name in _lexer_cache:\n        return _lexer_cache[name]\n    # lookup builtin lexers\n    for module_name, lname, aliases, _, _ in LEXERS.values():\n        if name == lname:\n            _load_lexers(module_name)\n            return _lexer_cache[name]\n    # continue with lexers from setuptools entrypoints\n    for cls in find_plugin_lexers():\n        if cls.name == name:\n            return cls\n\n\ndef find_lexer_class_by_name(_alias):\n    \"\"\"\n    Return the `Lexer` subclass that has `alias` in its aliases list, without\n    instantiating it.\n\n    Like `get_lexer_by_name`, but does not instantiate the class.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if no lexer with that alias is\n    found.\n\n    .. versionadded:: 2.2\n    \"\"\"\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n    # lookup builtin lexers\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name]\n    # continue with lexers from setuptools entrypoints\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls\n    raise ClassNotFound('no lexer for alias %r found' % _alias)\n\n\ndef get_lexer_by_name(_alias, **options):\n    \"\"\"\n    Return an instance of a `Lexer` subclass that has `alias` in its\n    aliases list. The lexer is given the `options` at its\n    instantiation.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if no lexer with that alias is\n    found.\n    \"\"\"\n    if not _alias:\n        raise ClassNotFound('no lexer for alias %r found' % _alias)\n\n    # lookup builtin lexers\n    for module_name, name, aliases, _, _ in LEXERS.values():\n        if _alias.lower() in aliases:\n            if name not in _lexer_cache:\n                _load_lexers(module_name)\n            return _lexer_cache[name](**options)\n    # continue with lexers from setuptools entrypoints\n    for cls in find_plugin_lexers():\n        if _alias.lower() in cls.aliases:\n            return cls(**options)\n    raise ClassNotFound('no lexer for alias %r found' % _alias)\n\n\ndef load_lexer_from_file(filename, lexername=\"CustomLexer\", **options):\n    \"\"\"Load a lexer from a file.\n\n    This method expects a file located relative to the current working\n    directory, which contains a Lexer class. By default, it expects the\n    Lexer to be name CustomLexer; you can specify your own class name\n    as the second argument to this function.\n\n    Users should be very careful with the input, because this method\n    is equivalent to running eval on the input file.\n\n    Raises ClassNotFound if there are any problems importing the Lexer.\n\n    .. versionadded:: 2.2\n    \"\"\"\n    try:\n        # This empty dict will contain the namespace for the exec'd file\n        custom_namespace = {}\n        with open(filename, 'rb') as f:\n            exec(f.read(), custom_namespace)\n        # Retrieve the class `lexername` from that namespace\n        if lexername not in custom_namespace:\n            raise ClassNotFound('no valid %s class found in %s' %\n                                (lexername, filename))\n        lexer_class = custom_namespace[lexername]\n        # And finally instantiate it with the options\n        return lexer_class(**options)\n    except OSError as err:\n        raise ClassNotFound('cannot read %s: %s' % (filename, err))\n    except ClassNotFound:\n        raise\n    except Exception as err:\n        raise ClassNotFound('error when loading custom lexer: %s' % err)\n\n\ndef find_lexer_class_for_filename(_fn, code=None):\n    \"\"\"Get a lexer for a filename.\n\n    If multiple lexers match the filename pattern, use ``analyse_text()`` to\n    figure out which one is more appropriate.\n\n    Returns None if not found.\n    \"\"\"\n    matches = []\n    fn = basename(_fn)\n    for modname, name, _, filenames, _ in LEXERS.values():\n        for filename in filenames:\n            if _fn_matches(fn, filename):\n                if name not in _lexer_cache:\n                    _load_lexers(modname)\n                matches.append((_lexer_cache[name], filename))\n    for cls in find_plugin_lexers():\n        for filename in cls.filenames:\n            if _fn_matches(fn, filename):\n                matches.append((cls, filename))\n\n    if isinstance(code, bytes):\n        # decode it, since all analyse_text functions expect unicode\n        code = guess_decode(code)\n\n    def get_rating(info):\n        cls, filename = info\n        # explicit patterns get a bonus\n        bonus = '*' not in filename and 0.5 or 0\n        # The class _always_ defines analyse_text because it's included in\n        # the Lexer class.  The default implementation returns None which\n        # gets turned into 0.0.  Run scripts/detect_missing_analyse_text.py\n        # to find lexers which need it overridden.\n        if code:\n            return cls.analyse_text(code) + bonus, cls.__name__\n        return cls.priority + bonus, cls.__name__\n\n    if matches:\n        matches.sort(key=get_rating)\n        # print \"Possible lexers, after sort:\", matches\n        return matches[-1][0]\n\n\ndef get_lexer_for_filename(_fn, code=None, **options):\n    \"\"\"Get a lexer for a filename.\n\n    Return a `Lexer` subclass instance that has a filename pattern\n    matching `fn`. The lexer is given the `options` at its\n    instantiation.\n\n    Raise :exc:`pygments.util.ClassNotFound` if no lexer for that filename\n    is found.\n\n    If multiple lexers match the filename pattern, use their ``analyse_text()``\n    methods to figure out which one is more appropriate.\n    \"\"\"\n    res = find_lexer_class_for_filename(_fn, code)\n    if not res:\n        raise ClassNotFound('no lexer for filename %r found' % _fn)\n    return res(**options)\n\n\ndef get_lexer_for_mimetype(_mime, **options):\n    \"\"\"\n    Return a `Lexer` subclass instance that has `mime` in its mimetype\n    list. The lexer is given the `options` at its instantiation.\n\n    Will raise :exc:`pygments.util.ClassNotFound` if not lexer for that mimetype\n    is found.\n    \"\"\"\n    for modname, name, _, _, mimetypes in LEXERS.values():\n        if _mime in mimetypes:\n            if name not in _lexer_cache:\n                _load_lexers(modname)\n            return _lexer_cache[name](**options)\n    for cls in find_plugin_lexers():\n        if _mime in cls.mimetypes:\n            return cls(**options)\n    raise ClassNotFound('no lexer for mimetype %r found' % _mime)\n\n\ndef _iter_lexerclasses(plugins=True):\n    \"\"\"Return an iterator over all lexer classes.\"\"\"\n    for key in sorted(LEXERS):\n        module_name, name = LEXERS[key][:2]\n        if name not in _lexer_cache:\n            _load_lexers(module_name)\n        yield _lexer_cache[name]\n    if plugins:\n        yield from find_plugin_lexers()\n\n\ndef guess_lexer_for_filename(_fn, _text, **options):\n    \"\"\"\n    As :func:`guess_lexer()`, but only lexers which have a pattern in `filenames`\n    or `alias_filenames` that matches `filename` are taken into consideration.\n\n    :exc:`pygments.util.ClassNotFound` is raised if no lexer thinks it can\n    handle the content.\n    \"\"\"\n    fn = basename(_fn)\n    primary = {}\n    matching_lexers = set()\n    for lexer in _iter_lexerclasses():\n        for filename in lexer.filenames:\n            if _fn_matches(fn, filename):\n                matching_lexers.add(lexer)\n                primary[lexer] = True\n        for filename in lexer.alias_filenames:\n            if _fn_matches(fn, filename):\n                matching_lexers.add(lexer)\n                primary[lexer] = False\n    if not matching_lexers:\n        raise ClassNotFound('no lexer for filename %r found' % fn)\n    if len(matching_lexers) == 1:\n        return matching_lexers.pop()(**options)\n    result = []\n    for lexer in matching_lexers:\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        result.append((rv, lexer))\n\n    def type_sort(t):\n        # sort by:\n        # - analyse score\n        # - is primary filename pattern?\n        # - priority\n        # - last resort: class name\n        return (t[0], primary[t[1]], t[1].priority, t[1].__name__)\n    result.sort(key=type_sort)\n\n    return result[-1][1](**options)\n\n\ndef guess_lexer(_text, **options):\n    \"\"\"\n    Return a `Lexer` subclass instance that's guessed from the text in\n    `text`. For that, the :meth:`.analyse_text()` method of every known lexer\n    class is called with the text as argument, and the lexer which returned the\n    highest value will be instantiated and returned.\n\n    :exc:`pygments.util.ClassNotFound` is raised if no lexer thinks it can\n    handle the content.\n    \"\"\"\n\n    if not isinstance(_text, str):\n        inencoding = options.get('inencoding', options.get('encoding'))\n        if inencoding:\n            _text = _text.decode(inencoding or 'utf8')\n        else:\n            _text, _ = guess_decode(_text)\n\n    # try to get a vim modeline first\n    ft = get_filetype_from_buffer(_text)\n\n    if ft is not None:\n        try:\n            return get_lexer_by_name(ft, **options)\n        except ClassNotFound:\n            pass\n\n    best_lexer = [0.0, None]\n    for lexer in _iter_lexerclasses():\n        rv = lexer.analyse_text(_text)\n        if rv == 1.0:\n            return lexer(**options)\n        if rv > best_lexer[0]:\n            best_lexer[:] = (rv, lexer)\n    if not best_lexer[0] or best_lexer[1] is None:\n        raise ClassNotFound('no lexer matching the text found')\n    return best_lexer[1](**options)\n\n\nclass _automodule(types.ModuleType):\n    \"\"\"Automatically import lexers.\"\"\"\n\n    def __getattr__(self, name):\n        info = LEXERS.get(name)\n        if info:\n            _load_lexers(info[0])\n            cls = _lexer_cache[info[1]]\n            setattr(self, name, cls)\n            return cls\n        if name in COMPAT:\n            return getattr(self, COMPAT[name])\n        raise AttributeError(name)\n\n\noldmod = sys.modules[__name__]\nnewmod = _automodule(__name__)\nnewmod.__dict__.update(oldmod.__dict__)\nsys.modules[__name__] = newmod\ndel newmod.newmod, newmod.oldmod, newmod.sys, newmod.types"
    },
    {
      "category": "command_injection",
      "severity": "high",
      "description": "Potential command injection detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py",
      "line_number": 1,
      "details": "# Copyright (c) 2010-2020 Benjamin Peterson\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\n\"\"\"Utilities for writing code that runs on Python 2 and 3\"\"\"\n\nfrom __future__ import absolute_import\n\nimport functools\nimport itertools\nimport operator\nimport sys\nimport types\n\n__author__ = \"Benjamin Peterson <benjamin@python.org>\"\n__version__ = \"1.16.0\"\n\n\n# Useful for very coarse version differentiation.\nPY2 = sys.version_info[0] == 2\nPY3 = sys.version_info[0] == 3\nPY34 = sys.version_info[0:2] >= (3, 4)\n\nif PY3:\n    string_types = (str,)\n    integer_types = (int,)\n    class_types = (type,)\n    text_type = str\n    binary_type = bytes\n\n    MAXSIZE = sys.maxsize\nelse:\n    string_types = (basestring,)\n    integer_types = (int, long)\n    class_types = (type, types.ClassType)\n    text_type = unicode\n    binary_type = str\n\n    if sys.platform.startswith(\"java\"):\n        # Jython always uses 32 bits.\n        MAXSIZE = int((1 << 31) - 1)\n    else:\n        # It's possible to have sizeof(long) != sizeof(Py_ssize_t).\n        class X(object):\n            def __len__(self):\n                return 1 << 31\n\n        try:\n            len(X())\n        except OverflowError:\n            # 32-bit\n            MAXSIZE = int((1 << 31) - 1)\n        else:\n            # 64-bit\n            MAXSIZE = int((1 << 63) - 1)\n        del X\n\nif PY34:\n    from importlib.util import spec_from_loader\nelse:\n    spec_from_loader = None\n\n\ndef _add_doc(func, doc):\n    \"\"\"Add documentation to a function.\"\"\"\n    func.__doc__ = doc\n\n\ndef _import_module(name):\n    \"\"\"Import module, returning the module after the last dot.\"\"\"\n    __import__(name)\n    return sys.modules[name]\n\n\nclass _LazyDescr(object):\n    def __init__(self, name):\n        self.name = name\n\n    def __get__(self, obj, tp):\n        result = self._resolve()\n        setattr(obj, self.name, result)  # Invokes __set__.\n        try:\n            # This is a bit ugly, but it avoids running this again by\n            # removing this descriptor.\n            delattr(obj.__class__, self.name)\n        except AttributeError:\n            pass\n        return result\n\n\nclass MovedModule(_LazyDescr):\n    def __init__(self, name, old, new=None):\n        super(MovedModule, self).__init__(name)\n        if PY3:\n            if new is None:\n                new = name\n            self.mod = new\n        else:\n            self.mod = old\n\n    def _resolve(self):\n        return _import_module(self.mod)\n\n    def __getattr__(self, attr):\n        _module = self._resolve()\n        value = getattr(_module, attr)\n        setattr(self, attr, value)\n        return value\n\n\nclass _LazyModule(types.ModuleType):\n    def __init__(self, name):\n        super(_LazyModule, self).__init__(name)\n        self.__doc__ = self.__class__.__doc__\n\n    def __dir__(self):\n        attrs = [\"__doc__\", \"__name__\"]\n        attrs += [attr.name for attr in self._moved_attributes]\n        return attrs\n\n    # Subclasses should override this\n    _moved_attributes = []\n\n\nclass MovedAttribute(_LazyDescr):\n    def __init__(self, name, old_mod, new_mod, old_attr=None, new_attr=None):\n        super(MovedAttribute, self).__init__(name)\n        if PY3:\n            if new_mod is None:\n                new_mod = name\n            self.mod = new_mod\n            if new_attr is None:\n                if old_attr is None:\n                    new_attr = name\n                else:\n                    new_attr = old_attr\n            self.attr = new_attr\n        else:\n            self.mod = old_mod\n            if old_attr is None:\n                old_attr = name\n            self.attr = old_attr\n\n    def _resolve(self):\n        module = _import_module(self.mod)\n        return getattr(module, self.attr)\n\n\nclass _SixMetaPathImporter(object):\n\n    \"\"\"\n    A meta path importer to import six.moves and its submodules.\n\n    This class implements a PEP302 finder and loader. It should be compatible\n    with Python 2.5 and all existing versions of Python3\n    \"\"\"\n\n    def __init__(self, six_module_name):\n        self.name = six_module_name\n        self.known_modules = {}\n\n    def _add_module(self, mod, *fullnames):\n        for fullname in fullnames:\n            self.known_modules[self.name + \".\" + fullname] = mod\n\n    def _get_module(self, fullname):\n        return self.known_modules[self.name + \".\" + fullname]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.known_modules:\n            return self\n        return None\n\n    def find_spec(self, fullname, path, target=None):\n        if fullname in self.known_modules:\n            return spec_from_loader(fullname, self)\n        return None\n\n    def __get_module(self, fullname):\n        try:\n            return self.known_modules[fullname]\n        except KeyError:\n            raise ImportError(\"This loader does not know module \" + fullname)\n\n    def load_module(self, fullname):\n        try:\n            # in case of a reload\n            return sys.modules[fullname]\n        except KeyError:\n            pass\n        mod = self.__get_module(fullname)\n        if isinstance(mod, MovedModule):\n            mod = mod._resolve()\n        else:\n            mod.__loader__ = self\n        sys.modules[fullname] = mod\n        return mod\n\n    def is_package(self, fullname):\n        \"\"\"\n        Return true, if the named module is a package.\n\n        We need this method to get correct spec objects with\n        Python 3.4 (see PEP451)\n        \"\"\"\n        return hasattr(self.__get_module(fullname), \"__path__\")\n\n    def get_code(self, fullname):\n        \"\"\"Return None\n\n        Required, if is_package is implemented\"\"\"\n        self.__get_module(fullname)  # eventually raises ImportError\n        return None\n\n    get_source = get_code  # same as get_code\n\n    def create_module(self, spec):\n        return self.load_module(spec.name)\n\n    def exec_module(self, module):\n        pass\n\n\n_importer = _SixMetaPathImporter(__name__)\n\n\nclass _MovedItems(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects\"\"\"\n\n    __path__ = []  # mark as package\n\n\n_moved_attributes = [\n    MovedAttribute(\"cStringIO\", \"cStringIO\", \"io\", \"StringIO\"),\n    MovedAttribute(\"filter\", \"itertools\", \"builtins\", \"ifilter\", \"filter\"),\n    MovedAttribute(\n        \"filterfalse\", \"itertools\", \"itertools\", \"ifilterfalse\", \"filterfalse\"\n    ),\n    MovedAttribute(\"input\", \"__builtin__\", \"builtins\", \"raw_input\", \"input\"),\n    MovedAttribute(\"intern\", \"__builtin__\", \"sys\"),\n    MovedAttribute(\"map\", \"itertools\", \"builtins\", \"imap\", \"map\"),\n    MovedAttribute(\"getcwd\", \"os\", \"os\", \"getcwdu\", \"getcwd\"),\n    MovedAttribute(\"getcwdb\", \"os\", \"os\", \"getcwd\", \"getcwdb\"),\n    MovedAttribute(\"getoutput\", \"commands\", \"subprocess\"),\n    MovedAttribute(\"range\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\n        \"reload_module\", \"__builtin__\", \"importlib\" if PY34 else \"imp\", \"reload\"\n    ),\n    MovedAttribute(\"reduce\", \"__builtin__\", \"functools\"),\n    MovedAttribute(\"shlex_quote\", \"pipes\", \"shlex\", \"quote\"),\n    MovedAttribute(\"StringIO\", \"StringIO\", \"io\"),\n    MovedAttribute(\"UserDict\", \"UserDict\", \"collections\"),\n    MovedAttribute(\"UserList\", \"UserList\", \"collections\"),\n    MovedAttribute(\"UserString\", \"UserString\", \"collections\"),\n    MovedAttribute(\"xrange\", \"__builtin__\", \"builtins\", \"xrange\", \"range\"),\n    MovedAttribute(\"zip\", \"itertools\", \"builtins\", \"izip\", \"zip\"),\n    MovedAttribute(\n        \"zip_longest\", \"itertools\", \"itertools\", \"izip_longest\", \"zip_longest\"\n    ),\n    MovedModule(\"builtins\", \"__builtin__\"),\n    MovedModule(\"configparser\", \"ConfigParser\"),\n    MovedModule(\n        \"collections_abc\",\n        \"collections\",\n        \"collections.abc\" if sys.version_info >= (3, 3) else \"collections\",\n    ),\n    MovedModule(\"copyreg\", \"copy_reg\"),\n    MovedModule(\"dbm_gnu\", \"gdbm\", \"dbm.gnu\"),\n    MovedModule(\"dbm_ndbm\", \"dbm\", \"dbm.ndbm\"),\n    MovedModule(\n        \"_dummy_thread\",\n        \"dummy_thread\",\n        \"_dummy_thread\" if sys.version_info < (3, 9) else \"_thread\",\n    ),\n    MovedModule(\"http_cookiejar\", \"cookielib\", \"http.cookiejar\"),\n    MovedModule(\"http_cookies\", \"Cookie\", \"http.cookies\"),\n    MovedModule(\"html_entities\", \"htmlentitydefs\", \"html.entities\"),\n    MovedModule(\"html_parser\", \"HTMLParser\", \"html.parser\"),\n    MovedModule(\"http_client\", \"httplib\", \"http.client\"),\n    MovedModule(\"email_mime_base\", \"email.MIMEBase\", \"email.mime.base\"),\n    MovedModule(\"email_mime_image\", \"email.MIMEImage\", \"email.mime.image\"),\n    MovedModule(\"email_mime_multipart\", \"email.MIMEMultipart\", \"email.mime.multipart\"),\n    MovedModule(\n        \"email_mime_nonmultipart\", \"email.MIMENonMultipart\", \"email.mime.nonmultipart\"\n    ),\n    MovedModule(\"email_mime_text\", \"email.MIMEText\", \"email.mime.text\"),\n    MovedModule(\"BaseHTTPServer\", \"BaseHTTPServer\", \"http.server\"),\n    MovedModule(\"CGIHTTPServer\", \"CGIHTTPServer\", \"http.server\"),\n    MovedModule(\"SimpleHTTPServer\", \"SimpleHTTPServer\", \"http.server\"),\n    MovedModule(\"cPickle\", \"cPickle\", \"pickle\"),\n    MovedModule(\"queue\", \"Queue\"),\n    MovedModule(\"reprlib\", \"repr\"),\n    MovedModule(\"socketserver\", \"SocketServer\"),\n    MovedModule(\"_thread\", \"thread\", \"_thread\"),\n    MovedModule(\"tkinter\", \"Tkinter\"),\n    MovedModule(\"tkinter_dialog\", \"Dialog\", \"tkinter.dialog\"),\n    MovedModule(\"tkinter_filedialog\", \"FileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_scrolledtext\", \"ScrolledText\", \"tkinter.scrolledtext\"),\n    MovedModule(\"tkinter_simpledialog\", \"SimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"tkinter_tix\", \"Tix\", \"tkinter.tix\"),\n    MovedModule(\"tkinter_ttk\", \"ttk\", \"tkinter.ttk\"),\n    MovedModule(\"tkinter_constants\", \"Tkconstants\", \"tkinter.constants\"),\n    MovedModule(\"tkinter_dnd\", \"Tkdnd\", \"tkinter.dnd\"),\n    MovedModule(\"tkinter_colorchooser\", \"tkColorChooser\", \"tkinter.colorchooser\"),\n    MovedModule(\"tkinter_commondialog\", \"tkCommonDialog\", \"tkinter.commondialog\"),\n    MovedModule(\"tkinter_tkfiledialog\", \"tkFileDialog\", \"tkinter.filedialog\"),\n    MovedModule(\"tkinter_font\", \"tkFont\", \"tkinter.font\"),\n    MovedModule(\"tkinter_messagebox\", \"tkMessageBox\", \"tkinter.messagebox\"),\n    MovedModule(\"tkinter_tksimpledialog\", \"tkSimpleDialog\", \"tkinter.simpledialog\"),\n    MovedModule(\"urllib_parse\", __name__ + \".moves.urllib_parse\", \"urllib.parse\"),\n    MovedModule(\"urllib_error\", __name__ + \".moves.urllib_error\", \"urllib.error\"),\n    MovedModule(\"urllib\", __name__ + \".moves.urllib\", __name__ + \".moves.urllib\"),\n    MovedModule(\"urllib_robotparser\", \"robotparser\", \"urllib.robotparser\"),\n    MovedModule(\"xmlrpc_client\", \"xmlrpclib\", \"xmlrpc.client\"),\n    MovedModule(\"xmlrpc_server\", \"SimpleXMLRPCServer\", \"xmlrpc.server\"),\n]\n# Add windows specific modules.\nif sys.platform == \"win32\":\n    _moved_attributes += [\n        MovedModule(\"winreg\", \"_winreg\"),\n    ]\n\nfor attr in _moved_attributes:\n    setattr(_MovedItems, attr.name, attr)\n    if isinstance(attr, MovedModule):\n        _importer._add_module(attr, \"moves.\" + attr.name)\ndel attr\n\n_MovedItems._moved_attributes = _moved_attributes\n\nmoves = _MovedItems(__name__ + \".moves\")\n_importer._add_module(moves, \"moves\")\n\n\nclass Module_six_moves_urllib_parse(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_parse\"\"\"\n\n\n_urllib_parse_moved_attributes = [\n    MovedAttribute(\"ParseResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"SplitResult\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qs\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"parse_qsl\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urldefrag\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urljoin\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunparse\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"urlunsplit\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"quote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"quote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"unquote_plus\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\n        \"unquote_to_bytes\", \"urllib\", \"urllib.parse\", \"unquote\", \"unquote_to_bytes\"\n    ),\n    MovedAttribute(\"urlencode\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitquery\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splittag\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splituser\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"splitvalue\", \"urllib\", \"urllib.parse\"),\n    MovedAttribute(\"uses_fragment\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_netloc\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_params\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_query\", \"urlparse\", \"urllib.parse\"),\n    MovedAttribute(\"uses_relative\", \"urlparse\", \"urllib.parse\"),\n]\nfor attr in _urllib_parse_moved_attributes:\n    setattr(Module_six_moves_urllib_parse, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_parse._moved_attributes = _urllib_parse_moved_attributes\n\n_importer._add_module(\n    Module_six_moves_urllib_parse(__name__ + \".moves.urllib_parse\"),\n    \"moves.urllib_parse\",\n    \"moves.urllib.parse\",\n)\n\n\nclass Module_six_moves_urllib_error(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_error\"\"\"\n\n\n_urllib_error_moved_attributes = [\n    MovedAttribute(\"URLError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"HTTPError\", \"urllib2\", \"urllib.error\"),\n    MovedAttribute(\"ContentTooShortError\", \"urllib\", \"urllib.error\"),\n]\nfor attr in _urllib_error_moved_attributes:\n    setattr(Module_six_moves_urllib_error, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_error._moved_attributes = _urllib_error_moved_attributes\n\n_importer._add_module(\n    Module_six_moves_urllib_error(__name__ + \".moves.urllib.error\"),\n    \"moves.urllib_error\",\n    \"moves.urllib.error\",\n)\n\n\nclass Module_six_moves_urllib_request(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_request\"\"\"\n\n\n_urllib_request_moved_attributes = [\n    MovedAttribute(\"urlopen\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"install_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"build_opener\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"pathname2url\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"url2pathname\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"getproxies\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"Request\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"OpenerDirector\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDefaultErrorHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPRedirectHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPCookieProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"BaseHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgr\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPPasswordMgrWithDefaultRealm\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyBasicAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"AbstractDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"ProxyDigestAuthHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPSHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FileHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"FTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"CacheFTPHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"UnknownHandler\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"HTTPErrorProcessor\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"urlretrieve\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"urlcleanup\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"URLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"FancyURLopener\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"proxy_bypass\", \"urllib\", \"urllib.request\"),\n    MovedAttribute(\"parse_http_list\", \"urllib2\", \"urllib.request\"),\n    MovedAttribute(\"parse_keqv_list\", \"urllib2\", \"urllib.request\"),\n]\nfor attr in _urllib_request_moved_attributes:\n    setattr(Module_six_moves_urllib_request, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_request._moved_attributes = _urllib_request_moved_attributes\n\n_importer._add_module(\n    Module_six_moves_urllib_request(__name__ + \".moves.urllib.request\"),\n    \"moves.urllib_request\",\n    \"moves.urllib.request\",\n)\n\n\nclass Module_six_moves_urllib_response(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_response\"\"\"\n\n\n_urllib_response_moved_attributes = [\n    MovedAttribute(\"addbase\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addclosehook\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfo\", \"urllib\", \"urllib.response\"),\n    MovedAttribute(\"addinfourl\", \"urllib\", \"urllib.response\"),\n]\nfor attr in _urllib_response_moved_attributes:\n    setattr(Module_six_moves_urllib_response, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_response._moved_attributes = _urllib_response_moved_attributes\n\n_importer._add_module(\n    Module_six_moves_urllib_response(__name__ + \".moves.urllib.response\"),\n    \"moves.urllib_response\",\n    \"moves.urllib.response\",\n)\n\n\nclass Module_six_moves_urllib_robotparser(_LazyModule):\n\n    \"\"\"Lazy loading of moved objects in six.moves.urllib_robotparser\"\"\"\n\n\n_urllib_robotparser_moved_attributes = [\n    MovedAttribute(\"RobotFileParser\", \"robotparser\", \"urllib.robotparser\"),\n]\nfor attr in _urllib_robotparser_moved_attributes:\n    setattr(Module_six_moves_urllib_robotparser, attr.name, attr)\ndel attr\n\nModule_six_moves_urllib_robotparser._moved_attributes = (\n    _urllib_robotparser_moved_attributes\n)\n\n_importer._add_module(\n    Module_six_moves_urllib_robotparser(__name__ + \".moves.urllib.robotparser\"),\n    \"moves.urllib_robotparser\",\n    \"moves.urllib.robotparser\",\n)\n\n\nclass Module_six_moves_urllib(types.ModuleType):\n\n    \"\"\"Create a six.moves.urllib namespace that resembles the Python 3 namespace\"\"\"\n\n    __path__ = []  # mark as package\n    parse = _importer._get_module(\"moves.urllib_parse\")\n    error = _importer._get_module(\"moves.urllib_error\")\n    request = _importer._get_module(\"moves.urllib_request\")\n    response = _importer._get_module(\"moves.urllib_response\")\n    robotparser = _importer._get_module(\"moves.urllib_robotparser\")\n\n    def __dir__(self):\n        return [\"parse\", \"error\", \"request\", \"response\", \"robotparser\"]\n\n\n_importer._add_module(\n    Module_six_moves_urllib(__name__ + \".moves.urllib\"), \"moves.urllib\"\n)\n\n\ndef add_move(move):\n    \"\"\"Add an item to six.moves.\"\"\"\n    setattr(_MovedItems, move.name, move)\n\n\ndef remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))\n\n\nif PY3:\n    _meth_func = \"__func__\"\n    _meth_self = \"__self__\"\n\n    _func_closure = \"__closure__\"\n    _func_code = \"__code__\"\n    _func_defaults = \"__defaults__\"\n    _func_globals = \"__globals__\"\nelse:\n    _meth_func = \"im_func\"\n    _meth_self = \"im_self\"\n\n    _func_closure = \"func_closure\"\n    _func_code = \"func_code\"\n    _func_defaults = \"func_defaults\"\n    _func_globals = \"func_globals\"\n\n\ntry:\n    advance_iterator = next\nexcept NameError:\n\n    def advance_iterator(it):\n        return it.next()\n\n\nnext = advance_iterator\n\n\ntry:\n    callable = callable\nexcept NameError:\n\n    def callable(obj):\n        return any(\"__call__\" in klass.__dict__ for klass in type(obj).__mro__)\n\n\nif PY3:\n\n    def get_unbound_function(unbound):\n        return unbound\n\n    create_bound_method = types.MethodType\n\n    def create_unbound_method(func, cls):\n        return func\n\n    Iterator = object\nelse:\n\n    def get_unbound_function(unbound):\n        return unbound.im_func\n\n    def create_bound_method(func, obj):\n        return types.MethodType(func, obj, obj.__class__)\n\n    def create_unbound_method(func, cls):\n        return types.MethodType(func, None, cls)\n\n    class Iterator(object):\n        def next(self):\n            return type(self).__next__(self)\n\n    callable = callable\n_add_doc(\n    get_unbound_function, \"\"\"Get the function out of a possibly unbound function\"\"\"\n)\n\n\nget_method_function = operator.attrgetter(_meth_func)\nget_method_self = operator.attrgetter(_meth_self)\nget_function_closure = operator.attrgetter(_func_closure)\nget_function_code = operator.attrgetter(_func_code)\nget_function_defaults = operator.attrgetter(_func_defaults)\nget_function_globals = operator.attrgetter(_func_globals)\n\n\nif PY3:\n\n    def iterkeys(d, **kw):\n        return iter(d.keys(**kw))\n\n    def itervalues(d, **kw):\n        return iter(d.values(**kw))\n\n    def iteritems(d, **kw):\n        return iter(d.items(**kw))\n\n    def iterlists(d, **kw):\n        return iter(d.lists(**kw))\n\n    viewkeys = operator.methodcaller(\"keys\")\n\n    viewvalues = operator.methodcaller(\"values\")\n\n    viewitems = operator.methodcaller(\"items\")\nelse:\n\n    def iterkeys(d, **kw):\n        return d.iterkeys(**kw)\n\n    def itervalues(d, **kw):\n        return d.itervalues(**kw)\n\n    def iteritems(d, **kw):\n        return d.iteritems(**kw)\n\n    def iterlists(d, **kw):\n        return d.iterlists(**kw)\n\n    viewkeys = operator.methodcaller(\"viewkeys\")\n\n    viewvalues = operator.methodcaller(\"viewvalues\")\n\n    viewitems = operator.methodcaller(\"viewitems\")\n\n_add_doc(iterkeys, \"Return an iterator over the keys of a dictionary.\")\n_add_doc(itervalues, \"Return an iterator over the values of a dictionary.\")\n_add_doc(iteritems, \"Return an iterator over the (key, value) pairs of a dictionary.\")\n_add_doc(\n    iterlists, \"Return an iterator over the (key, [values]) pairs of a dictionary.\"\n)\n\n\nif PY3:\n\n    def b(s):\n        return s.encode(\"latin-1\")\n\n    def u(s):\n        return s\n\n    unichr = chr\n    import struct\n\n    int2byte = struct.Struct(\">B\").pack\n    del struct\n    byte2int = operator.itemgetter(0)\n    indexbytes = operator.getitem\n    iterbytes = iter\n    import io\n\n    StringIO = io.StringIO\n    BytesIO = io.BytesIO\n    del io\n    _assertCountEqual = \"assertCountEqual\"\n    if sys.version_info[1] <= 1:\n        _assertRaisesRegex = \"assertRaisesRegexp\"\n        _assertRegex = \"assertRegexpMatches\"\n        _assertNotRegex = \"assertNotRegexpMatches\"\n    else:\n        _assertRaisesRegex = \"assertRaisesRegex\"\n        _assertRegex = \"assertRegex\"\n        _assertNotRegex = \"assertNotRegex\"\nelse:\n\n    def b(s):\n        return s\n\n    # Workaround for standalone backslash\n\n    def u(s):\n        return unicode(s.replace(r\"\\\\\", r\"\\\\\\\\\"), \"unicode_escape\")\n\n    unichr = unichr\n    int2byte = chr\n\n    def byte2int(bs):\n        return ord(bs[0])\n\n    def indexbytes(buf, i):\n        return ord(buf[i])\n\n    iterbytes = functools.partial(itertools.imap, ord)\n    import StringIO\n\n    StringIO = BytesIO = StringIO.StringIO\n    _assertCountEqual = \"assertItemsEqual\"\n    _assertRaisesRegex = \"assertRaisesRegexp\"\n    _assertRegex = \"assertRegexpMatches\"\n    _assertNotRegex = \"assertNotRegexpMatches\"\n_add_doc(b, \"\"\"Byte literal\"\"\")\n_add_doc(u, \"\"\"Text literal\"\"\")\n\n\ndef assertCountEqual(self, *args, **kwargs):\n    return getattr(self, _assertCountEqual)(*args, **kwargs)\n\n\ndef assertRaisesRegex(self, *args, **kwargs):\n    return getattr(self, _assertRaisesRegex)(*args, **kwargs)\n\n\ndef assertRegex(self, *args, **kwargs):\n    return getattr(self, _assertRegex)(*args, **kwargs)\n\n\ndef assertNotRegex(self, *args, **kwargs):\n    return getattr(self, _assertNotRegex)(*args, **kwargs)\n\n\nif PY3:\n    exec_ = getattr(moves.builtins, \"exec\")\n\n    def reraise(tp, value, tb=None):\n        try:\n            if value is None:\n                value = tp()\n            if value.__traceback__ is not tb:\n                raise value.with_traceback(tb)\n            raise value\n        finally:\n            value = None\n            tb = None\n\nelse:\n\n    def exec_(_code_, _globs_=None, _locs_=None):\n        \"\"\"Execute code in a namespace.\"\"\"\n        if _globs_ is None:\n            frame = sys._getframe(1)\n            _globs_ = frame.f_globals\n            if _locs_ is None:\n                _locs_ = frame.f_locals\n            del frame\n        elif _locs_ is None:\n            _locs_ = _globs_\n        exec (\"\"\"exec _code_ in _globs_, _locs_\"\"\")\n\n    exec_(\n        \"\"\"def reraise(tp, value, tb=None):\n    try:\n        raise tp, value, tb\n    finally:\n        tb = None\n\"\"\"\n    )\n\n\nif sys.version_info[:2] > (3,):\n    exec_(\n        \"\"\"def raise_from(value, from_value):\n    try:\n        raise value from from_value\n    finally:\n        value = None\n\"\"\"\n    )\nelse:\n\n    def raise_from(value, from_value):\n        raise value\n\n\nprint_ = getattr(moves.builtins, \"print\", None)\nif print_ is None:\n\n    def print_(*args, **kwargs):\n        \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n        fp = kwargs.pop(\"file\", sys.stdout)\n        if fp is None:\n            return\n\n        def write(data):\n            if not isinstance(data, basestring):\n                data = str(data)\n            # If the file has an encoding, encode unicode with it.\n            if (\n                isinstance(fp, file)\n                and isinstance(data, unicode)\n                and fp.encoding is not None\n            ):\n                errors = getattr(fp, \"errors\", None)\n                if errors is None:\n                    errors = \"strict\"\n                data = data.encode(fp.encoding, errors)\n            fp.write(data)\n\n        want_unicode = False\n        sep = kwargs.pop(\"sep\", None)\n        if sep is not None:\n            if isinstance(sep, unicode):\n                want_unicode = True\n            elif not isinstance(sep, str):\n                raise TypeError(\"sep must be None or a string\")\n        end = kwargs.pop(\"end\", None)\n        if end is not None:\n            if isinstance(end, unicode):\n                want_unicode = True\n            elif not isinstance(end, str):\n                raise TypeError(\"end must be None or a string\")\n        if kwargs:\n            raise TypeError(\"invalid keyword arguments to print()\")\n        if not want_unicode:\n            for arg in args:\n                if isinstance(arg, unicode):\n                    want_unicode = True\n                    break\n        if want_unicode:\n            newline = unicode(\""
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docker-compose.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docs/github-workflows/autonomous-value-discovery.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docs/github-workflows/ci.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "docs/github-workflows/performance.yml",
      "line_number": null,
      "details": null
    },
    {
      "category": "config_credentials",
      "severity": "high",
      "description": "Hardcoded credentials in config file",
      "file_path": "security_audit_report.json",
      "line_number": null,
      "details": null
    },
    {
      "category": "dangerous_functions",
      "severity": "high",
      "description": "Use of dangerous function: exec",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/six.py",
      "line_number": 735,
      "details": null
    },
    {
      "category": "dangerous_functions",
      "severity": "high",
      "description": "Use of dangerous function: exec",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pkg_resources/__init__.py",
      "line_number": 1561,
      "details": null
    },
    {
      "category": "dangerous_functions",
      "severity": "high",
      "description": "Use of dangerous function: exec",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pkg_resources/__init__.py",
      "line_number": 1572,
      "details": null
    },
    {
      "category": "dangerous_functions",
      "severity": "high",
      "description": "Use of dangerous function: exec",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pygments/formatters/__init__.py",
      "line_number": 103,
      "details": null
    },
    {
      "category": "dangerous_functions",
      "severity": "high",
      "description": "Use of dangerous function: exec",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/pygments/lexers/__init__.py",
      "line_number": 153,
      "details": null
    },
    {
      "category": "dangerous_functions",
      "severity": "high",
      "description": "Use of dangerous function: exec",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/urllib3/packages/six.py",
      "line_number": 787,
      "details": null
    },
    {
      "category": "hardcoded_secrets",
      "severity": "high",
      "description": "Potential hardcoded secrets detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_internal/utils/misc.py",
      "line_number": 2,
      "details": "')\n\n    Taken from Lib/support/__init__.py in the CPython repo.\n    \"\"\"\n    return captured_output(\"stdout\")\n\n\ndef captured_stderr() -> ContextManager[StreamWrapper]:\n    \"\"\"\n    See captured_stdout().\n    \"\"\"\n    return captured_output(\"stderr\")\n\n\n# Simulates an enum\ndef enum(*sequential: Any, **named: Any) -> Type[Any]:\n    enums = dict(zip(sequential, range(len(sequential))), **named)\n    reverse = {value: key for key, value in enums.items()}\n    enums[\"reverse_mapping\"] = reverse\n    return type(\"Enum\", (), enums)\n\n\ndef build_netloc(host: str, port: Optional[int]) -> str:\n    \"\"\"\n    Build a netloc from a host-port pair\n    \"\"\"\n    if port is None:\n        return host\n    if \":\" in host:\n        # Only wrap host with square brackets when it is IPv6\n        host = f\"[{host}]\"\n    return f\"{host}:{port}\"\n\n\ndef build_url_from_netloc(netloc: str, scheme: str = \"https\") -> str:\n    \"\"\"\n    Build a full URL from a netloc.\n    \"\"\"\n    if netloc.count(\":\") >= 2 and \"@\" not in netloc and \"[\" not in netloc:\n        # It must be a bare IPv6 address, so wrap it with brackets.\n        netloc = f\"[{netloc}]\"\n    return f\"{scheme}://{netloc}\"\n\n\ndef parse_netloc(netloc: str) -> Tuple[Optional[str], Optional[int]]:\n    \"\"\"\n    Return the host-port pair from a netloc.\n    \"\"\"\n    url = build_url_from_netloc(netloc)\n    parsed = urllib.parse.urlparse(url)\n    return parsed.hostname, parsed.port\n\n\ndef split_auth_from_netloc(netloc: str) -> NetlocTuple:\n    \"\"\"\n    Parse out and remove the auth information from a netloc.\n\n    Returns: (netloc, (username, password)).\n    \"\"\"\n    if \"@\" not in netloc:\n        return netloc, (None, None)\n\n    # Split from the right because that's how urllib.parse.urlsplit()\n    # behaves if more than one @ is present (which can be checked using\n    # the password attribute of urlsplit()'s return value).\n    auth, netloc = netloc.rsplit(\"@\", 1)\n    pw: Optional[str] = None\n    if \":\" in auth:\n        # Split from the left because that's how urllib.parse.urlsplit()\n        # behaves if more than one : is present (which again can be checked\n        # using the password attribute of the return value)\n        user, pw = auth.split(\":\", 1)\n    else:\n        user, pw = auth, None\n\n    user = urllib.parse.unquote(user)\n    if pw is not None:\n        pw = urllib.parse.unquote(pw)\n\n    return netloc, (user, pw)\n\n\ndef redact_netloc(netloc: str) -> str:\n    \"\"\"\n    Replace the sensitive data in a netloc with \"****\", if it exists.\n\n    For example:\n        - \"user:pass@example.com\" returns \"user:****@example.com\"\n        - \"accesstoken@example.com\" returns \"****@example.com\"\n    \"\"\"\n    netloc, (user, password) = split_auth_from_netloc(netloc)\n    if user is None:\n        return netloc\n    if password is None:\n        user = \"****\"\n        password = \"\"\n    else:\n        user = urllib.parse.quote(user)\n        password = \":****\"\n    return f\"{user}{password}@{netloc}\"\n\n\ndef _transform_url(\n    url: str, transform_netloc: Callable[[str], Tuple[Any, ...]]\n) -> Tuple[str, NetlocTuple]:\n    \"\"\"Transform and replace netloc in a url.\n\n    transform_netloc is a function taking the netloc and returning a\n    tuple. The first element of this tuple is the new netloc. The\n    entire tuple is returned.\n\n    Returns a tuple containing the transformed url as item 0 and the\n    original tuple returned by transform_netloc as item 1.\n    \"\"\"\n    purl = urllib.parse.urlsplit(url)\n    netloc_tuple = transform_netloc(purl.netloc)\n    # stripped url\n    url_pieces = (purl.scheme, netloc_tuple[0], purl.path, purl.query, purl.fragment)\n    surl = urllib.parse.urlunsplit(url_pieces)\n    return surl, cast(\"NetlocTuple\", netloc_tuple)\n\n\ndef _get_netloc(netloc: str) -> NetlocTuple:\n    return split_auth_from_netloc(netloc)\n\n\ndef _redact_netloc(netloc: str) -> Tuple[str]:\n    return (redact_netloc(netloc),)\n\n\ndef split_auth_netloc_from_url(\n    url: str,\n) -> Tuple[str, str, Tuple[Optional[str], Optional[str]]]:\n    \"\"\"\n    Parse a url into separate netloc, auth, and url with no auth.\n\n    Returns: (url_without_auth, netloc, (username, password))\n    \"\"\"\n    url_without_auth, (netloc, auth) = _transform_url(url, _get_netloc)\n    return url_without_auth, netloc, auth\n\n\ndef remove_auth_from_url(url: str) -> str:\n    \"\"\"Return a copy of url with 'username:password@' removed.\"\"\"\n    # username/pass params are passed to subversion through flags\n    # and are not recognized in the url.\n    return _transform_url(url, _get_netloc)[0]\n\n\ndef redact_auth_from_url(url: str) -> str:\n    \"\"\"Replace the password in a given url with ****.\"\"\"\n    return _transform_url(url, _redact_netloc)[0]\n\n\ndef redact_auth_from_requirement(req: Requirement) -> str:\n    \"\"\"Replace the password in a given requirement url with ****.\"\"\"\n    if not req.url:\n        return str(req)\n    return str(req).replace(req.url, redact_auth_from_url(req.url))\n\n\nclass HiddenText:\n    def __init__(self, secret: str, redacted: str) -> None:\n        self.secret = secret\n        self.redacted = redacted\n\n    def __repr__(self) -> str:\n        return f\"<HiddenText {str(self)!r}>\"\n\n    def __str__(self) -> str:\n        return self.redacted\n\n    # This is useful for testing.\n    def __eq__(self, other: Any) -> bool:\n        if type(self) != type(other):\n            return False\n\n        # The string being used for redaction doesn't also have to match,\n        # just the raw, original string.\n        return self.secret == other.secret\n\n\ndef hide_value(value: str) -> HiddenText:\n    return HiddenText(value, redacted=\"****\")\n\n\ndef hide_url(url: str) -> HiddenText:\n    redacted = redact_auth_from_url(url)\n    return HiddenText(url, redacted=redacted)\n\n\ndef protect_pip_from_modification_on_windows(modifying_pip: bool) -> None:\n    \"\"\"Protection of pip.exe from modification on Windows\n\n    On Windows, any operation modifying pip should be run as:\n        python -m pip ...\n    \"\"\"\n    pip_names = [\n        \"pip\",\n        f\"pip{sys.version_info.major}\",\n        f\"pip{sys.version_info.major}.{sys.version_info.minor}\",\n    ]\n\n    # See https://github.com/pypa/pip/issues/1299 for more discussion\n    should_show_use_python_msg = (\n        modifying_pip and WINDOWS and os.path.basename(sys.argv[0]) in pip_names\n    )\n\n    if should_show_use_python_msg:\n        new_command = [sys.executable, \"-m\", \"pip\"] + sys.argv[1:]\n        raise CommandError(\n            \"To modify pip, please run the following command:"
    },
    {
      "category": "git_history_secrets",
      "severity": "medium",
      "description": "Potential secrets found in git commit messages",
      "file_path": "git_history",
      "line_number": null,
      "details": "ad1cf68 feat(ci): add comprehensive GitHub Actions workflows and documentation\n14ef4ca \ud83d\ude80 Comprehensive SDLC Enhancement: Developing to Maturing Maturity\n3e48f33 \ud83d\ude80 Foundational SDLC Enhancement: Nascent to Developing Maturity"
    },
    {
      "category": "pickle_usage",
      "severity": "medium",
      "description": "Pickle usage detected - can execute arbitrary code",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/json.py",
      "line_number": 37,
      "details": null
    },
    {
      "category": "pickle_usage",
      "severity": "medium",
      "description": "Pickle usage detected - can execute arbitrary code",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/style.py",
      "line_number": 475,
      "details": null
    },
    {
      "category": "pickle_usage",
      "severity": "medium",
      "description": "Pickle usage detected - can execute arbitrary code",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/tomli/_parser.py",
      "line_number": 66,
      "details": null
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "examples/advanced_algorithms_showcase.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python3\n\"\"\"Advanced Algorithms Showcase Example.\n\nThis example demonstrates all advanced algorithms implemented in the research module:\n1. Analog Physics-Informed Crossbar Networks (APICNs)\n2. Temporal Crossbar Cascading (TCC)\n3. Heterogeneous Precision Analog Computing (HPAC)\n4. Analog Multi-Physics Coupling (AMPC)\n5. Neuromorphic PDE Acceleration (NPA)\n\nIt shows how to use the integrated framework for automatic algorithm selection\nand provides comprehensive examples of breakthrough performance capabilities.\n\"\"\"\n\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Add the project root to the path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom analog_pde_solver.core.equations import PoissonEquation, HeatEquation, WaveEquation\nfrom analog_pde_solver.research.integrated_solver_framework import (\n    AdvancedSolverFramework,\n    AlgorithmType,\n    ProblemCharacteristics\n)\nfrom analog_pde_solver.research.validation_benchmark_suite import (\n    ValidationBenchmarkSuite,\n    BenchmarkType\n)\n\n\ndef demonstrate_physics_informed_crossbar():\n    \"\"\"Demonstrate Analog Physics-Informed Crossbar Networks (APICNs).\"\"\"\n    print(\"=\" * 60)\n    print(\"ANALOG PHYSICS-INFORMED CROSSBAR NETWORKS (APICNs)\")\n    print(\"=\" * 60)\n    print(\"Embedding physics constraints directly into crossbar hardware\")\n    print()\n    \n    # Create a Poisson equation with conservation requirements\n    pde = PoissonEquation(\n        domain_size=(128, 128),\n        boundary_conditions='dirichlet',\n        source_function=lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n    )\n    \n    # Initialize framework\n    framework = AdvancedSolverFramework(\n        base_crossbar_size=128,\n        performance_mode='accuracy'\n    )\n    \n    # Problem characteristics that favor physics-informed approach\n    characteristics = ProblemCharacteristics(\n        problem_size=(128, 128),\n        sparsity_level=0.2,\n        time_dependent=False,\n        multi_physics=False,\n        conservation_required=True,  # Key for physics-informed\n        accuracy_requirement=1e-8,\n        energy_budget=None,\n        real_time_requirement=False,\n        physics_constraints=['conservation', 'symmetry'],  # Physics constraints\n        boundary_complexity='simple'\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Domain size: {characteristics.problem_size}\")\n    print(f\"- Conservation required: {characteristics.conservation_required}\")\n    print(f\"- Physics constraints: {characteristics.physics_constraints}\")\n    print(f\"- Accuracy requirement: {characteristics.accuracy_requirement}\")\n    print()\n    \n    try:\n        # Solve with physics-informed algorithm\n        print(\"Solving with APICNs...\")\n        solution, solve_info = framework.solve_pde(\n            pde,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.PHYSICS_INFORMED\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Solution range: [{np.min(solution):.6f}, {np.max(solution):.6f}]\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n            print(f\"- Reasoning: {rec['reasoning']}\")\n        \n    except Exception as e:\n        print(f\"Physics-informed demonstration failed: {e}\")\n    \n    print()\n\n\ndef demonstrate_temporal_cascading():\n    \"\"\"Demonstrate Temporal Crossbar Cascading (TCC).\"\"\"\n    print(\"=\" * 60)\n    print(\"TEMPORAL CROSSBAR CASCADING (TCC)\")\n    print(\"=\" * 60)\n    print(\"Hardware pipelining of temporal discretization for 100\u00d7 speedup\")\n    print()\n    \n    # Create a time-dependent heat equation\n    heat_equation = HeatEquation(\n        domain_size=(128,),\n        boundary_conditions='dirichlet',\n        initial_condition=lambda x: np.sin(np.pi * x),\n        diffusivity=0.1\n    )\n    \n    framework = AdvancedSolverFramework(\n        base_crossbar_size=128,\n        performance_mode='speed'\n    )\n    \n    # Problem characteristics that favor temporal cascading\n    characteristics = ProblemCharacteristics(\n        problem_size=(128,),\n        sparsity_level=0.1,\n        time_dependent=True,  # Key for temporal cascading\n        multi_physics=False,\n        conservation_required=False,\n        accuracy_requirement=1e-6,\n        energy_budget=None,\n        real_time_requirement=True,  # Real-time favors cascading\n        physics_constraints=[],\n        boundary_complexity='simple'\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Domain size: {characteristics.problem_size}\")\n    print(f\"- Time dependent: {characteristics.time_dependent}\")\n    print(f\"- Real-time requirement: {characteristics.real_time_requirement}\")\n    print(f\"- Expected speedup: 100\u00d7\")\n    print()\n    \n    try:\n        # Solve with temporal cascading\n        print(\"Solving with TCC...\")\n        solution, solve_info = framework.solve_pde(\n            heat_equation,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.TEMPORAL_CASCADE,\n            time_span=(0.0, 1.0),\n            num_time_steps=100,\n            initial_solution=np.sin(np.pi * np.linspace(0, 1, 128))\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Final solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Pipeline stages: 4\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Estimated speedup: {rec['estimated_speedup']:.0f}\u00d7\")\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n        \n    except Exception as e:\n        print(f\"Temporal cascading demonstration failed: {e}\")\n    \n    print()\n\n\ndef demonstrate_heterogeneous_precision():\n    \"\"\"Demonstrate Heterogeneous Precision Analog Computing (HPAC).\"\"\"\n    print(\"=\" * 60)\n    print(\"HETEROGENEOUS PRECISION ANALOG COMPUTING (HPAC)\")\n    print(\"=\" * 60)\n    print(\"Adaptive precision allocation for 50\u00d7 energy reduction\")\n    print()\n    \n    # Create a large multi-scale problem\n    multiscale_pde = PoissonEquation(\n        domain_size=(256, 256),\n        boundary_conditions='dirichlet',\n        source_function=lambda x, y: (np.sin(5 * np.pi * x) * np.sin(5 * np.pi * y) +\n                                    0.1 * np.sin(50 * np.pi * x) * np.sin(50 * np.pi * y))\n    )\n    \n    framework = AdvancedSolverFramework(\n        base_crossbar_size=256,\n        performance_mode='energy'\n    )\n    \n    # Problem characteristics that favor heterogeneous precision\n    characteristics = ProblemCharacteristics(\n        problem_size=(256, 256),\n        sparsity_level=0.3,\n        time_dependent=False,\n        multi_physics=False,\n        conservation_required=False,\n        accuracy_requirement=1e-6,\n        energy_budget=1.0,  # Energy budget constraint\n        real_time_requirement=False,\n        physics_constraints=[],\n        boundary_complexity='complex'  # Multi-scale = complex\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Domain size: {characteristics.problem_size}\")\n    print(f\"- Multi-scale features: Yes (5\u00d7 and 50\u00d7 frequencies)\")\n    print(f\"- Energy budget: {characteristics.energy_budget}\")\n    print(f\"- Expected energy reduction: 50\u00d7\")\n    print()\n    \n    try:\n        # Solve with heterogeneous precision\n        print(\"Solving with HPAC...\")\n        solution, solve_info = framework.solve_pde(\n            multiscale_pde,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.HETEROGENEOUS_PRECISION,\n            initial_solution=np.random.random(256*256)\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Precision levels used: LOW, MEDIUM, HIGH, ULTRA\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Estimated energy savings: {rec['estimated_energy_savings']:.1%}\")\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n        \n        # Show precision distribution (simulated)\n        print(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "examples/advanced_algorithms_showcase.py",
      "line_number": 2,
      "details": "Precision Allocation:\")\n        print(\"- Low precision regions: 40% (smooth areas)\")\n        print(\"- Medium precision regions: 35% (moderate gradients)\")\n        print(\"- High precision regions: 20% (sharp features)\")\n        print(\"- Ultra precision regions: 5% (critical boundaries)\")\n        \n    except Exception as e:\n        print(f\"Heterogeneous precision demonstration failed: {e}\")\n    \n    print()\n\n\ndef demonstrate_multi_physics_coupling():\n    \"\"\"Demonstrate Analog Multi-Physics Coupling (AMPC).\"\"\"\n    print(\"=\" * 60)\n    print(\"ANALOG MULTI-PHYSICS COUPLING (AMPC)\")\n    print(\"=\" * 60)\n    print(\"Direct analog coupling eliminating 90% of interface overhead\")\n    print()\n    \n    framework = AdvancedSolverFramework(\n        base_crossbar_size=128,\n        enable_multi_physics=True,\n        performance_mode='balanced'\n    )\n    \n    # Multi-physics problem characteristics\n    characteristics = ProblemCharacteristics(\n        problem_size=(128, 128),\n        sparsity_level=0.2,\n        time_dependent=True,\n        multi_physics=True,  # Key for multi-physics coupling\n        conservation_required=True,  # Conservation across domains\n        accuracy_requirement=1e-6,\n        energy_budget=None,\n        real_time_requirement=False,\n        physics_constraints=['conservation', 'coupling'],\n        boundary_complexity='complex'\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Coupled domains: Thermal + Fluid\")\n    print(f\"- Domain size per physics: 64\u00d764 each\")\n    print(f\"- Conservation required: {characteristics.conservation_required}\")\n    print(f\"- Coupling type: Bidirectional thermal-fluid\")\n    print(f\"- Expected interface overhead reduction: 90%\")\n    print()\n    \n    try:\n        # Create dummy PDE for multi-physics (framework handles the coupling)\n        dummy_pde = PoissonEquation(\n            domain_size=(64, 64),\n            boundary_conditions='dirichlet'\n        )\n        \n        print(\"Solving with AMPC...\")\n        solution, solve_info = framework.solve_pde(\n            dummy_pde,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.MULTI_PHYSICS,\n            time_span=(0.0, 1.0),\n            num_time_steps=50,\n            initial_conditions={\n                'thermal': np.random.random(64),\n                'fluid': np.random.random(64)\n            }\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Combined solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Coupling domains: 2 (thermal, fluid)\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Estimated speedup: {rec['estimated_speedup']:.0f}\u00d7\")\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n        \n        print(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/conftest.py",
      "line_number": 1,
      "details": "import pytest\nimport numpy as np\nimport tempfile\nimport os\nfrom pathlib import Path\n\n\n@pytest.fixture\ndef temp_dir():\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    with tempfile.TemporaryDirectory() as tmpdir:\n        yield Path(tmpdir)\n\n\n@pytest.fixture\ndef sample_grid():\n    \"\"\"Create a sample 2D grid for testing.\"\"\"\n    return np.meshgrid(np.linspace(0, 1, 32), np.linspace(0, 1, 32))\n\n\n@pytest.fixture\ndef mock_spice_simulator():\n    \"\"\"Mock SPICE simulator for testing without external dependencies.\"\"\"\n    class MockSpiceSimulator:\n        def __init__(self):\n            self.components = []\n            self.simulated = False\n            \n        def add_component(self, name, type_, **kwargs):\n            self.components.append({\"name\": name, \"type\": type_, **kwargs})\n            \n        def simulate(self, **kwargs):\n            self.simulated = True\n            return {\"success\": True, \"results\": np.random.random((10, 10))}\n    \n    return MockSpiceSimulator()\n\n\n@pytest.fixture\ndef sample_conductance_matrix():\n    \"\"\"Sample conductance matrix for crossbar testing.\"\"\"\n    return np.random.uniform(1e-9, 1e-6, (16, 16))\n\n\n@pytest.fixture(scope=\"session\")\ndef test_data_dir():\n    \"\"\"Path to test data directory.\"\"\"\n    return Path(__file__).parent / \"data\"\n\n\n@pytest.fixture\ndef enable_slow_tests(request):\n    \"\"\"Enable slow tests when explicitly requested.\"\"\"\n    if request.config.getoption(\"--runslow\"):\n        return True\n    pytest.skip(\"need --runslow option to run\")\n\n\ndef pytest_addoption(parser):\n    \"\"\"Add custom command line options.\"\"\"\n    parser.addoption(\n        \"--runslow\", action=\"store_true\", default=False, help=\"run slow tests\"\n    )\n    parser.addoption(\n        \"--runhardware\", action=\"store_true\", default=False, help=\"run hardware tests\"\n    )"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/test_advanced_algorithms.py",
      "line_number": 1,
      "details": "\"\"\"Comprehensive tests for advanced analog algorithms.\n\nTests all novel algorithms implemented in the research module:\n- Analog Physics-Informed Crossbar Networks (APICNs)\n- Temporal Crossbar Cascading (TCC)\n- Heterogeneous Precision Analog Computing (HPAC)\n- Analog Multi-Physics Coupling (AMPC)\n- Neuromorphic PDE Acceleration (NPA)\n- Integrated Solver Framework\n- Validation and Benchmarking\n\"\"\"\n\nimport pytest\nimport numpy as np\nimport sys\nfrom pathlib import Path\nfrom unittest.mock import Mock, patch\nfrom typing import Dict, Any, List\n\n# Add project root to path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom analog_pde_solver.core.solver import AnalogPDESolver\nfrom analog_pde_solver.core.crossbar import AnalogCrossbarArray\nfrom analog_pde_solver.core.equations import PoissonEquation, HeatEquation\n\nfrom analog_pde_solver.research.advanced_analog_algorithms import (\n    AnalogPhysicsInformedCrossbar,\n    TemporalCrossbarCascade,\n    HeterogeneousPrecisionAnalogComputing,\n    LocalErrorEstimator,\n    PrecisionLevel,\n    CrossbarRegion,\n    PhysicsConstraint\n)\n\nfrom analog_pde_solver.research.multi_physics_coupling import (\n    AnalogMultiPhysicsCoupler,\n    PhysicsDomain,\n    PhysicsDomainConfig,\n    CouplingInterface\n)\n\nfrom analog_pde_solver.research.neuromorphic_acceleration import (\n    NeuromorphicPDESolver,\n    NeuromorphicSpikeEncoder,\n    NeuromorphicSpikeDecoder,\n    SparseEventBuffer,\n    SpikeEncoding,\n    SpikeEvent,\n    NeuronState\n)\n\nfrom analog_pde_solver.research.ml_acceleration import (\n    NeuralNetworkSurrogate,\n    PhysicsInformedSurrogate,\n    MLAcceleratedPDESolver,\n    TrainingData\n)\n\nfrom analog_pde_solver.research.integrated_solver_framework import (\n    AdvancedSolverFramework,\n    AlgorithmType,\n    ProblemCharacteristics,\n    AlgorithmSelector,\n    PerformanceTracker\n)\n\nfrom analog_pde_solver.research.validation_benchmark_suite import (\n    ValidationBenchmarkSuite,\n    BenchmarkType,\n    BenchmarkProblem,\n    BenchmarkResult\n)\n\n\nclass TestAnalogPhysicsInformedCrossbar:\n    \"\"\"Test Analog Physics-Informed Crossbar Networks.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.base_crossbar = AnalogCrossbarArray(32, 32)\n        \n        # Create physics constraints\n        self.physics_constraints = [\n            PhysicsConstraint(\n                constraint_type='conservation',\n                constraint_function=lambda x: np.sum(x),\n                weight=1.0,\n                conductance_mapping=None,\n                active_regions=[(0, 16, 0, 16)],\n                conservation_required=True,\n                bidirectional=False\n            ),\n            PhysicsConstraint(\n                constraint_type='symmetry',\n                constraint_function=None,\n                weight=0.5,\n                conductance_mapping=None,\n                active_regions=[(16, 32, 16, 32)],\n                conservation_required=False,\n                bidirectional=True\n            )\n        ]\n        \n        self.apicn = AnalogPhysicsInformedCrossbar(\n            self.base_crossbar,\n            self.physics_constraints,\n            residual_threshold=1e-6,\n            adaptation_rate=0.01\n        )\n    \n    def test_initialization(self):\n        \"\"\"Test APICN initialization.\"\"\"\n        assert self.apicn.base_crossbar is self.base_crossbar\n        assert len(self.apicn.physics_constraints) == 2\n        assert len(self.apicn.physics_conductances) == 2\n        assert 'constraint_0' in self.apicn.physics_conductances\n        assert 'constraint_1' in self.apicn.physics_conductances\n    \n    def test_physics_aware_programming(self):\n        \"\"\"Test physics-aware conductance programming.\"\"\"\n        target_matrix = np.random.random((32, 32))\n        \n        metrics = self.apicn.program_physics_aware_conductances(target_matrix)\n        \n        # Check that metrics are returned\n        assert 'total_violation' in metrics\n        assert 'programming_time' in metrics\n        assert 'constraints_satisfied' in metrics\n        \n        # Check that conductances were modified\n        assert not np.array_equal(\n            self.apicn.base_crossbar.conductance_matrix,\n            target_matrix\n        )\n    \n    def test_solve_with_physics_constraints(self):\n        \"\"\"Test solving with physics constraint enforcement.\"\"\"\n        input_vector = np.random.random(32)\n        \n        solution, metrics = self.apicn.solve_with_physics_constraints(\n            input_vector,\n            max_physics_iterations=10\n        )\n        \n        # Check solution properties\n        assert solution.shape == (32,)\n        assert np.isfinite(solution).all()\n        \n        # Check metrics\n        assert 'iterations' in metrics\n        assert 'final_violation' in metrics\n        assert 'solve_time' in metrics\n        assert metrics['iterations'] <= 10\n    \n    def test_constraint_residual_computation(self):\n        \"\"\"Test physics constraint residual computation.\"\"\"\n        conductances = np.random.random((32, 32))\n        \n        for constraint in self.physics_constraints:\n            residual = self.apicn._compute_constraint_residual(\n                constraint, conductances\n            )\n            \n            assert isinstance(residual, (int, float))\n            assert np.isfinite(residual)\n    \n    def test_conductance_adjustment(self):\n        \"\"\"Test conductance adjustment computation.\"\"\"\n        conductances = np.random.random((32, 32))\n        \n        for constraint in self.physics_constraints:\n            residual = 0.1  # Test residual\n            \n            adjustment = self.apicn._compute_conductance_adjustment(\n                constraint, residual, conductances\n            )\n            \n            assert adjustment.shape == conductances.shape\n            assert np.isfinite(adjustment).all()\n\n\nclass TestTemporalCrossbarCascade:\n    \"\"\"Test Temporal Crossbar Cascading.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.cascade_crossbars = [\n            AnalogCrossbarArray(16, 16) for _ in range(4)\n        ]\n        \n        self.tcc = TemporalCrossbarCascade(\n            self.cascade_crossbars,\n            time_step=0.01,\n            temporal_scheme='forward_euler',\n            cascade_depth=4\n        )\n    \n    def test_initialization(self):\n        \"\"\"Test TCC initialization.\"\"\"\n        assert len(self.tcc.base_crossbars) == 4\n        assert self.tcc.time_step == 0.01\n        assert self.tcc.temporal_scheme == 'forward_euler'\n        assert self.tcc.cascade_depth == 4\n    \n    def test_temporal_pipeline_setup(self):\n        \"\"\"Test temporal pipeline setup.\"\"\"\n        spatial_operator = np.random.random((16, 16))\n        boundary_conditions = {'dirichlet': True, 'dirichlet_value': 0.0}\n        \n        # Should not raise an exception\n        self.tcc.setup_temporal_pipeline(spatial_operator, boundary_conditions)\n        \n        # Check that crossbars were programmed\n        for crossbar in self.tcc.base_crossbars:\n            assert not np.array_equal(\n                crossbar.conductance_matrix,\n                np.zeros((16, 16))\n            )\n    \n    def test_sequential_pipeline_evolution(self):\n        \"\"\"Test sequential pipeline evolution.\"\"\"\n        spatial_operator = np.eye(16)  # Simple identity operator\n        boundary_conditions = {'dirichlet': True}\n        \n        self.tcc.setup_temporal_pipeline(spatial_operator, boundary_conditions)\n        \n        initial_state = np.random.random(16)\n        \n        final_state, metrics = self.tcc.evolve_temporal_pipeline(\n            initial_state,\n            num_time_steps=10,\n            parallel_execution=False\n        )\n        \n        # Check results\n        assert final_state.shape == initial_state.shape\n        assert np.isfinite(final_state).all()\n        \n        # Check metrics\n        assert 'time_steps' in metrics\n        assert 'speedup_vs_sequential' in metrics\n        assert 'evolution_time' in metrics\n        assert metrics['time_steps'] == 10\n    \n    def test_parallel_pipeline_evolution(self):\n        \"\"\"Test parallel pipeline evolution.\"\"\"\n        spatial_operator = np.eye(16)\n        boundary_conditions = {'dirichlet': True}\n        \n        self.tcc.setup_temporal_pipeline(spatial_operator, boundary_conditions)\n        \n        initial_state = np.random.random(16)\n        \n        final_state, metrics = self.tcc.evolve_temporal_pipeline(\n            initial_state,\n            num_time_steps=5,  # Fewer steps for parallel test\n            parallel_execution=True\n        )\n        \n        assert final_state.shape == initial_state.shape\n        assert np.isfinite(final_state).all()\n        assert 'pipeline_efficiency' in metrics\n\n\nclass TestHeterogeneousPrecisionAnalogComputing:\n    \"\"\"Test Heterogeneous Precision Analog Computing.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.base_crossbar = AnalogCrossbarArray(64, 64)\n        self.hpac = HeterogeneousPrecisionAnalogComputing(\n            self.base_crossbar,\n            precision_levels=list(PrecisionLevel),\n            adaptation_threshold=1e-4,\n            energy_weight=0.5\n        )\n    \n    def test_initialization(self):\n        \"\"\"Test HPAC initialization.\"\"\"\n        assert self.hpac.base_crossbar is self.base_crossbar\n        assert len(self.hpac.precision_levels) == 4\n        assert len(self.hpac.crossbar_regions) > 0\n    \n    def test_region_initialization(self):\n        \"\"\"Test crossbar region initialization.\"\"\"\n        regions = self.hpac._initialize_regions()\n        \n        assert len(regions) == 16  # 4x4 grid\n        \n        for region in regions:\n            assert isinstance(region, CrossbarRegion)\n            assert region.precision in PrecisionLevel\n            assert 0 <= region.start_row < region.end_row <= 64\n            assert 0 <= region.start_col < region.end_col <= 64\n    \n    def test_precision_adaptation(self):\n        \"\"\"Test precision adaptation.\"\"\"\n        current_solution = np.random.random(64)\n        \n        metrics = self.hpac.adapt_precision_allocation(\n            current_solution,\n            target_accuracy=1e-6\n        )\n        \n        # Check metrics\n        assert 'regions_adapted' in metrics\n        assert 'energy_reduction' in metrics\n        assert 'adaptation_time' in metrics\n        assert 'precision_distribution' in metrics\n    \n    def test_heterogeneous_vmm(self):\n        \"\"\"Test heterogeneous precision VMM computation.\"\"\"\n        input_vector = np.random.random(64)\n        \n        output, metrics = self.hpac.compute_heterogeneous_vmm(input_vector)\n        \n        # Check output\n        assert output.shape == (64,)\n        assert np.isfinite(output).all()\n        \n        # Check metrics\n        assert 'total_energy' in metrics\n        assert 'estimated_accuracy' in metrics\n        assert 'computation_time' in metrics\n        assert 'precision_utilization' in metrics\n    \n    def test_region_energy_computation(self):\n        \"\"\"Test region energy computation.\"\"\"\n        region = self.hpac.crossbar_regions[0]\n        \n        for precision in PrecisionLevel:\n            energy = self.hpac._compute_region_energy(region, precision)\n            assert energy >= 0\n            assert np.isfinite(energy)\n    \n    def test_optimal_precision_selection(self):\n        \"\"\"Test optimal precision selection.\"\"\"\n        region_error = 1e-5\n        target_accuracy = 1e-6\n        current_precision = PrecisionLevel.MEDIUM\n        \n        new_precision = self.hpac._select_optimal_precision(\n            region_error,\n            target_accuracy,\n            current_precision\n        )\n        \n        assert new_precision in PrecisionLevel\n\n\nclass TestLocalErrorEstimator:\n    \"\"\"Test Local Error Estimator.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.estimator = LocalErrorEstimator(32, 32)\n    \n    def test_initialization(self):\n        \"\"\"Test error estimator initialization.\"\"\"\n        assert self.estimator.rows == 32\n        assert self.estimator.cols == 32\n        assert self.estimator.previous_solution is None\n    \n    def test_error_estimation(self):\n        \"\"\"Test local error estimation.\"\"\"\n        solution = np.random.random(32 * 32)\n        \n        errors = self.estimator.estimate_local_errors(solution)\n        \n        assert errors.shape == (32, 32)\n        assert np.isfinite(errors).all()\n        assert (errors >= 0).all()\n    \n    def test_adaptation_indicators(self):\n        \"\"\"Test adaptation indicators.\"\"\"\n        errors = np.random.random((32, 32))\n        \n        indicators = self.estimator.get_adaptation_indicators(errors)\n        \n        assert 'high_error_regions' in indicators\n        assert 'low_error_regions' in indicators\n        assert 'stable_regions' in indicators\n        assert 'error_magnitude' in indicators\n        \n        for key, indicator in indicators.items():\n            if key != 'error_magnitude':\n                assert indicator.dtype == bool\n            assert indicator.shape == errors.shape\n\n\nclass TestNeuromorphicAcceleration:\n    \"\"\"Test Neuromorphic PDE Acceleration.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.base_solver = AnalogPDESolver(crossbar_size=32)\n        \n        self.encoder = NeuromorphicSpikeEncoder(\n            encoding_scheme=SpikeEncoding.RATE,\n            time_window=1.0,\n            max_spike_rate=1000.0\n        )\n        \n        self.decoder = NeuromorphicSpikeDecoder(\n            decoding_scheme=SpikeEncoding.RATE,\n            time_window=1.0,\n            output_size=32\n        )\n        \n        self.npa_solver = NeuromorphicPDESolver(\n            self.base_solver,\n            spike_encoder=self.encoder,\n            spike_decoder=self.decoder,\n            sparsity_threshold=0.9\n        )\n    \n    def test_spike_encoder_initialization(self):\n        \"\"\"Test spike encoder initialization.\"\"\"\n        assert self.encoder.encoding_scheme == SpikeEncoding.RATE\n        assert self.encoder.time_window == 1.0\n        assert self.encoder.max_spike_rate == 1000.0\n    \n    def test_spike_encoding(self):\n        \"\"\"Test spike encoding.\"\"\"\n        data = np.random.random(32)\n        current_time = 0.0\n        \n        events = self.encoder.encode_data(data, current_time)\n        \n        assert isinstance(events, list)\n        \n        for event in events:\n            assert isinstance(event, SpikeEvent)\n            assert event.timestamp >= current_time\n            assert event.timestamp <= current_time + self.encoder.time_window\n            assert 0 <= event.neuron_id < len(data)\n    \n    def test_spike_decoding(self):\n        \"\"\"Test spike decoding.\"\"\"\n        # Create sample spike events\n        events = [\n            SpikeEvent(timestamp=0.1, neuron_id=0, spike_value=0.5),\n            SpikeEvent(timestamp=0.3, neuron_id=1, spike_value=0.7),\n            SpikeEvent(timestamp=0.8, neuron_id=0, spike_value=0.3)\n        ]\n        \n        current_time = 1.0\n        decoded_data = self.decoder.decode_events(events, current_time)\n        \n        assert decoded_data.shape == (32,)\n        assert np.isfinite(decoded_data).all()\n    \n    def test_sparse_event_buffer(self):\n        \"\"\"Test sparse event buffer.\"\"\"\n        buffer = SparseEventBuffer(capacity=100)\n        \n        # Add events\n        for i in range(10):\n            event = SpikeEvent(\n                timestamp=i * 0.1,\n                neuron_id=i % 5,\n                spike_value=0.5\n            )\n            buffer.add_event(event)\n        \n        assert len(buffer.events) == 10\n        assert len(buffer.active_neurons) == 5\n        \n        # Test windowed retrieval\n        events_in_window = buffer.get_events_in_window(0.0, 0.5, None)\n        assert len(events_in_window) == 6  # Events at 0.0, 0.1, 0.2, 0.3, 0.4, 0.5\n        \n        # Test sparsity statistics\n        stats = buffer.get_sparsity_statistics()\n        assert 'sparsity' in stats\n        assert 'active_fraction' in stats\n        assert 'event_rate' in stats\n    \n    def test_neuromorphic_solver_initialization(self):\n        \"\"\"Test neuromorphic solver initialization.\"\"\"\n        assert self.npa_solver.base_solver is self.base_solver\n        assert self.npa_solver.sparsity_threshold == 0.9\n        assert len(self.npa_solver.neuron_states) > 0\n    \n    def test_sparsity_computation(self):\n        \"\"\"Test sparsity level computation.\"\"\"\n        # Dense solution\n        dense_solution = np.ones(32)\n        sparsity = self.npa_solver._compute_sparsity(dense_solution)\n        assert sparsity < 0.5\n        \n        # Sparse solution\n        sparse_solution = np.zeros(32)\n        sparse_solution[0] = 1.0  # Only one non-zero element\n        sparsity = self.npa_solver._compute_sparsity(sparse_solution)\n        assert sparsity > 0.9\n\n\nclass TestMultiPhysicsCoupling:\n    \"\"\"Test Analog Multi-Physics Coupling.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.primary_crossbar = AnalogCrossbarArray(64, 64)\n        \n        # Create physics domains\n        self.thermal_domain = PhysicsDomainConfig(\n            domain_type=PhysicsDomain.THERMAL,\n            governing_equations=['heat_equation'],\n            crossbar_allocation=(0, 32, 0, 32),\n            boundary_conditions={'dirichlet': True},\n            material_properties={'conductivity': 1.0},\n            source_terms=None,\n            time_scale=1.0,\n            length_scale=1.0\n        )\n        \n        self.fluid_domain = PhysicsDomainConfig(\n            domain_type=PhysicsDomain.FLUID,\n            governing_equations=['navier_stokes'],\n            crossbar_allocation=(32, 64, 0, 32),\n            boundary_conditions={'dirichlet': True},\n            material_properties={'viscosity': 1e-3},\n            source_terms=None,\n            time_scale=0.1,\n            length_scale=1.0\n        )\n        \n        # Create coupling interface\n        self.coupling_interface = CouplingInterface(\n            source_domain=PhysicsDomain.THERMAL,\n            target_domain=PhysicsDomain.FLUID,\n            coupling_type='source_term',\n            coupling_strength=0.1,\n            coupling_function=lambda x: 0.1 * x,\n            interface_regions=[(16, 48, 16, 48)],\n            conservation_required=True,\n            bidirectional=False\n        )\n        \n        self.ampc = AnalogMultiPhysicsCoupler(\n            self.primary_crossbar,\n            [self.thermal_domain, self.fluid_domain],\n            [self.coupling_interface]\n        )\n    \n    def test_initialization(self):\n        \"\"\"Test AMPC initialization.\"\"\"\n        assert len(self.ampc.physics_domains) == 2\n        assert PhysicsDomain.THERMAL in self.ampc.physics_domains\n        assert PhysicsDomain.FLUID in self.ampc.physics_domains\n        assert len(self.ampc.coupling_interfaces) == 1\n    \n    def test_domain_crossbar_initialization(self):\n        \"\"\"Test domain crossbar initialization.\"\"\"\n        assert len(self.ampc.domain_crossbars) == 2\n        \n        thermal_info = self.ampc.domain_crossbars[PhysicsDomain.THERMAL]\n        assert thermal_info['allocation'] == (0, 32, 0, 32)\n        assert thermal_info['crossbar'].rows == 32\n        assert thermal_info['crossbar'].cols == 32\n    \n    def test_coupling_matrix_initialization(self):\n        \"\"\"Test coupling matrix initialization.\"\"\"\n        interface_key = \"thermal_to_fluid\"\n        assert interface_key in self.ampc.coupling_matrices\n        \n        coupling_matrix = self.ampc.coupling_matrices[interface_key]\n        assert coupling_matrix.shape[0] > 0\n        assert coupling_matrix.shape[1] > 0\n    \n    def test_solve_coupled_system(self):\n        \"\"\"Test coupled system solving.\"\"\"\n        initial_conditions = {\n            PhysicsDomain.THERMAL: np.random.random(32),\n            PhysicsDomain.FLUID: np.random.random(32)\n        }\n        \n        final_states, metrics = self.ampc.solve_coupled_system(\n            initial_conditions,\n            time_span=(0.0, 0.1),\n            num_time_steps=5,\n            coupling_iterations=3\n        )\n        \n        # Check results\n        assert len(final_states) == 2\n        assert PhysicsDomain.THERMAL in final_states\n        assert PhysicsDomain.FLUID in final_states\n        \n        # Check metrics\n        assert 'time_steps' in metrics\n        assert 'coupling_iterations_per_step' in metrics\n        assert 'conservation_errors' in metrics\n        assert 'coupling_residuals' in metrics\n    \n    def test_coupling_efficiency_analysis(self):\n        \"\"\"Test coupling efficiency analysis.\"\"\"\n        analysis = self.ampc.analyze_coupling_efficiency()\n        \n        assert 'interface_utilization' in analysis\n        assert 'domain_efficiency' in analysis\n        assert 'conservation_quality' in analysis\n        assert 'coupling_overhead_estimate' in analysis\n\n\nclass TestMLAcceleration:\n    \"\"\"Test Machine Learning acceleration.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.base_solver = AnalogPDESolver(crossbar_size=32)\n        \n        self.nn_surrogate = NeuralNetworkSurrogate(\n            input_dim=32,\n            hidden_layers=[16, 8],\n            activation='relu'\n        )\n        \n        self.ml_solver = MLAcceleratedPDESolver(\n            self.base_solver,\n            surrogate_type='neural_network',\n            training_threshold=5\n        )\n    \n    def test_neural_network_initialization(self):\n        \"\"\"Test neural network surrogate initialization.\"\"\"\n        assert self.nn_surrogate.input_dim == 32\n        assert self.nn_surrogate.hidden_layers == [16, 8]\n        assert len(self.nn_surrogate.layers) == 3  # Input->16->8->Output\n    \n    def test_neural_network_forward_pass(self):\n        \"\"\"Test neural network forward pass.\"\"\"\n        input_data = np.random.random(32)\n        \n        output = self.nn_surrogate.forward(input_data)\n        \n        assert output.shape == (32,)\n        assert np.isfinite(output).all()\n    \n    def test_neural_network_training(self):\n        \"\"\"Test neural network training.\"\"\"\n        # Create training data\n        inputs = np.random.random((10, 32))\n        outputs = np.random.random((10, 32))\n        \n        training_data = TrainingData(\n            inputs=inputs,\n            outputs=outputs,\n            metadata={'test': True}\n        )\n        \n        history = self.nn_surrogate.train(training_data, epochs=10)\n        \n        assert 'loss' in history\n        assert len(history['loss']) == 10\n    \n    def test_ml_accelerated_solver_initialization(self):\n        \"\"\"Test ML accelerated solver initialization.\"\"\"\n        assert self.ml_solver.base_solver is self.base_solver\n        assert self.ml_solver.training_threshold == 5\n        assert self.ml_solver.solve_count == 0\n\n\nclass TestIntegratedSolverFramework:\n    \"\"\"Test Integrated Solver Framework.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.framework = AdvancedSolverFramework(\n            base_crossbar_size=32,\n            performance_mode='balanced'\n        )\n        \n        self.algorithm_selector = AlgorithmSelector('balanced')\n        self.performance_tracker = PerformanceTracker()\n    \n    def test_framework_initialization(self):\n        \"\"\"Test framework initialization.\"\"\"\n        assert self.framework.base_crossbar_size == 32\n        assert self.framework.performance_mode == 'balanced'\n        assert len(self.framework.algorithms) > 0\n        assert AlgorithmType.BASE_ANALOG in self.framework.algorithms\n    \n    def test_problem_analysis(self):\n        \"\"\"Test problem characteristics analysis.\"\"\"\n        pde = PoissonEquation(\n            domain_size=(32, 32),\n            boundary_conditions='dirichlet'\n        )\n        \n        characteristics = self.framework._analyze_problem(\n            pde,\n            convergence_threshold=1e-6,\n            time_dependent=False\n        )\n        \n        assert isinstance(characteristics, ProblemCharacteristics)\n        assert characteristics.problem_size == (32, 32)\n        assert characteristics.accuracy_requirement == 1e-6\n        assert not characteristics.time_dependent\n    \n    def test_algorithm_selector(self):\n        \"\"\"Test algorithm selector.\"\"\"\n        characteristics = ProblemCharacteristics(\n            problem_size=(64, 64),\n            sparsity_level=0.95,  # High sparsity\n            time_dependent=False,\n            multi_physics=False,\n            conservation_required=False,\n            accuracy_requirement=1e-6,\n            energy_budget=0.01,\n            real_time_requirement=False,\n            physics_constraints=[],\n            boundary_complexity='simple'\n        )\n        \n        available_algorithms = {\n            AlgorithmType.BASE_ANALOG: Mock(),\n            AlgorithmType.NEUROMORPHIC: Mock()\n        }\n        \n        recommendation = self.algorithm_selector.recommend_algorithm(\n            characteristics,\n            available_algorithms,\n            {}\n        )\n        \n        assert recommendation.algorithm_type in available_algorithms\n        assert 0 <= recommendation.confidence <= 1\n    \n    def test_performance_tracker(self):\n        \"\"\"Test performance tracker.\"\"\"\n        # Start tracking\n        self.performance_tracker.start_tracking('test_operation')\n        \n        # Simulate some work\n        import time\n        time.sleep(0.01)\n        \n        # End tracking\n        duration = self.performance_tracker.end_tracking(\n            'test_operation',\n            additional_metrics={'operations': 100}\n        )\n        \n        assert duration > 0\n        \n        # Get summary\n        summary = self.performance_tracker.get_performance_summary()\n        assert 'test_operation' in summary\n        assert summary['test_operation']['count'] == 1\n\n\nclass TestValidationBenchmarkSuite:\n    \"\"\"Test Validation and Benchmark Suite.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup test fixtures.\"\"\"\n        self.framework = AdvancedSolverFramework(base_crossbar_size=32)\n        self.benchmark_suite = ValidationBenchmarkSuite(\n            self.framework,\n            output_directory=\"test_benchmark_results\",\n            num_statistical_runs=2  # Reduced for testing\n        )\n    \n    def test_benchmark_suite_initialization(self):\n        \"\"\"Test benchmark suite initialization.\"\"\"\n        assert self.benchmark_suite.framework is self.framework\n        assert len(self.benchmark_suite.benchmark_problems) > 0\n        assert self.benchmark_suite.num_statistical_runs == 2\n    \n    def test_benchmark_problem_creation(self):\n        \"\"\"Test benchmark problem creation.\"\"\"\n        problems = self.benchmark_suite._create_benchmark_problems()\n        \n        assert len(problems) > 0\n        \n        for problem in problems:\n            assert isinstance(problem, BenchmarkProblem)\n            assert problem.name\n            assert problem.pde_constructor\n            assert problem.difficulty_level in ['easy', 'medium', 'hard', 'extreme']\n    \n    def test_performance_benchmark(self):\n        \"\"\"Test performance benchmark.\"\"\"\n        algorithms_to_test = [AlgorithmType.BASE_ANALOG]\n        \n        results = self.benchmark_suite.run_performance_benchmark(algorithms_to_test)\n        \n        assert 'solve_times' in results\n        assert 'throughput' in results\n        assert 'memory_usage' in results\n    \n    def test_statistical_analysis(self):\n        \"\"\"Test statistical analysis methods.\"\"\"\n        group1 = [1.0, 2.0, 3.0, 4.0, 5.0]\n        group2 = [2.0, 3.0, 4.0, 5.0, 6.0]\n        \n        effect_size = self.benchmark_suite._compute_effect_size(group1, group2)\n        \n        assert isinstance(effect_size, float)\n        assert effect_size >= 0\n\n\nclass TestIntegration:\n    \"\"\"Integration tests for the complete system.\"\"\"\n    \n    def setup_method(self):\n        \"\"\"Setup integration test fixtures.\"\"\"\n        self.framework = AdvancedSolverFramework(\n            base_crossbar_size=32,\n            performance_mode='balanced'\n        )\n    \n    def test_end_to_end_solve(self):\n        \"\"\"Test complete end-to-end solving pipeline.\"\"\"\n        # Create a simple PDE\n        pde = PoissonEquation(\n            domain_size=(32, 32),\n            boundary_conditions='dirichlet',\n            source_function=lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n        )\n        \n        # Define problem characteristics\n        characteristics = ProblemCharacteristics(\n            problem_size=(32, 32),\n            sparsity_level=0.2,\n            time_dependent=False,\n            multi_physics=False,\n            conservation_required=False,\n            accuracy_requirement=1e-6,\n            energy_budget=None,\n            real_time_requirement=False,\n            physics_constraints=[],\n            boundary_complexity='simple'\n        )\n        \n        # Solve\n        solution, solve_info = self.framework.solve_pde(\n            pde,\n            problem_characteristics=characteristics\n        )\n        \n        # Verify results\n        assert solution.shape == (32*32,) or solution.shape == (32, 32)\n        assert np.isfinite(solution).all()\n        assert 'selected_algorithm' in solve_info\n        assert 'total_framework_time' in solve_info\n    \n    def test_algorithm_comparison(self):\n        \"\"\"Test comparison between different algorithms.\"\"\"\n        pde = PoissonEquation(\n            domain_size=(16, 16),\n            boundary_conditions='dirichlet'\n        )\n        \n        characteristics = ProblemCharacteristics(\n            problem_size=(16, 16),\n            sparsity_level=0.5,\n            time_dependent=False,\n            multi_physics=False,\n            conservation_required=False,\n            accuracy_requirement=1e-6,\n            energy_budget=None,\n            real_time_requirement=False,\n            physics_constraints=[],\n            boundary_complexity='simple'\n        )\n        \n        # Test multiple algorithms\n        algorithms_to_test = [AlgorithmType.BASE_ANALOG]\n        \n        results = {}\n        for algorithm in algorithms_to_test:\n            try:\n                solution, solve_info = self.framework.solve_pde(\n                    pde,\n                    problem_characteristics=characteristics,\n                    algorithm_preference=algorithm\n                )\n                \n                results[algorithm] = {\n                    'solution': solution,\n                    'solve_info': solve_info,\n                    'success': True\n                }\n                \n            except Exception as e:\n                results[algorithm] = {\n                    'success': False,\n                    'error': str(e)\n                }\n        \n        # Verify at least one algorithm worked\n        successful_algorithms = [alg for alg, result in results.items() if result['success']]\n        assert len(successful_algorithms) > 0\n\n\n# Pytest fixtures and configuration\n@pytest.fixture\ndef sample_crossbar():\n    \"\"\"Sample crossbar for testing.\"\"\"\n    return AnalogCrossbarArray(16, 16)\n\n\n@pytest.fixture  \ndef sample_pde():\n    \"\"\"Sample PDE for testing.\"\"\"\n    return PoissonEquation(\n        domain_size=(16, 16),\n        boundary_conditions='dirichlet',\n        source_function=lambda x, y: np.ones_like(x)\n    )\n\n\n@pytest.fixture\ndef sample_solver():\n    \"\"\"Sample solver for testing.\"\"\"\n    return AnalogPDESolver(crossbar_size=16)\n\n\n# Test configuration\ndef pytest_configure(config):\n    \"\"\"Configure pytest.\"\"\"\n    import warnings\n    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n    warnings.filterwarnings(\"ignore\", category=PendingDeprecationWarning)\n\n\nif __name__ == \"__main__\":\n    # Run tests if executed directly\n    pytest.main([__file__, \"-v\", \"--tb=short\"])"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/test_working_core.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python3\n\"\"\"Unit tests for analog PDE solver core functionality.\"\"\"\n\nimport sys\nimport os\nimport unittest\nimport numpy as np\n\n# Add the root directory to the Python path\nsys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))\n\nfrom analog_pde_solver import (\n    AnalogPDESolver, \n    PoissonEquation, \n    HeatEquation, \n    WaveEquation,\n    VerilogGenerator,\n    RTLConfig\n)\nfrom analog_pde_solver.core.crossbar import AnalogCrossbarArray\n\n\nclass TestAnalogCrossbarArray(unittest.TestCase):\n    \"\"\"Test analog crossbar array functionality.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.crossbar = AnalogCrossbarArray(32, 32)\n        \n    def test_initialization(self):\n        \"\"\"Test crossbar initialization.\"\"\"\n        self.assertEqual(self.crossbar.rows, 32)\n        self.assertEqual(self.crossbar.cols, 32)\n        self.assertEqual(self.crossbar.cell_type, \"1T1R\")\n        \n    def test_conductance_programming(self):\n        \"\"\"Test conductance programming.\"\"\"\n        test_matrix = np.random.random((32, 32)) - 0.5  # Mixed positive/negative\n        self.crossbar.program_conductances(test_matrix)\n        \n        # Check that conductances are in valid range\n        self.assertTrue(np.all(self.crossbar.g_positive >= 0))\n        self.assertTrue(np.all(self.crossbar.g_negative >= 0))\n        self.assertTrue(np.all(self.crossbar.g_positive <= 1e-6))\n        self.assertTrue(np.all(self.crossbar.g_negative <= 1e-6))\n        \n    def test_vector_matrix_multiplication(self):\n        \"\"\"Test analog vector-matrix multiplication.\"\"\"\n        # Simple identity matrix test\n        identity = np.eye(32)\n        self.crossbar.program_conductances(identity)\n        \n        input_vector = np.ones(32)\n        output = self.crossbar.compute_vmm(input_vector)\n        \n        # Should be approximately the input (with noise)\n        self.assertEqual(len(output), 32)\n        self.assertTrue(np.all(np.abs(output) < 1.0))  # Reasonable bounds\n        \n\nclass TestPoissonEquation(unittest.TestCase):\n    \"\"\"Test Poisson equation implementation.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.pde = PoissonEquation(\n            domain_size=(32,),\n            boundary_conditions=\"dirichlet\"\n        )\n        \n    def test_initialization(self):\n        \"\"\"Test Poisson equation initialization.\"\"\"\n        self.assertEqual(self.pde.domain_size, (32,))\n        self.assertEqual(self.pde.boundary_conditions, \"dirichlet\")\n        \n    def test_digital_solver(self):\n        \"\"\"Test digital reference solver.\"\"\"\n        solution = self.pde.solve_digital()\n        self.assertEqual(len(solution), 32)\n        \n        # Boundary conditions should be satisfied\n        self.assertAlmostEqual(solution[0], 0.0, places=6)\n        self.assertAlmostEqual(solution[-1], 0.0, places=6)\n        \n\nclass TestHeatEquation(unittest.TestCase):\n    \"\"\"Test heat equation implementation.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.heat_eq = HeatEquation(\n            domain_size=(32,),\n            thermal_diffusivity=0.1,\n            time_step=0.001\n        )\n        \n    def test_initialization(self):\n        \"\"\"Test heat equation initialization.\"\"\"\n        self.assertEqual(self.heat_eq.domain_size, (32,))\n        self.assertEqual(self.heat_eq.thermal_diffusivity, 0.1)\n        self.assertEqual(self.heat_eq.time_step, 0.001)\n        \n    def test_field_initialization(self):\n        \"\"\"Test temperature field initialization.\"\"\"\n        def initial_temp(x):\n            return np.sin(np.pi * x)\n            \n        field = self.heat_eq.initialize_field(initial_condition=initial_temp)\n        self.assertEqual(len(field), 32)\n        \n        # Check boundary conditions\n        self.assertAlmostEqual(field[0], 0.0, places=6)\n        self.assertAlmostEqual(field[-1], 0.0, places=6)\n        \n    def test_time_stepping(self):\n        \"\"\"Test time stepping functionality.\"\"\"\n        self.heat_eq.initialize_field()\n        initial_field = self.heat_eq.temperature_field.copy()\n        \n        # Take a time step\n        new_field = self.heat_eq.step()\n        \n        # Field should evolve (unless it's zero everywhere)\n        self.assertEqual(len(new_field), 32)\n        \n\nclass TestWaveEquation(unittest.TestCase):\n    \"\"\"Test wave equation implementation.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.wave_eq = WaveEquation(\n            domain_size=(32,),\n            wave_speed=1.0,\n            time_step=0.01\n        )\n        \n    def test_initialization(self):\n        \"\"\"Test wave equation initialization.\"\"\"\n        self.assertEqual(self.wave_eq.domain_size, (32,))\n        self.assertEqual(self.wave_eq.wave_speed, 1.0)\n        self.assertEqual(self.wave_eq.time_step, 0.01)\n        \n    def test_field_initialization(self):\n        \"\"\"Test wave field initialization.\"\"\"\n        def initial_displacement(x):\n            return np.exp(-((x - 0.5) / 0.1)**2)\n            \n        u_current, u_previous = self.wave_eq.initialize_field(\n            initial_displacement=initial_displacement\n        )\n        \n        self.assertEqual(len(u_current), 32)\n        self.assertEqual(len(u_previous), 32)\n        \n        # Check that initial displacement is set\n        self.assertTrue(np.max(u_current) > 0.1)\n\n\nclass TestAnalogPDESolver(unittest.TestCase):\n    \"\"\"Test main analog PDE solver.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.solver = AnalogPDESolver(\n            crossbar_size=32,\n            conductance_range=(1e-9, 1e-6),\n            noise_model=\"realistic\"\n        )\n        \n    def test_initialization(self):\n        \"\"\"Test solver initialization.\"\"\"\n        self.assertEqual(self.solver.crossbar_size, 32)\n        self.assertEqual(self.solver.conductance_range, (1e-9, 1e-6))\n        self.assertEqual(self.solver.noise_model, \"realistic\")\n        \n    def test_laplacian_matrix_generation(self):\n        \"\"\"Test Laplacian matrix generation.\"\"\"\n        laplacian = self.solver._create_laplacian_matrix(32)\n        \n        self.assertEqual(laplacian.shape, (32, 32))\n        \n        # Check diagonal elements are -2\n        self.assertTrue(np.all(np.diag(laplacian) == -2.0))\n        \n        # Check off-diagonals are 1\n        for i in range(31):\n            self.assertEqual(laplacian[i, i+1], 1.0)\n            self.assertEqual(laplacian[i+1, i], 1.0)\n            \n    def test_pde_mapping(self):\n        \"\"\"Test PDE to crossbar mapping.\"\"\"\n        pde = PoissonEquation((32,))\n        config = self.solver.map_pde_to_crossbar(pde)\n        \n        self.assertEqual(config[\"matrix_size\"], 32)\n        self.assertEqual(config[\"conductance_range\"], (1e-9, 1e-6))\n        self.assertTrue(config[\"programming_success\"])\n        \n    def test_solving(self):\n        \"\"\"Test PDE solving functionality.\"\"\"\n        pde = PoissonEquation((32,))\n        solution = self.solver.solve(pde, iterations=10)\n        \n        self.assertEqual(len(solution), 32)\n        self.assertTrue(np.isfinite(solution).all())\n\n\nclass TestVerilogGenerator(unittest.TestCase):\n    \"\"\"Test Verilog RTL generation.\"\"\"\n    \n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.config = RTLConfig(\n            dac_bits=8,\n            adc_bits=10,\n            clock_frequency_mhz=100.0\n        )\n        self.generator = VerilogGenerator(self.config)\n        \n    def test_initialization(self):\n        \"\"\"Test generator initialization.\"\"\"\n        self.assertEqual(self.generator.config.dac_bits, 8)\n        self.assertEqual(self.generator.config.adc_bits, 10)\n        self.assertEqual(self.generator.config.clock_frequency_mhz, 100.0)\n        \n    def test_top_module_generation(self):\n        \"\"\"Test top-level module generation.\"\"\"\n        verilog_code = self.generator.generate_top_module(\n            crossbar_size=16,\n            num_crossbars=2,\n            pde_type=\"poisson\"\n        )\n        \n        self.assertIsInstance(verilog_code, str)\n        self.assertTrue(len(verilog_code) > 1000)\n        self.assertIn(\"module analog_pde_solver_poisson\", verilog_code)\n        self.assertIn(\"parameter GRID_SIZE = 16\", verilog_code)\n        self.assertIn(\"NUM_CROSSBARS = 2\", verilog_code)\n\n\nif __name__ == '__main__':\n    # Run the tests\n    unittest.main(verbosity=2)"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/acceleration/gpu_solver.py",
      "line_number": 1,
      "details": "\"\"\"GPU-accelerated analog PDE solver implementation.\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, Optional, Tuple, List\nfrom dataclasses import dataclass\nfrom ..core.solver import AnalogPDESolver\nfrom ..utils.logger import get_logger, PerformanceLogger\n\n# Attempt to import GPU libraries with fallback\nHAS_CUPY = False\nHAS_NUMBA_CUDA = False\n\ntry:\n    import cupy as cp\n    HAS_CUPY = True\n    CupyArray = cp.ndarray\nexcept ImportError:\n    cp = None\n    HAS_CUPY = False\n    # Create dummy type for annotations\n    CupyArray = type(None)\n\ntry:\n    from numba import cuda\n    import numba\n    HAS_NUMBA_CUDA = True\nexcept ImportError:\n    cuda = None\n    numba = None\n    HAS_NUMBA_CUDA = False\n\n\n@dataclass\nclass GPUConfig:\n    \"\"\"Configuration for GPU acceleration.\"\"\"\n    device_id: int = 0\n    memory_pool_size_gb: float = 4.0\n    use_streams: bool = True\n    num_streams: int = 4\n    block_size: int = 256\n    preferred_backend: str = 'cupy'  # 'cupy' or 'numba'\n\n\nclass GPUAcceleratedSolver:\n    \"\"\"GPU-accelerated analog PDE solver.\"\"\"\n    \n    def __init__(\n        self,\n        base_solver: AnalogPDESolver,\n        gpu_config: Optional[GPUConfig] = None,\n        fallback_to_cpu: bool = True\n    ):\n        \"\"\"Initialize GPU-accelerated solver.\n        \n        Args:\n            base_solver: Base analog PDE solver\n            gpu_config: GPU configuration\n            fallback_to_cpu: Whether to fallback to CPU if GPU unavailable\n        \"\"\"\n        self.logger = get_logger('gpu_accelerated_solver')\n        self.perf_logger = PerformanceLogger(self.logger)\n        \n        self.base_solver = base_solver\n        self.config = gpu_config or GPUConfig()\n        self.fallback_to_cpu = fallback_to_cpu\n        \n        # GPU availability check\n        self.gpu_available = self._check_gpu_availability()\n        self.backend = self._select_backend()\n        \n        if self.gpu_available:\n            self._initialize_gpu_resources()\n        else:\n            self.logger.warning(\"GPU acceleration not available, using CPU fallback\")\n    \n    def _check_gpu_availability(self) -> bool:\n        \"\"\"Check if GPU acceleration is available.\"\"\"\n        if not (HAS_CUPY or HAS_NUMBA_CUDA):\n            return False\n        \n        try:\n            if self.config.preferred_backend == 'cupy' and HAS_CUPY:\n                # Test CuPy availability\n                cp.cuda.Device(self.config.device_id).use()\n                test_array = cp.array([1, 2, 3])\n                _ = cp.sum(test_array)\n                return True\n            elif self.config.preferred_backend == 'numba' and HAS_NUMBA_CUDA:\n                # Test Numba CUDA availability\n                cuda.select_device(self.config.device_id)\n                return True\n            else:\n                # Try any available backend\n                if HAS_CUPY:\n                    cp.cuda.Device(self.config.device_id).use()\n                    return True\n                elif HAS_NUMBA_CUDA:\n                    cuda.select_device(self.config.device_id)\n                    return True\n        except Exception as e:\n            self.logger.debug(f\"GPU availability check failed: {e}\")\n        \n        return False\n    \n    def _select_backend(self) -> str:\n        \"\"\"Select the best available GPU backend.\"\"\"\n        if not self.gpu_available:\n            return 'cpu'\n        \n        if self.config.preferred_backend == 'cupy' and HAS_CUPY:\n            return 'cupy'\n        elif self.config.preferred_backend == 'numba' and HAS_NUMBA_CUDA:\n            return 'numba'\n        elif HAS_CUPY:\n            return 'cupy'\n        elif HAS_NUMBA_CUDA:\n            return 'numba'\n        else:\n            return 'cpu'\n    \n    def _initialize_gpu_resources(self) -> None:\n        \"\"\"Initialize GPU resources.\"\"\"\n        if self.backend == 'cupy':\n            self._initialize_cupy_resources()\n        elif self.backend == 'numba':\n            self._initialize_numba_resources()\n        \n        self.logger.info(f\"GPU acceleration initialized with {self.backend} backend\")\n    \n    def _initialize_cupy_resources(self) -> None:\n        \"\"\"Initialize CuPy resources.\"\"\"\n        cp.cuda.Device(self.config.device_id).use()\n        \n        # Initialize memory pool\n        memory_pool_bytes = int(self.config.memory_pool_size_gb * 1024**3)\n        mempool = cp.get_default_memory_pool()\n        mempool.set_limit(size=memory_pool_bytes)\n        \n        # Initialize streams if requested\n        if self.config.use_streams:\n            self.streams = [cp.cuda.Stream() for _ in range(self.config.num_streams)]\n        else:\n            self.streams = None\n    \n    def _initialize_numba_resources(self) -> None:\n        \"\"\"Initialize Numba CUDA resources.\"\"\"\n        cuda.select_device(self.config.device_id)\n        \n        # Initialize streams if requested\n        if self.config.use_streams:\n            self.streams = [cuda.stream() for _ in range(self.config.num_streams)]\n        else:\n            self.streams = None\n    \n    def solve_gpu(\n        self,\n        pde,\n        iterations: int = 100,\n        convergence_threshold: float = 1e-6\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Solve PDE using GPU acceleration.\n        \n        Args:\n            pde: PDE object to solve\n            iterations: Maximum iterations\n            convergence_threshold: Convergence threshold\n            \n        Returns:\n            Tuple of (solution, solve_info)\n        \"\"\"\n        if not self.gpu_available and not self.fallback_to_cpu:\n            raise RuntimeError(\"GPU not available and CPU fallback disabled\")\n        \n        if not self.gpu_available:\n            self.logger.info(\"Using CPU fallback\")\n            return self.base_solver.solve(pde, iterations, convergence_threshold), {\n                'method': 'cpu_fallback',\n                'gpu_available': False\n            }\n        \n        solve_info = {\n            'method': f'gpu_{self.backend}',\n            'gpu_available': True,\n            'device_id': self.config.device_id\n        }\n        \n        self.perf_logger.start_timer('gpu_solve_total')\n        \n        try:\n            if self.backend == 'cupy':\n                solution = self._solve_cupy(pde, iterations, convergence_threshold)\n            elif self.backend == 'numba':\n                solution = self._solve_numba(pde, iterations, convergence_threshold)\n            else:\n                raise RuntimeError(f\"Unknown GPU backend: {self.backend}\")\n            \n            solve_time = self.perf_logger.end_timer('gpu_solve_total')\n            solve_info['solve_time'] = solve_time\n            \n            return solution, solve_info\n            \n        except Exception as e:\n            self.logger.error(f\"GPU solving failed: {e}\")\n            if self.fallback_to_cpu:\n                self.logger.info(\"Falling back to CPU solver\")\n                solution = self.base_solver.solve(pde, iterations, convergence_threshold)\n                solve_info.update({\n                    'method': 'cpu_fallback_after_gpu_error',\n                    'gpu_error': str(e)\n                })\n                return solution, solve_info\n            else:\n                raise\n    \n    def _solve_cupy(\n        self,\n        pde,\n        iterations: int,\n        convergence_threshold: float\n    ) -> np.ndarray:\n        \"\"\"Solve using CuPy backend.\"\"\"\n        # Transfer data to GPU\n        size = self.base_solver.crossbar_size\n        \n        # Create Laplacian matrix on GPU\n        laplacian_gpu = self._create_laplacian_cupy(size)\n        \n        # Initialize solution on GPU\n        phi_gpu = cp.random.random(size).astype(cp.float32) * 0.1\n        \n        # Create source term on GPU\n        if hasattr(pde, 'source_function') and pde.source_function:\n            x = cp.linspace(0, 1, size)\n            source_gpu = cp.array([pde.source_function(float(xi), 0) for xi in x])\n        else:\n            source_gpu = cp.ones(size, dtype=cp.float32) * 0.1\n        \n        # Iterative solver on GPU\n        for i in range(iterations):\n            # Matrix-vector multiplication\n            residual = cp.dot(laplacian_gpu, phi_gpu) + source_gpu\n            \n            # Jacobi update\n            phi_new = phi_gpu - 0.1 * residual\n            \n            # Apply boundary conditions\n            phi_new[0] = 0.0\n            phi_new[-1] = 0.0\n            \n            # Check convergence\n            error = cp.linalg.norm(phi_new - phi_gpu)\n            phi_gpu = phi_new\n            \n            if error < convergence_threshold:\n                self.logger.debug(f\"GPU solver converged after {i+1} iterations\")\n                break\n        \n        # Transfer result back to CPU\n        return cp.asnumpy(phi_gpu)\n    \n    def _solve_numba(\n        self,\n        pde,\n        iterations: int,\n        convergence_threshold: float\n    ) -> np.ndarray:\n        \"\"\"Solve using Numba CUDA backend.\"\"\"\n        size = self.base_solver.crossbar_size\n        \n        # Create data on CPU first\n        phi = np.random.random(size).astype(np.float32) * 0.1\n        laplacian = self._create_laplacian_matrix_numpy(size).astype(np.float32)\n        \n        if hasattr(pde, 'source_function') and pde.source_function:\n            x = np.linspace(0, 1, size)\n            source = np.array([pde.source_function(xi, 0) for xi in x], dtype=np.float32)\n        else:\n            source = np.ones(size, dtype=np.float32) * 0.1\n        \n        # Transfer to GPU\n        phi_gpu = cuda.to_device(phi)\n        laplacian_gpu = cuda.to_device(laplacian)\n        source_gpu = cuda.to_device(source)\n        \n        # Create work arrays on GPU\n        residual_gpu = cuda.device_array(size, dtype=np.float32)\n        phi_new_gpu = cuda.device_array(size, dtype=np.float32)\n        \n        # Configure CUDA kernel\n        threads_per_block = min(self.config.block_size, size)\n        blocks_per_grid = (size + threads_per_block - 1) // threads_per_block\n        \n        # Iterative solver\n        for i in range(iterations):\n            # Matrix-vector multiplication kernel\n            self._matvec_kernel[blocks_per_grid, threads_per_block](\n                laplacian_gpu, phi_gpu, residual_gpu, size\n            )\n            \n            # Update kernel\n            self._jacobi_update_kernel[blocks_per_grid, threads_per_block](\n                phi_gpu, residual_gpu, source_gpu, phi_new_gpu, size\n            )\n            \n            # Apply boundary conditions\n            self._apply_bc_kernel[1, 1](phi_new_gpu, size)\n            \n            # Check convergence (simplified)\n            if i % 10 == 0:\n                phi_cpu = phi_new_gpu.copy_to_host()\n                phi_old_cpu = phi_gpu.copy_to_host()\n                error = np.linalg.norm(phi_cpu - phi_old_cpu)\n                \n                if error < convergence_threshold:\n                    self.logger.debug(f\"GPU solver converged after {i+1} iterations\")\n                    break\n            \n            # Swap arrays\n            phi_gpu, phi_new_gpu = phi_new_gpu, phi_gpu\n        \n        # Transfer result back to CPU\n        return phi_gpu.copy_to_host()\n    \n    def _create_laplacian_cupy(self, size: int) -> CupyArray:\n        \"\"\"Create Laplacian matrix using CuPy.\"\"\"\n        laplacian = cp.zeros((size, size), dtype=cp.float32)\n        \n        # Main diagonal\n        cp.fill_diagonal(laplacian, -2.0)\n        \n        # Off-diagonals\n        for i in range(size - 1):\n            laplacian[i, i + 1] = 1.0\n            laplacian[i + 1, i] = 1.0\n        \n        return laplacian\n    \n    def _create_laplacian_matrix_numpy(self, size: int) -> np.ndarray:\n        \"\"\"Create Laplacian matrix using NumPy.\"\"\"\n        laplacian = np.zeros((size, size), dtype=np.float32)\n        \n        # Main diagonal\n        np.fill_diagonal(laplacian, -2.0)\n        \n        # Off-diagonals\n        for i in range(size - 1):\n            laplacian[i, i + 1] = 1.0\n            laplacian[i + 1, i] = 1.0\n        \n        return laplacian\n    \n    @property\n    def _matvec_kernel(self):\n        \"\"\"Matrix-vector multiplication kernel.\"\"\"\n        if not hasattr(self, '_matvec_kernel_cached'):\n            @cuda.jit\n            def matvec_kernel(A, x, y, n):\n                i = cuda.grid(1)\n                if i < n:\n                    result = 0.0\n                    for j in range(n):\n                        result += A[i, j] * x[j]\n                    y[i] = result\n            \n            self._matvec_kernel_cached = matvec_kernel\n        \n        return self._matvec_kernel_cached\n    \n    @property\n    def _jacobi_update_kernel(self):\n        \"\"\"Jacobi update kernel.\"\"\"\n        if not hasattr(self, '_jacobi_update_kernel_cached'):\n            @cuda.jit\n            def jacobi_update_kernel(phi, residual, source, phi_new, n):\n                i = cuda.grid(1)\n                if i < n:\n                    phi_new[i] = phi[i] - 0.1 * (residual[i] + source[i])\n            \n            self._jacobi_update_kernel_cached = jacobi_update_kernel\n        \n        return self._jacobi_update_kernel_cached\n    \n    @property\n    def _apply_bc_kernel(self):\n        \"\"\"Apply boundary conditions kernel.\"\"\"\n        if not hasattr(self, '_apply_bc_kernel_cached'):\n            @cuda.jit\n            def apply_bc_kernel(phi, n):\n                phi[0] = 0.0\n                phi[n-1] = 0.0\n            \n            self._apply_bc_kernel_cached = apply_bc_kernel\n        \n        return self._apply_bc_kernel_cached\n    \n    def benchmark_gpu_vs_cpu(\n        self,\n        pde,\n        iterations: int = 100,\n        num_runs: int = 5\n    ) -> Dict[str, Any]:\n        \"\"\"Benchmark GPU vs CPU performance.\n        \n        Args:\n            pde: PDE to solve\n            iterations: Number of iterations per solve\n            num_runs: Number of benchmark runs\n            \n        Returns:\n            Benchmark results\n        \"\"\"\n        self.logger.info(f\"Starting GPU vs CPU benchmark with {num_runs} runs\")\n        \n        results = {\n            'num_runs': num_runs,\n            'iterations': iterations,\n            'gpu_available': self.gpu_available,\n            'backend': self.backend,\n            'problem_size': self.base_solver.crossbar_size\n        }\n        \n        # CPU benchmark\n        cpu_times = []\n        for i in range(num_runs):\n            self.perf_logger.start_timer(f'cpu_run_{i}')\n            _ = self.base_solver.solve(pde, iterations=iterations)\n            cpu_time = self.perf_logger.end_timer(f'cpu_run_{i}')\n            cpu_times.append(cpu_time)\n        \n        results['cpu_times'] = cpu_times\n        results['avg_cpu_time'] = np.mean(cpu_times)\n        results['std_cpu_time'] = np.std(cpu_times)\n        \n        # GPU benchmark\n        if self.gpu_available:\n            gpu_times = []\n            for i in range(num_runs):\n                self.perf_logger.start_timer(f'gpu_run_{i}')\n                _ = self.solve_gpu(pde, iterations=iterations)\n                gpu_time = self.perf_logger.end_timer(f'gpu_run_{i}')\n                gpu_times.append(gpu_time)\n            \n            results['gpu_times'] = gpu_times\n            results['avg_gpu_time'] = np.mean(gpu_times)\n            results['std_gpu_time'] = np.std(gpu_times)\n            results['speedup'] = results['avg_cpu_time'] / results['avg_gpu_time']\n        else:\n            results['gpu_times'] = None\n            results['speedup'] = None\n        \n        self.logger.info(f\"Benchmark completed. Speedup: {results.get('speedup', 'N/A'):.2f}x\")\n        return results\n    \n    def get_gpu_memory_info(self) -> Dict[str, Any]:\n        \"\"\"Get GPU memory information.\"\"\"\n        if not self.gpu_available:\n            return {'gpu_available': False}\n        \n        info = {'gpu_available': True, 'backend': self.backend}\n        \n        try:\n            if self.backend == 'cupy':\n                mempool = cp.get_default_memory_pool()\n                info.update({\n                    'used_bytes': mempool.used_bytes(),\n                    'total_bytes': mempool.total_bytes(),\n                    'limit_bytes': mempool.get_limit(),\n                    'n_free_blocks': mempool.n_free_blocks()\n                })\n            elif self.backend == 'numba':\n                # Numba doesn't provide detailed memory info\n                info['memory_details'] = 'Not available with Numba backend'\n        except Exception as e:\n            info['error'] = str(e)\n        \n        return info\n\n\nclass GPUMemoryManager:\n    \"\"\"GPU memory management utilities.\"\"\"\n    \n    def __init__(self, backend: str = 'cupy'):\n        \"\"\"Initialize GPU memory manager.\n        \n        Args:\n            backend: GPU backend ('cupy' or 'numba')\n        \"\"\"\n        self.backend = backend\n        self.logger = get_logger('gpu_memory_manager')\n    \n    def clear_cache(self) -> None:\n        \"\"\"Clear GPU memory cache.\"\"\"\n        if self.backend == 'cupy' and HAS_CUPY:\n            mempool = cp.get_default_memory_pool()\n            pinned_mempool = cp.get_default_pinned_memory_pool()\n            \n            mempool.free_all_blocks()\n            pinned_mempool.free_all_blocks()\n            \n            self.logger.info(\"CuPy memory cache cleared\")\n        elif self.backend == 'numba' and HAS_NUMBA_CUDA:\n            # Numba doesn't have explicit cache clearing\n            self.logger.info(\"Numba CUDA cache management not available\")\n    \n    def get_memory_stats(self) -> Dict[str, Any]:\n        \"\"\"Get detailed memory statistics.\"\"\"\n        stats = {'backend': self.backend}\n        \n        if self.backend == 'cupy' and HAS_CUPY:\n            try:\n                mempool = cp.get_default_memory_pool()\n                stats.update({\n                    'used_bytes': mempool.used_bytes(),\n                    'total_bytes': mempool.total_bytes(),\n                    'n_free_blocks': mempool.n_free_blocks(),\n                    'limit_bytes': mempool.get_limit()\n                })\n            except Exception as e:\n                stats['error'] = str(e)\n        \n        return stats"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/core/crossbar_robust.py",
      "line_number": 1,
      "details": "\"\"\"Enhanced analog crossbar array with comprehensive device modeling.\"\"\"\n\nimport numpy as np\nfrom typing import Tuple, Optional\nimport logging\nfrom ..utils.logging_config import get_logger\n\n\nclass RobustAnalogCrossbarArray:\n    \"\"\"Enhanced analog crossbar array with realistic device modeling and error handling.\"\"\"\n    \n    def __init__(\n        self, \n        rows: int, \n        cols: int, \n        cell_type: str = \"1T1R\",\n        noise_model: str = \"realistic\"\n    ):\n        \"\"\"Initialize enhanced crossbar array.\n        \n        Args:\n            rows: Number of rows (input size)\n            cols: Number of columns (output size)  \n            cell_type: Crossbar cell type ('1T1R', 'ReRAM', 'PCM')\n            noise_model: Noise modeling approach\n        \"\"\"\n        self.rows = rows\n        self.cols = cols\n        self.cell_type = cell_type\n        self.noise_model = noise_model\n        \n        # Initialize conductance matrices\n        self.g_positive = np.zeros((rows, cols), dtype=np.float64)\n        self.g_negative = np.zeros((rows, cols), dtype=np.float64)\n        \n        # Device parameters\n        self.device_params = self._get_device_parameters()\n        \n        # Initialize logger\n        self.logger = get_logger('crossbar')\n        \n        # State tracking\n        self.is_programmed = False\n        self.programming_errors = 0\n        self.operation_count = 0\n        \n        self.logger.info(\n            f\"Initialized {rows}x{cols} crossbar array \"\n            f\"(cell_type={cell_type}, noise_model={noise_model})\"\n        )\n    \n    def program_conductances(self, target_matrix: np.ndarray) -> None:\n        \"\"\"Map target matrix to positive/negative conductance pairs with validation.\"\"\"\n        try:\n            self.logger.debug(f\"Programming {target_matrix.shape} matrix to crossbar\")\n            \n            # Validate input matrix\n            if target_matrix.shape != (self.rows, self.cols):\n                raise ValueError(\n                    f\"Matrix shape {target_matrix.shape} does not match \"\n                    f\"crossbar size {(self.rows, self.cols)}\"\n                )\n            \n            if not np.isfinite(target_matrix).all():\n                raise ValueError(\"Target matrix contains NaN or infinity\")\n            \n            g_min, g_max = self.device_params['g_range']\n            \n            # Decompose into positive and negative components\n            pos_matrix = np.maximum(target_matrix, 0)\n            neg_matrix = np.maximum(-target_matrix, 0)\n            \n            # Scale to conductance range with error handling\n            self.g_positive = self._scale_to_conductance(pos_matrix, g_min, g_max)\n            self.g_negative = self._scale_to_conductance(neg_matrix, g_min, g_max)\n            \n            # Add programming variations\n            self._add_programming_variations()\n            \n            self.is_programmed = True\n            self.programming_errors = 0\n            \n            self.logger.info(\n                f\"Successfully programmed crossbar with matrix range \"\n                f\"[{target_matrix.min():.2e}, {target_matrix.max():.2e}]\"\n            )\n            \n        except Exception as e:\n            self.programming_errors += 1\n            self.logger.error(f\"Programming failed: {e}\")\n            raise\n    \n    def compute_vmm(self, input_vector: np.ndarray) -> np.ndarray:\n        \"\"\"Analog vector-matrix multiplication with comprehensive error handling.\"\"\"\n        try:\n            # Validate inputs\n            if not self.is_programmed:\n                raise RuntimeError(\"Crossbar not programmed\")\n            \n            if len(input_vector) != self.rows:\n                raise ValueError(\n                    f\"Input vector length {len(input_vector)} does not match \"\n                    f\"crossbar rows {self.rows}\"\n                )\n            \n            if not np.isfinite(input_vector).all():\n                raise ValueError(\"Input vector contains NaN or infinity\")\n            \n            # Clamp input voltage range for realistic operation\n            input_clamped = np.clip(input_vector, -1.0, 1.0)\n            \n            # Ohm's law: I = G \u00d7 V with error handling\n            try:\n                i_pos = np.dot(self.g_positive.T, input_clamped)\n                i_neg = np.dot(self.g_negative.T, input_clamped)\n            except np.linalg.LinAlgError as e:\n                raise RuntimeError(f\"Matrix multiplication failed: {e}\")\n            \n            # Differential current sensing\n            output_current = i_pos - i_neg\n            \n            # Add device noise based on model\n            noise = self._compute_noise(output_current)\n            output_with_noise = output_current + noise\n            \n            # Add device non-linearities\n            output_final = self._apply_device_nonlinearities(output_with_noise)\n            \n            # Check for numerical issues\n            if not np.isfinite(output_final).all():\n                self.logger.warning(\"VMM output contains non-finite values\")\n                output_final = np.nan_to_num(output_final, nan=0.0, posinf=1e6, neginf=-1e6)\n            \n            # Update operation counter\n            self.operation_count += 1\n            \n            # Apply device aging effects\n            if self.operation_count % 1000 == 0:\n                self._apply_aging_effects()\n            \n            return output_final\n            \n        except Exception as e:\n            self.logger.error(f\"VMM computation failed: {e}\")\n            raise\n    \n    def _scale_to_conductance(\n        self, \n        matrix: np.ndarray, \n        g_min: float, \n        g_max: float\n    ) -> np.ndarray:\n        \"\"\"Scale matrix values to conductance range with robust handling.\"\"\"\n        try:\n            # Handle zero matrix\n            if np.allclose(matrix, 0):\n                return np.full_like(matrix, g_min, dtype=np.float64)\n            \n            # Robust scaling\n            matrix_max = np.max(np.abs(matrix))\n            if matrix_max == 0:\n                return np.full_like(matrix, g_min, dtype=np.float64)\n            \n            # Linear scaling with bounds checking\n            scaled = g_min + (g_max - g_min) * np.abs(matrix) / matrix_max\n            \n            # Ensure values are within bounds\n            scaled = np.clip(scaled, g_min, g_max)\n            \n            return scaled.astype(np.float64)\n            \n        except Exception as e:\n            self.logger.error(f\"Conductance scaling failed: {e}\")\n            return np.full_like(matrix, g_min, dtype=np.float64)\n    \n    def _compute_noise(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"Add realistic device noise based on specified model.\"\"\"\n        if self.noise_model == \"none\":\n            return np.zeros_like(signal)\n        \n        try:\n            noise = np.zeros_like(signal, dtype=np.float64)\n            \n            if self.noise_model in [\"gaussian\", \"realistic\"]:\n                # Thermal noise (Johnson-Nyquist)\n                thermal_std = self.device_params['thermal_noise'] * np.sqrt(np.abs(signal))\n                thermal_noise = np.random.normal(0, thermal_std)\n                noise += thermal_noise\n            \n            if self.noise_model == \"realistic\":\n                # Shot noise (Poissonian)\n                shot_std = self.device_params['shot_noise'] * np.sqrt(np.abs(signal))\n                shot_noise = np.random.normal(0, shot_std)\n                noise += shot_noise\n                \n                # 1/f (flicker) noise\n                flicker_std = self.device_params['flicker_noise'] * np.abs(signal)\n                flicker_noise = np.random.normal(0, flicker_std)\n                noise += flicker_noise\n                \n                # Random telegraph signal (RTS) noise for memristors\n                if self.cell_type in ['ReRAM', 'PCM']:\n                    rts_noise = self._generate_rts_noise(signal)\n                    noise += rts_noise\n            \n            return noise\n            \n        except Exception as e:\n            self.logger.warning(f\"Noise generation failed: {e}, using no noise\")\n            return np.zeros_like(signal)\n    \n    def _get_device_parameters(self) -> dict:\n        \"\"\"Get device-specific parameters.\"\"\"\n        base_params = {\n            'g_range': (1e-9, 1e-6),  # 1nS to 1\u03bcS\n            'thermal_noise': 0.01,\n            'shot_noise': 0.005,\n            'flicker_noise': 0.001,\n            'programming_variation': 0.05,\n            'drift_rate': 1e-6,\n            'aging_rate': 1e-8\n        }\n        \n        # Device-specific modifications\n        if self.cell_type == \"ReRAM\":\n            base_params['g_range'] = (1e-8, 1e-5)\n            base_params['programming_variation'] = 0.1\n            base_params['aging_rate'] = 5e-8\n        elif self.cell_type == \"PCM\":\n            base_params['g_range'] = (1e-7, 1e-4)\n            base_params['drift_rate'] = 1e-5\n            base_params['aging_rate'] = 2e-8\n        elif self.cell_type == \"1T1R\":\n            base_params['programming_variation'] = 0.02\n            base_params['aging_rate'] = 1e-9\n        \n        return base_params\n    \n    def _add_programming_variations(self):\n        \"\"\"Add realistic programming variations to conductances.\"\"\"\n        variation = self.device_params['programming_variation']\n        \n        # Multiplicative variations (log-normal distribution)\n        pos_variation = np.random.lognormal(0, variation, self.g_positive.shape)\n        neg_variation = np.random.lognormal(0, variation, self.g_negative.shape)\n        \n        self.g_positive *= pos_variation\n        self.g_negative *= neg_variation\n        \n        # Ensure bounds are maintained\n        g_min, g_max = self.device_params['g_range']\n        self.g_positive = np.clip(self.g_positive, g_min, g_max)\n        self.g_negative = np.clip(self.g_negative, g_min, g_max)\n    \n    def _generate_rts_noise(self, signal: np.ndarray) -> np.ndarray:\n        \"\"\"Generate random telegraph signal noise for memristive devices.\"\"\"\n        # Simplified RTS model\n        rts_amplitude = 0.001 * np.abs(signal)\n        rts_probability = 0.1  # 10% chance of switching\n        \n        rts_noise = np.zeros_like(signal)\n        for i in range(len(signal)):\n            if np.random.random() < rts_probability:\n                rts_noise[i] = np.random.choice([-1, 1]) * rts_amplitude[i]\n        \n        return rts_noise\n    \n    def _apply_device_nonlinearities(self, current: np.ndarray) -> np.ndarray:\n        \"\"\"Apply device non-linearities and saturation effects.\"\"\"\n        # Current saturation\n        max_current = 1e-3  # 1mA max per column\n        current_saturated = np.tanh(current / max_current) * max_current\n        \n        # Add conductance modulation effects\n        if self.cell_type == \"ReRAM\":\n            # ReRAM shows conductance modulation under high currents\n            modulation = 1 - 0.1 * np.tanh(np.abs(current_saturated) / 1e-4)\n            current_saturated *= modulation\n        elif self.cell_type == \"PCM\":\n            # PCM shows threshold switching behavior\n            threshold = 1e-5\n            mask = np.abs(current_saturated) > threshold\n            current_saturated[mask] *= 1.2  # Increased conductance above threshold\n        \n        return current_saturated\n    \n    def _apply_aging_effects(self):\n        \"\"\"Apply long-term device aging effects.\"\"\"\n        if not self.is_programmed:\n            return\n            \n        aging_rate = self.device_params['aging_rate']\n        \n        # Gradual conductance drift\n        drift_factor = 1 - aging_rate * self.operation_count\n        drift_factor = max(0.8, drift_factor)  # Limit maximum drift\n        \n        self.g_positive *= drift_factor\n        self.g_negative *= drift_factor\n        \n        # Ensure minimum conductances\n        g_min, _ = self.device_params['g_range']\n        self.g_positive = np.maximum(self.g_positive, g_min)\n        self.g_negative = np.maximum(self.g_negative, g_min)\n        \n        if self.operation_count % 10000 == 0:\n            self.logger.info(f\"Applied aging effects after {self.operation_count} operations\")\n    \n    def get_device_stats(self) -> dict:\n        \"\"\"Get comprehensive device statistics and health metrics.\"\"\"\n        stats = {\n            \"is_programmed\": self.is_programmed,\n            \"programming_errors\": self.programming_errors,\n            \"operation_count\": self.operation_count,\n            \"device_type\": self.cell_type,\n            \"noise_model\": self.noise_model,\n            \"array_size\": (self.rows, self.cols),\n            \"total_devices\": self.rows * self.cols\n        }\n        \n        if self.is_programmed:\n            # Conductance statistics\n            stats.update({\n                \"g_positive_range\": (float(self.g_positive.min()), float(self.g_positive.max())),\n                \"g_negative_range\": (float(self.g_negative.min()), float(self.g_negative.max())),\n                \"g_positive_mean\": float(self.g_positive.mean()),\n                \"g_negative_mean\": float(self.g_negative.mean()),\n                \"g_positive_std\": float(self.g_positive.std()),\n                \"g_negative_std\": float(self.g_negative.std())\n            })\n            \n            # Health metrics\n            g_min, g_max = self.device_params['g_range']\n            stuck_low = np.sum(self.g_positive <= g_min * 1.1) + np.sum(self.g_negative <= g_min * 1.1)\n            stuck_high = np.sum(self.g_positive >= g_max * 0.9) + np.sum(self.g_negative >= g_max * 0.9)\n            \n            stats.update({\n                \"health_stuck_low_devices\": int(stuck_low),\n                \"health_stuck_high_devices\": int(stuck_high),\n                \"health_percentage\": float(100 * (1 - (stuck_low + stuck_high) / (2 * self.rows * self.cols)))\n            })\n        \n        return stats\n    \n    def perform_calibration(self) -> dict:\n        \"\"\"Perform device calibration and return calibration data.\"\"\"\n        if not self.is_programmed:\n            return {\"status\": \"error\", \"message\": \"Crossbar not programmed\"}\n        \n        try:\n            # Test with known input patterns\n            test_patterns = [\n                np.ones(self.rows),\n                np.zeros(self.rows),\n                np.random.random(self.rows)\n            ]\n            \n            calibration_data = {\n                \"status\": \"success\",\n                \"timestamp\": np.datetime64('now').item().isoformat(),\n                \"test_results\": []\n            }\n            \n            for i, pattern in enumerate(test_patterns):\n                result = self.compute_vmm(pattern)\n                calibration_data[\"test_results\"].append({\n                    \"pattern_id\": i,\n                    \"input_norm\": float(np.linalg.norm(pattern)),\n                    \"output_norm\": float(np.linalg.norm(result)),\n                    \"output_mean\": float(np.mean(result)),\n                    \"output_std\": float(np.std(result))\n                })\n            \n            self.logger.info(\"Device calibration completed successfully\")\n            return calibration_data\n            \n        except Exception as e:\n            self.logger.error(f\"Calibration failed: {e}\")\n            return {\"status\": \"error\", \"message\": str(e)}\n    \n    def reset_device(self):\n        \"\"\"Reset device to initial state.\"\"\"\n        self.g_positive = np.zeros((self.rows, self.cols), dtype=np.float64)\n        self.g_negative = np.zeros((self.rows, self.cols), dtype=np.float64)\n        self.is_programmed = False\n        self.programming_errors = 0\n        self.operation_count = 0\n        \n        self.logger.info(\"Device reset to initial state\")"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/core/solver.py",
      "line_number": 1,
      "details": "\"\"\"Main analog PDE solver implementation.\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, Optional, Union\nfrom .crossbar import AnalogCrossbarArray\n\n\nclass AnalogPDESolver:\n    \"\"\"Analog crossbar-based PDE solver with noise modeling.\"\"\"\n    \n    def __init__(\n        self,\n        crossbar_size: int = 128,\n        conductance_range: tuple = (1e-9, 1e-6),\n        noise_model: str = \"realistic\"\n    ):\n        \"\"\"Initialize analog PDE solver.\n        \n        Args:\n            crossbar_size: Size of crossbar array (must be > 0)\n            conductance_range: Min/max conductance values in Siemens (min < max, both > 0)\n            noise_model: Noise modeling approach ('none', 'gaussian', 'realistic')\n            \n        Raises:\n            ValueError: If parameters are invalid\n            TypeError: If parameters have wrong type\n        \"\"\"\n        self.logger = logging.getLogger(__name__)\n        \n        # Validate inputs\n        self._validate_initialization_parameters(crossbar_size, conductance_range, noise_model)\n        \n        self.crossbar_size = crossbar_size\n        self.conductance_range = conductance_range\n        self.noise_model = noise_model\n        \n        try:\n            self.crossbar = AnalogCrossbarArray(crossbar_size, crossbar_size)\n            self.logger.info(f\"Initialized AnalogPDESolver with {crossbar_size}\u00d7{crossbar_size} crossbar\")\n        except Exception as e:\n            self.logger.error(f\"Failed to initialize crossbar: {e}\")\n            raise RuntimeError(f\"Crossbar initialization failed: {e}\") from e\n        \n    def map_pde_to_crossbar(self, pde) -> Dict[str, Any]:\n        \"\"\"Map PDE discretization matrix to crossbar conductances.\n        \n        Args:\n            pde: PDE object to map\n            \n        Returns:\n            Configuration dictionary with mapping results\n            \n        Raises:\n            ValueError: If PDE is invalid\n            RuntimeError: If mapping fails\n        \"\"\"\n        self.logger.debug(\"Starting PDE to crossbar mapping\")\n        \n        # Validate PDE object\n        self._validate_pde_object(pde)\n        \n        try:\n            # Generate finite difference Laplacian matrix\n            size = self.crossbar_size\n            laplacian = self._create_laplacian_matrix(size)\n            \n            # Program crossbar with Laplacian operator\n            self.crossbar.program_conductances(laplacian)\n            \n            self.logger.info(f\"Successfully mapped {size}\u00d7{size} PDE to crossbar\")\n            \n            return {\n                \"matrix_size\": size,\n                \"conductance_range\": self.conductance_range,\n                \"programming_success\": True,\n                \"pde_type\": type(pde).__name__\n            }\n            \n        except Exception as e:\n            self.logger.error(f\"PDE to crossbar mapping failed: {e}\")\n            raise RuntimeError(f\"Failed to map PDE to crossbar: {e}\") from e\n        \n    def solve(\n        self, \n        pde,\n        iterations: int = 100,\n        convergence_threshold: float = 1e-6\n    ) -> np.ndarray:\n        \"\"\"Solve PDE using analog crossbar computation.\n        \n        Args:\n            pde: PDE object to solve\n            iterations: Maximum number of iterations (must be > 0)\n            convergence_threshold: Convergence threshold (must be > 0)\n            \n        Returns:\n            Solution array\n            \n        Raises:\n            ValueError: If parameters are invalid\n            RuntimeError: If solving fails\n        \"\"\"\n        self.logger.debug(f\"Starting PDE solve with {iterations} max iterations\")\n        \n        # Validate inputs\n        self._validate_solve_parameters(iterations, convergence_threshold)\n        \n        try:\n            # Map PDE to crossbar\n            config = self.map_pde_to_crossbar(pde)\n            \n            # Initialize solution vector\n            size = config[\"matrix_size\"]\n            phi = np.random.random(size) * 0.1\n            \n            # Create source term\n            if hasattr(pde, 'source_function') and pde.source_function:\n                x = np.linspace(0, 1, size)\n                source = np.array([pde.source_function(xi, 0) for xi in x])\n            else:\n                source = np.ones(size) * 0.1\n            \n            # Iterative analog solver with convergence tracking\n            convergence_history = []\n            \n            for i in range(iterations):\n                try:\n                    # Analog matrix-vector multiplication\n                    residual = self.crossbar.compute_vmm(phi) + source\n                    \n                    # Jacobi-style update with stability check\n                    phi_new = phi - 0.1 * residual\n                    \n                    # Check for numerical instability\n                    if not np.isfinite(phi_new).all():\n                        self.logger.warning(f\"Numerical instability detected at iteration {i}\")\n                        phi_new = np.clip(phi_new, -1e6, 1e6)  # Clamp values\n                        phi_new[~np.isfinite(phi_new)] = 0.0   # Replace NaN/inf with zero\n                    \n                    # Apply boundary conditions\n                    phi_new[0] = 0.0  # Dirichlet BC\n                    phi_new[-1] = 0.0\n                    \n                    # Check convergence\n                    error = np.linalg.norm(phi_new - phi)\n                    convergence_history.append(error)\n                    \n                    phi = phi_new\n                    \n                    if error < convergence_threshold:\n                        self.logger.info(f\"Converged after {i+1} iterations (error: {error:.2e})\")\n                        break\n                        \n                    if i > 10 and error > convergence_history[-10]:\n                        self.logger.warning(\"Convergence may be stalling or diverging\")\n                        \n                except Exception as e:\n                    self.logger.error(f\"Error in iteration {i}: {e}\")\n                    raise RuntimeError(f\"Solver failed at iteration {i}: {e}\") from e\n            else:\n                self.logger.warning(f\"Did not converge after {iterations} iterations (error: {convergence_history[-1]:.2e})\")\n            \n            self.logger.debug(f\"Solution computed, norm: {np.linalg.norm(phi):.6f}\")\n            return phi\n            \n        except Exception as e:\n            self.logger.error(f\"PDE solving failed: {e}\")\n            if isinstance(e, (ValueError, RuntimeError)):\n                raise  # Re-raise validation and solver errors as-is\n            else:\n                raise RuntimeError(f\"Unexpected error during solving: {e}\") from e\n    \n    def _create_laplacian_matrix(self, size: int) -> np.ndarray:\n        \"\"\"Create finite difference Laplacian matrix.\"\"\"\n        laplacian = np.zeros((size, size))\n        \n        # Main diagonal\n        np.fill_diagonal(laplacian, -2.0)\n        \n        # Off-diagonals\n        for i in range(size - 1):\n            laplacian[i, i + 1] = 1.0\n            laplacian[i + 1, i] = 1.0\n            \n        return laplacian\n    \n    def _validate_initialization_parameters(\n        self, \n        crossbar_size: int, \n        conductance_range: tuple, \n        noise_model: str\n    ) -> None:\n        \"\"\"Validate initialization parameters.\n        \n        Args:\n            crossbar_size: Crossbar array size\n            conductance_range: Conductance range tuple\n            noise_model: Noise model string\n            \n        Raises:\n            ValueError: If parameters are invalid\n            TypeError: If parameters have wrong type\n        \"\"\"\n        # Validate crossbar_size\n        if not isinstance(crossbar_size, int):\n            raise TypeError(f\"crossbar_size must be int, got {type(crossbar_size)}\")\n        if crossbar_size <= 0:\n            raise ValueError(f\"crossbar_size must be positive, got {crossbar_size}\")\n        if crossbar_size > 10000:\n            raise ValueError(f\"crossbar_size too large (>{10000}), got {crossbar_size}\")\n            \n        # Validate conductance_range\n        if not isinstance(conductance_range, (tuple, list)):\n            raise TypeError(f\"conductance_range must be tuple/list, got {type(conductance_range)}\")\n        if len(conductance_range) != 2:\n            raise ValueError(f\"conductance_range must have 2 elements, got {len(conductance_range)}\")\n        \n        g_min, g_max = conductance_range\n        if not isinstance(g_min, (int, float)) or not isinstance(g_max, (int, float)):\n            raise TypeError(\"conductance_range values must be numeric\")\n        if g_min <= 0 or g_max <= 0:\n            raise ValueError(f\"conductance values must be positive, got ({g_min}, {g_max})\")\n        if g_min >= g_max:\n            raise ValueError(f\"g_min must be < g_max, got ({g_min}, {g_max})\")\n            \n        # Validate noise_model\n        if not isinstance(noise_model, str):\n            raise TypeError(f\"noise_model must be str, got {type(noise_model)}\")\n        valid_noise_models = [\"none\", \"gaussian\", \"realistic\"]\n        if noise_model not in valid_noise_models:\n            raise ValueError(f\"noise_model must be one of {valid_noise_models}, got '{noise_model}'\")\n    \n    def _validate_pde_object(self, pde) -> None:\n        \"\"\"Validate PDE object has required attributes.\n        \n        Args:\n            pde: PDE object to validate\n            \n        Raises:\n            ValueError: If PDE object is invalid\n            AttributeError: If required attributes are missing\n        \"\"\"\n        if pde is None:\n            raise ValueError(\"PDE object cannot be None\")\n            \n        # Check for required attributes\n        if not hasattr(pde, 'domain_size'):\n            raise AttributeError(\"PDE object must have 'domain_size' attribute\")\n            \n        # Validate domain size compatibility\n        if isinstance(pde.domain_size, (tuple, list)):\n            domain_size = pde.domain_size[0] if len(pde.domain_size) > 0 else 0\n        else:\n            domain_size = pde.domain_size\n            \n        if domain_size != self.crossbar_size:\n            self.logger.warning(\n                f\"PDE domain size ({domain_size}) != crossbar size ({self.crossbar_size}). \"\n                \"This may cause issues.\"\n            )\n    \n    def _validate_solve_parameters(\n        self, \n        iterations: int, \n        convergence_threshold: float\n    ) -> None:\n        \"\"\"Validate solve method parameters.\n        \n        Args:\n            iterations: Number of iterations\n            convergence_threshold: Convergence threshold\n            \n        Raises:\n            ValueError: If parameters are invalid\n            TypeError: If parameters have wrong type\n        \"\"\"\n        if not isinstance(iterations, int):\n            raise TypeError(f\"iterations must be int, got {type(iterations)}\")\n        if iterations <= 0:\n            raise ValueError(f\"iterations must be positive, got {iterations}\")\n        if iterations > 100000:\n            raise ValueError(f\"iterations too large (>100000), got {iterations}\")\n            \n        if not isinstance(convergence_threshold, (int, float)):\n            raise TypeError(f\"convergence_threshold must be numeric, got {type(convergence_threshold)}\")\n        if convergence_threshold <= 0:\n            raise ValueError(f\"convergence_threshold must be positive, got {convergence_threshold}\")\n        if convergence_threshold > 1.0:\n            raise ValueError(f\"convergence_threshold too large (>1.0), got {convergence_threshold}\")"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/optimization/advanced_algorithms.py",
      "line_number": 1,
      "details": "\"\"\"Advanced optimization algorithms for analog PDE solving.\"\"\"\n\nimport numpy as np\nimport time\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple, Callable, Union\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport threading\n\nlogger = logging.getLogger(__name__)\n\n\nclass AlgorithmType(Enum):\n    \"\"\"Advanced algorithm types for optimization.\"\"\"\n    MULTIGRID = \"multigrid\"\n    ADAPTIVE_MESH_REFINEMENT = \"amr\"\n    PRECONDITIONING = \"preconditioning\"\n    KRYLOV_SUBSPACE = \"krylov\"\n    DOMAIN_DECOMPOSITION = \"domain_decomp\"\n    NEURAL_ACCELERATION = \"neural_accel\"\n\n\n@dataclass \nclass OptimizationResult:\n    \"\"\"Result from advanced optimization algorithm.\"\"\"\n    solution: np.ndarray\n    convergence_rate: float\n    iterations: int\n    computation_time: float\n    memory_usage: float\n    algorithm_used: AlgorithmType\n    accuracy_metrics: Dict[str, float]\n    performance_metrics: Dict[str, float]\n\n\nclass MultiGridSolver:\n    \"\"\"Multigrid solver for efficient hierarchical PDE solving.\"\"\"\n    \n    def __init__(\n        self,\n        levels: int = 4,\n        smoother: str = \"jacobi\",\n        coarsening_factor: int = 2,\n        max_iterations_per_level: int = 10\n    ):\n        \"\"\"Initialize multigrid solver.\n        \n        Args:\n            levels: Number of grid levels\n            smoother: Smoothing method (jacobi, gauss_seidel, sor)\n            coarsening_factor: Grid coarsening ratio\n            max_iterations_per_level: Max smoothing iterations per level\n        \"\"\"\n        self.levels = levels\n        self.smoother = smoother\n        self.coarsening_factor = coarsening_factor\n        self.max_iterations = max_iterations_per_level\n        self.logger = logging.getLogger(__name__)\n        \n    def solve(\n        self,\n        crossbar_hierarchy: List[Any],\n        initial_solution: np.ndarray,\n        rhs: np.ndarray,\n        tolerance: float = 1e-6\n    ) -> OptimizationResult:\n        \"\"\"Solve using V-cycle multigrid algorithm.\n        \n        Args:\n            crossbar_hierarchy: List of crossbar arrays for each level\n            initial_solution: Initial guess\n            rhs: Right-hand side vector\n            tolerance: Convergence tolerance\n            \n        Returns:\n            Optimization result with solution and metrics\n        \"\"\"\n        start_time = time.perf_counter()\n        \n        # Initialize grid hierarchy\n        solutions = [initial_solution]\n        rhs_vectors = [rhs]\n        \n        # Create coarser levels\n        for level in range(1, self.levels):\n            coarse_size = len(solutions[-1]) // self.coarsening_factor\n            coarse_solution = self._restrict(solutions[-1], coarse_size)\n            coarse_rhs = self._restrict(rhs_vectors[-1], coarse_size)\n            \n            solutions.append(coarse_solution)\n            rhs_vectors.append(coarse_rhs)\n        \n        residuals = []\n        v_cycles = 0\n        \n        # V-cycle iterations\n        while v_cycles < 50:  # Max V-cycles\n            old_solution = solutions[0].copy()\n            \n            # V-cycle down (restriction)\n            for level in range(self.levels - 1):\n                solutions[level] = self._smooth(\n                    crossbar_hierarchy[level],\n                    solutions[level],\n                    rhs_vectors[level],\n                    self.max_iterations\n                )\n                \n                # Compute residual and restrict to coarser level\n                residual = self._compute_residual(\n                    crossbar_hierarchy[level],\n                    solutions[level],\n                    rhs_vectors[level]\n                )\n                \n                if level + 1 < len(rhs_vectors):\n                    rhs_vectors[level + 1] = self._restrict(residual, len(solutions[level + 1]))\n            \n            # Coarsest level solve\n            solutions[-1] = self._direct_solve(\n                crossbar_hierarchy[-1] if len(crossbar_hierarchy) >= self.levels else None,\n                solutions[-1],\n                rhs_vectors[-1]\n            )\n            \n            # V-cycle up (prolongation and correction)\n            for level in range(self.levels - 2, -1, -1):\n                # Prolongate correction from coarser level\n                correction = self._prolongate(solutions[level + 1], len(solutions[level]))\n                solutions[level] += correction\n                \n                # Post-smoothing\n                solutions[level] = self._smooth(\n                    crossbar_hierarchy[level],\n                    solutions[level],\n                    rhs_vectors[level],\n                    self.max_iterations // 2\n                )\n            \n            # Check convergence\n            residual_norm = np.linalg.norm(solutions[0] - old_solution)\n            residuals.append(residual_norm)\n            v_cycles += 1\n            \n            self.logger.debug(f\"V-cycle {v_cycles}: residual = {residual_norm:.2e}\")\n            \n            if residual_norm < tolerance:\n                break\n        \n        computation_time = time.perf_counter() - start_time\n        \n        # Calculate convergence rate\n        if len(residuals) > 1:\n            convergence_rate = np.log(residuals[-1] / residuals[0]) / len(residuals)\n        else:\n            convergence_rate = 0.0\n        \n        return OptimizationResult(\n            solution=solutions[0],\n            convergence_rate=abs(convergence_rate),\n            iterations=v_cycles,\n            computation_time=computation_time,\n            memory_usage=sum(sol.nbytes for sol in solutions) / 1024**2,  # MB\n            algorithm_used=AlgorithmType.MULTIGRID,\n            accuracy_metrics={\"final_residual\": residuals[-1] if residuals else 0.0},\n            performance_metrics={\n                \"v_cycles\": v_cycles,\n                \"avg_time_per_cycle\": computation_time / max(1, v_cycles)\n            }\n        )\n    \n    def _restrict(self, fine_vector: np.ndarray, coarse_size: int) -> np.ndarray:\n        \"\"\"Restrict fine grid vector to coarse grid.\"\"\"\n        if coarse_size >= len(fine_vector):\n            return fine_vector.copy()\n        \n        # Simple injection for now (could use full weighting)\n        stride = len(fine_vector) // coarse_size\n        return fine_vector[::stride][:coarse_size]\n    \n    def _prolongate(self, coarse_vector: np.ndarray, fine_size: int) -> np.ndarray:\n        \"\"\"Prolongate coarse grid vector to fine grid.\"\"\"\n        if fine_size <= len(coarse_vector):\n            return coarse_vector[:fine_size]\n        \n        # Linear interpolation\n        fine_vector = np.zeros(fine_size)\n        ratio = len(coarse_vector) / fine_size\n        \n        for i in range(fine_size):\n            coarse_idx = i * ratio\n            idx_low = int(np.floor(coarse_idx))\n            idx_high = min(idx_low + 1, len(coarse_vector) - 1)\n            \n            if idx_low == idx_high:\n                fine_vector[i] = coarse_vector[idx_low]\n            else:\n                weight = coarse_idx - idx_low\n                fine_vector[i] = (1 - weight) * coarse_vector[idx_low] + weight * coarse_vector[idx_high]\n        \n        return fine_vector\n    \n    def _smooth(\n        self,\n        crossbar: Any,\n        solution: np.ndarray,\n        rhs: np.ndarray,\n        iterations: int\n    ) -> np.ndarray:\n        \"\"\"Apply smoothing iterations.\"\"\"\n        current_solution = solution.copy()\n        \n        for _ in range(iterations):\n            if self.smoother == \"jacobi\":\n                current_solution = self._jacobi_step(crossbar, current_solution, rhs)\n            elif self.smoother == \"gauss_seidel\":\n                current_solution = self._gauss_seidel_step(crossbar, current_solution, rhs)\n            else:\n                # Default to simple damped iteration\n                residual = self._compute_residual(crossbar, current_solution, rhs)\n                current_solution -= 0.1 * residual\n        \n        return current_solution\n    \n    def _jacobi_step(self, crossbar: Any, solution: np.ndarray, rhs: np.ndarray) -> np.ndarray:\n        \"\"\"Single Jacobi smoothing step.\"\"\"\n        if crossbar is None:\n            return solution\n        \n        # Compute residual using crossbar\n        residual = crossbar.compute_vmm(solution) - rhs\n        \n        # Jacobi update with damping\n        return solution - 0.2 * residual  # 0.2 is damping factor\n    \n    def _gauss_seidel_step(self, crossbar: Any, solution: np.ndarray, rhs: np.ndarray) -> np.ndarray:\n        \"\"\"Single Gauss-Seidel smoothing step.\"\"\"\n        if crossbar is None:\n            return solution\n        \n        # Simplified Gauss-Seidel (would need diagonal extraction for true GS)\n        residual = crossbar.compute_vmm(solution) - rhs\n        return solution - 0.15 * residual\n    \n    def _compute_residual(self, crossbar: Any, solution: np.ndarray, rhs: np.ndarray) -> np.ndarray:\n        \"\"\"Compute residual r = Ax - b.\"\"\"\n        if crossbar is None:\n            return np.zeros_like(solution)\n        \n        return crossbar.compute_vmm(solution) - rhs\n    \n    def _direct_solve(self, crossbar: Any, solution: np.ndarray, rhs: np.ndarray) -> np.ndarray:\n        \"\"\"Direct solve on coarsest level.\"\"\"\n        if crossbar is None or len(solution) > 10:\n            # Too large for direct solve, use iterative\n            return self._smooth(crossbar, solution, rhs, 20)\n        \n        # For very small systems, could use direct methods\n        return self._smooth(crossbar, solution, rhs, 50)\n\n\nclass AdaptiveMeshRefinement:\n    \"\"\"Adaptive mesh refinement for analog PDE solving.\"\"\"\n    \n    def __init__(\n        self,\n        refinement_threshold: float = 0.1,\n        coarsening_threshold: float = 0.01,\n        max_refinement_levels: int = 3\n    ):\n        \"\"\"Initialize adaptive mesh refinement.\n        \n        Args:\n            refinement_threshold: Error threshold for refinement\n            coarsening_threshold: Error threshold for coarsening  \n            max_refinement_levels: Maximum refinement levels\n        \"\"\"\n        self.refinement_threshold = refinement_threshold\n        self.coarsening_threshold = coarsening_threshold\n        self.max_levels = max_refinement_levels\n        self.logger = logging.getLogger(__name__)\n    \n    def solve_with_amr(\n        self,\n        base_solver: Any,\n        pde: Any,\n        initial_mesh_size: int = 64\n    ) -> OptimizationResult:\n        \"\"\"Solve PDE with adaptive mesh refinement.\n        \n        Args:\n            base_solver: Base PDE solver\n            pde: PDE problem definition\n            initial_mesh_size: Initial mesh size\n            \n        Returns:\n            Optimization result with adapted solution\n        \"\"\"\n        start_time = time.perf_counter()\n        \n        # Start with coarse mesh\n        current_mesh_size = initial_mesh_size\n        refinement_level = 0\n        \n        solutions = []\n        error_estimates = []\n        \n        while refinement_level <= self.max_levels:\n            self.logger.info(f\"AMR level {refinement_level}: mesh size {current_mesh_size}\")\n            \n            # Solve on current mesh\n            if hasattr(pde, 'set_domain_size'):\n                pde.set_domain_size((current_mesh_size,))\n            \n            solution = base_solver.solve(pde)\n            solutions.append(solution)\n            \n            # Estimate error\n            error_estimate = self._estimate_error(solution, pde)\n            error_estimates.append(error_estimate)\n            \n            self.logger.debug(f\"Error estimate at level {refinement_level}: {error_estimate:.2e}\")\n            \n            # Check refinement criteria\n            if error_estimate > self.refinement_threshold and refinement_level < self.max_levels:\n                # Refine mesh\n                current_mesh_size *= 2\n                refinement_level += 1\n                \n                # Update solver for new mesh size\n                if hasattr(base_solver, 'crossbar_size'):\n                    base_solver.crossbar_size = current_mesh_size\n                    base_solver.crossbar = type(base_solver.crossbar)(current_mesh_size, current_mesh_size)\n                \n            else:\n                # Converged or max levels reached\n                break\n        \n        computation_time = time.perf_counter() - start_time\n        \n        # Calculate convergence rate from error reduction\n        if len(error_estimates) > 1:\n            convergence_rate = np.log(error_estimates[-1] / error_estimates[0]) / len(error_estimates)\n        else:\n            convergence_rate = 0.0\n        \n        return OptimizationResult(\n            solution=solutions[-1],\n            convergence_rate=abs(convergence_rate),\n            iterations=refinement_level + 1,\n            computation_time=computation_time,\n            memory_usage=sum(sol.nbytes for sol in solutions) / 1024**2,\n            algorithm_used=AlgorithmType.ADAPTIVE_MESH_REFINEMENT,\n            accuracy_metrics={\"final_error_estimate\": error_estimates[-1]},\n            performance_metrics={\n                \"refinement_levels\": refinement_level,\n                \"final_mesh_size\": current_mesh_size,\n                \"error_reduction\": error_estimates[0] / error_estimates[-1] if len(error_estimates) > 1 else 1.0\n            }\n        )\n    \n    def _estimate_error(self, solution: np.ndarray, pde: Any) -> float:\n        \"\"\"Estimate solution error using various indicators.\"\"\"\n        # Gradient-based error estimate\n        if solution.ndim == 1:\n            gradients = np.gradient(solution)\n            second_derivatives = np.gradient(gradients)\n            \n            # High second derivative indicates need for refinement\n            error_indicator = np.std(second_derivatives)\n            \n        elif solution.ndim == 2:\n            # 2D gradient-based estimate\n            grad_x, grad_y = np.gradient(solution)\n            error_indicator = np.std(grad_x) + np.std(grad_y)\n            \n        else:\n            # Fallback: use solution variation\n            error_indicator = np.std(solution)\n        \n        return error_indicator\n\n\nclass PreconditionedSolver:\n    \"\"\"Preconditioned iterative solver for improved convergence.\"\"\"\n    \n    def __init__(\n        self,\n        preconditioner_type: str = \"jacobi\",\n        tolerance: float = 1e-6,\n        max_iterations: int = 1000\n    ):\n        \"\"\"Initialize preconditioned solver.\n        \n        Args:\n            preconditioner_type: Type of preconditioner (jacobi, ilu, multigrid)\n            tolerance: Convergence tolerance\n            max_iterations: Maximum iterations\n        \"\"\"\n        self.preconditioner_type = preconditioner_type\n        self.tolerance = tolerance\n        self.max_iterations = max_iterations\n        self.logger = logging.getLogger(__name__)\n    \n    def solve_preconditioned(\n        self,\n        crossbar: Any,\n        rhs: np.ndarray,\n        initial_guess: Optional[np.ndarray] = None\n    ) -> OptimizationResult:\n        \"\"\"Solve using preconditioned conjugate gradient method.\n        \n        Args:\n            crossbar: Crossbar array representing system matrix\n            rhs: Right-hand side vector\n            initial_guess: Initial solution guess\n            \n        Returns:\n            Optimization result with preconditioned solution\n        \"\"\"\n        start_time = time.perf_counter()\n        \n        # Initialize\n        n = len(rhs)\n        x = initial_guess if initial_guess is not None else np.zeros(n)\n        \n        # Build preconditioner\n        preconditioner = self._build_preconditioner(crossbar)\n        \n        # Initial residual\n        r = rhs - crossbar.compute_vmm(x)\n        z = self._apply_preconditioner(preconditioner, r)\n        p = z.copy()\n        \n        residuals = []\n        rsold = np.dot(r, z)\n        \n        for iteration in range(self.max_iterations):\n            # CG step\n            Ap = crossbar.compute_vmm(p)\n            alpha = rsold / np.dot(p, Ap)\n            x = x + alpha * p\n            r = r - alpha * Ap\n            \n            residual_norm = np.linalg.norm(r)\n            residuals.append(residual_norm)\n            \n            if residual_norm < self.tolerance:\n                self.logger.info(f\"Preconditioned CG converged in {iteration + 1} iterations\")\n                break\n            \n            # Update search direction\n            z = self._apply_preconditioner(preconditioner, r)\n            rsnew = np.dot(r, z)\n            beta = rsnew / rsold\n            p = z + beta * p\n            rsold = rsnew\n        \n        computation_time = time.perf_counter() - start_time\n        \n        # Calculate convergence rate\n        if len(residuals) > 1:\n            convergence_rate = np.log(residuals[-1] / residuals[0]) / len(residuals)\n        else:\n            convergence_rate = 0.0\n        \n        return OptimizationResult(\n            solution=x,\n            convergence_rate=abs(convergence_rate),\n            iterations=len(residuals),\n            computation_time=computation_time,\n            memory_usage=(x.nbytes + len(residuals) * 8) / 1024**2,\n            algorithm_used=AlgorithmType.PRECONDITIONING,\n            accuracy_metrics={\"final_residual\": residuals[-1] if residuals else 0.0},\n            performance_metrics={\n                \"preconditioner_type\": self.preconditioner_type,\n                \"condition_improvement\": self._estimate_condition_improvement(crossbar, preconditioner)\n            }\n        )\n    \n    def _build_preconditioner(self, crossbar: Any) -> Dict[str, Any]:\n        \"\"\"Build preconditioner matrix/operator.\"\"\"\n        if self.preconditioner_type == \"jacobi\":\n            # Diagonal preconditioner\n            diagonal = self._extract_diagonal(crossbar)\n            return {\"type\": \"jacobi\", \"diagonal\": diagonal}\n            \n        elif self.preconditioner_type == \"block_jacobi\":\n            # Block diagonal preconditioner\n            return {\"type\": \"block_jacobi\", \"crossbar\": crossbar}\n            \n        else:\n            # Identity preconditioner (no preconditioning)\n            return {\"type\": \"identity\"}\n    \n    def _apply_preconditioner(self, preconditioner: Dict[str, Any], vector: np.ndarray) -> np.ndarray:\n        \"\"\"Apply preconditioner to vector.\"\"\"\n        if preconditioner[\"type\"] == \"jacobi\":\n            diagonal = preconditioner[\"diagonal\"]\n            return vector / (diagonal + 1e-12)  # Avoid division by zero\n            \n        elif preconditioner[\"type\"] == \"block_jacobi\":\n            # Simplified block application\n            return vector * 0.5  # Simple scaling\n            \n        else:\n            # Identity preconditioner\n            return vector\n    \n    def _extract_diagonal(self, crossbar: Any) -> np.ndarray:\n        \"\"\"Extract diagonal elements from crossbar.\"\"\"\n        n = crossbar.rows\n        diagonal = np.zeros(n)\n        \n        # Estimate diagonal by applying unit vectors\n        for i in range(n):\n            unit_vec = np.zeros(n)\n            unit_vec[i] = 1.0\n            result = crossbar.compute_vmm(unit_vec)\n            diagonal[i] = result[i] if i < len(result) else 1.0\n        \n        return diagonal\n    \n    def _estimate_condition_improvement(self, crossbar: Any, preconditioner: Dict[str, Any]) -> float:\n        \"\"\"Estimate condition number improvement from preconditioning.\"\"\"\n        # Simplified estimate - in practice would need eigenvalue analysis\n        if preconditioner[\"type\"] == \"jacobi\":\n            return 2.0  # Typical improvement for Jacobi preconditioning\n        elif preconditioner[\"type\"] == \"block_jacobi\":\n            return 5.0  # Better improvement for block methods\n        else:\n            return 1.0  # No improvement\n\n\nclass AdvancedAlgorithmSuite:\n    \"\"\"Suite of advanced optimization algorithms for analog PDE solving.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize advanced algorithm suite.\"\"\"\n        self.multigrid = MultiGridSolver()\n        self.amr = AdaptiveMeshRefinement()\n        self.preconditioned = PreconditionedSolver()\n        self.logger = logging.getLogger(__name__)\n        \n        # Algorithm selection history for learning\n        self.algorithm_performance: Dict[str, List[float]] = {}\n        \n    def auto_select_algorithm(\n        self,\n        pde_characteristics: Dict[str, Any],\n        performance_targets: Dict[str, float]\n    ) -> AlgorithmType:\n        \"\"\"Automatically select best algorithm based on problem characteristics.\n        \n        Args:\n            pde_characteristics: PDE problem characteristics\n            performance_targets: Desired performance metrics\n            \n        Returns:\n            Recommended algorithm type\n        \"\"\"\n        problem_size = pde_characteristics.get(\"size\", 100)\n        conditioning = pde_characteristics.get(\"condition_number\", 100)\n        sparsity = pde_characteristics.get(\"sparsity\", 0.1)\n        geometry_complexity = pde_characteristics.get(\"geometry_complexity\", 0.5)\n        \n        target_time = performance_targets.get(\"max_time_seconds\", 60.0)\n        target_accuracy = performance_targets.get(\"accuracy\", 1e-6)\n        \n        # Decision logic based on problem characteristics\n        if problem_size > 1000 and geometry_complexity < 0.3:\n            # Large, simple geometry -> Multigrid\n            return AlgorithmType.MULTIGRID\n            \n        elif geometry_complexity > 0.7 or pde_characteristics.get(\"has_singularities\", False):\n            # Complex geometry or singularities -> AMR\n            return AlgorithmType.ADAPTIVE_MESH_REFINEMENT\n            \n        elif conditioning > 1000 or sparsity > 0.8:\n            # Ill-conditioned or very sparse -> Preconditioning\n            return AlgorithmType.PRECONDITIONING\n            \n        else:\n            # Default to multigrid for most problems\n            return AlgorithmType.MULTIGRID\n    \n    def solve_with_algorithm(\n        self,\n        algorithm_type: AlgorithmType,\n        solver: Any,\n        pde: Any,\n        **kwargs\n    ) -> OptimizationResult:\n        \"\"\"Solve using specified advanced algorithm.\n        \n        Args:\n            algorithm_type: Algorithm to use\n            solver: Base solver instance\n            pde: PDE problem\n            **kwargs: Algorithm-specific parameters\n            \n        Returns:\n            Optimization result\n        \"\"\"\n        start_time = time.perf_counter()\n        \n        try:\n            if algorithm_type == AlgorithmType.MULTIGRID:\n                # Create crossbar hierarchy\n                crossbar_hierarchy = self._create_crossbar_hierarchy(solver, **kwargs)\n                initial_solution = np.random.random(solver.crossbar_size) * 0.1\n                rhs = np.ones(solver.crossbar_size) * 0.1\n                \n                result = self.multigrid.solve(crossbar_hierarchy, initial_solution, rhs)\n                \n            elif algorithm_type == AlgorithmType.ADAPTIVE_MESH_REFINEMENT:\n                result = self.amr.solve_with_amr(solver, pde)\n                \n            elif algorithm_type == AlgorithmType.PRECONDITIONING:\n                rhs = np.ones(solver.crossbar_size) * 0.1\n                result = self.preconditioned.solve_preconditioned(solver.crossbar, rhs)\n                \n            else:\n                raise ValueError(f\"Algorithm type {algorithm_type} not implemented\")\n            \n            # Record performance for future algorithm selection\n            self._record_algorithm_performance(algorithm_type, result)\n            \n            return result\n            \n        except Exception as e:\n            self.logger.error(f\"Algorithm {algorithm_type} failed: {e}\")\n            \n            # Fallback result\n            return OptimizationResult(\n                solution=np.zeros(solver.crossbar_size),\n                convergence_rate=0.0,\n                iterations=0,\n                computation_time=time.perf_counter() - start_time,\n                memory_usage=0.0,\n                algorithm_used=algorithm_type,\n                accuracy_metrics={\"error\": \"algorithm_failed\"},\n                performance_metrics={\"status\": \"failed\"}\n            )\n    \n    def _create_crossbar_hierarchy(self, solver: Any, **kwargs) -> List[Any]:\n        \"\"\"Create hierarchy of crossbar arrays for multigrid.\"\"\"\n        hierarchy = [solver.crossbar]\n        \n        levels = kwargs.get(\"levels\", 4)\n        current_size = solver.crossbar_size\n        \n        for level in range(1, levels):\n            current_size = max(4, current_size // 2)\n            \n            # Create smaller crossbar for coarser level\n            coarse_crossbar = type(solver.crossbar)(current_size, current_size)\n            \n            # Initialize with appropriate conductances\n            coarse_crossbar.g_positive = np.random.uniform(1e-8, 1e-6, (current_size, current_size))\n            coarse_crossbar.g_negative = np.random.uniform(1e-9, 1e-7, (current_size, current_size))\n            \n            hierarchy.append(coarse_crossbar)\n        \n        return hierarchy\n    \n    def _record_algorithm_performance(self, algorithm_type: AlgorithmType, result: OptimizationResult):\n        \"\"\"Record algorithm performance for learning.\"\"\"\n        if algorithm_type.value not in self.algorithm_performance:\n            self.algorithm_performance[algorithm_type.value] = []\n        \n        # Compute performance score (higher is better)\n        score = result.convergence_rate / max(0.001, result.computation_time)\n        \n        self.algorithm_performance[algorithm_type.value].append(score)\n        \n        # Keep only recent performance data\n        if len(self.algorithm_performance[algorithm_type.value]) > 100:\n            self.algorithm_performance[algorithm_type.value] = \\\n                self.algorithm_performance[algorithm_type.value][-50:]\n    \n    def get_algorithm_recommendations(self, pde_characteristics: Dict[str, Any]) -> List[Tuple[AlgorithmType, float]]:\n        \"\"\"Get ranked algorithm recommendations based on historical performance.\n        \n        Args:\n            pde_characteristics: Problem characteristics\n            \n        Returns:\n            List of (algorithm, confidence_score) tuples, ranked by recommendation\n        \"\"\"\n        recommendations = []\n        \n        for algorithm_name, performance_history in self.algorithm_performance.items():\n            if performance_history:\n                avg_performance = np.mean(performance_history[-20:])  # Recent performance\n                confidence = min(1.0, len(performance_history) / 20.0)  # More data = higher confidence\n                \n                algorithm_type = AlgorithmType(algorithm_name)\n                recommendations.append((algorithm_type, avg_performance * confidence))\n        \n        # Sort by recommendation score\n        recommendations.sort(key=lambda x: x[1], reverse=True)\n        \n        return recommendations\n    \n    def benchmark_algorithms(\n        self,\n        solver: Any,\n        pde: Any,\n        algorithms: Optional[List[AlgorithmType]] = None\n    ) -> Dict[AlgorithmType, OptimizationResult]:\n        \"\"\"Benchmark multiple algorithms on the same problem.\n        \n        Args:\n            solver: Base solver instance\n            pde: PDE problem\n            algorithms: List of algorithms to benchmark (default: all available)\n            \n        Returns:\n            Dictionary mapping algorithms to their results\n        \"\"\"\n        if algorithms is None:\n            algorithms = [\n                AlgorithmType.MULTIGRID,\n                AlgorithmType.ADAPTIVE_MESH_REFINEMENT,\n                AlgorithmType.PRECONDITIONING\n            ]\n        \n        results = {}\n        \n        for algorithm in algorithms:\n            self.logger.info(f\"Benchmarking {algorithm.value}\")\n            \n            try:\n                # Create fresh solver instance to avoid state contamination\n                fresh_solver = type(solver)(\n                    crossbar_size=solver.crossbar_size,\n                    conductance_range=solver.conductance_range,\n                    noise_model=solver.noise_model\n                )\n                \n                result = self.solve_with_algorithm(algorithm, fresh_solver, pde)\n                results[algorithm] = result\n                \n                self.logger.info(f\"{algorithm.value}: {result.iterations} iterations, \"\n                               f\"{result.computation_time:.3f}s, \"\n                               f\"convergence rate: {result.convergence_rate:.3f}\")\n                \n            except Exception as e:\n                self.logger.error(f\"Benchmark failed for {algorithm.value}: {e}\")\n                \n                # Create failure result\n                results[algorithm] = OptimizationResult(\n                    solution=np.zeros(solver.crossbar_size),\n                    convergence_rate=0.0,\n                    iterations=0,\n                    computation_time=float('inf'),\n                    memory_usage=0.0,\n                    algorithm_used=algorithm,\n                    accuracy_metrics={\"status\": \"failed\"},\n                    performance_metrics={\"error\": str(e)}\n                )\n        \n        return results\n    \n    def generate_algorithm_report(self, results: Dict[AlgorithmType, OptimizationResult]) -> str:\n        \"\"\"Generate comprehensive algorithm benchmark report.\n        \n        Args:\n            results: Algorithm benchmark results\n            \n        Returns:\n            Formatted report string\n        \"\"\"\n        report_lines = [\n            \"=\" * 60,\n            \"ADVANCED ALGORITHM BENCHMARK REPORT\",\n            \"=\" * 60,\n            \"\"\n        ]\n        \n        # Sort results by performance (fastest convergence with lowest time)\n        sorted_results = sorted(\n            results.items(),\n            key=lambda x: x[1].convergence_rate / max(0.001, x[1].computation_time),\n            reverse=True\n        )\n        \n        report_lines.append(\"\ud83c\udfc6 Algorithm Performance Ranking:\")\n        for i, (algorithm, result) in enumerate(sorted_results, 1):\n            status = \"\u2705\" if result.computation_time < float('inf') else \"\u274c\"\n            \n            report_lines.append(f\"{i}. {status} {algorithm.value.upper()}\")\n            report_lines.append(f\"   Time: {result.computation_time:.3f}s\")\n            report_lines.append(f\"   Iterations: {result.iterations}\")\n            report_lines.append(f\"   Convergence Rate: {result.convergence_rate:.3f}\")\n            report_lines.append(f\"   Memory: {result.memory_usage:.1f} MB\")\n            report_lines.append(\"\")\n        \n        # Performance metrics comparison\n        report_lines.extend([\n            \"\ud83d\udcca Detailed Metrics Comparison:\",\n            \"\"\n        ])\n        \n        metrics_table = []\n        headers = [\"Algorithm\", \"Time (s)\", \"Iterations\", \"Conv. Rate\", \"Memory (MB)\"]\n        metrics_table.append(\" | \".join(f\"{h:>12}\" for h in headers))\n        metrics_table.append(\"-\" * 65)\n        \n        for algorithm, result in sorted_results:\n            row = [\n                algorithm.value[:12],\n                f\"{result.computation_time:.3f}\",\n                f\"{result.iterations}\",\n                f\"{result.convergence_rate:.3f}\",\n                f\"{result.memory_usage:.1f}\"\n            ]\n            metrics_table.append(\" | \".join(f\"{v:>12}\" for v in row))\n        \n        report_lines.extend(metrics_table)\n        report_lines.extend([\n            \"\",\n            \"=\" * 60,\n            \"Report generated by Terragon Labs Advanced Algorithm Suite\",\n            \"=\" * 60\n        ])\n        \n        return \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/optimization/memory_optimizer.py",
      "line_number": 1,
      "details": "\"\"\"Memory optimization and caching for analog PDE solver.\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, Optional, Tuple, List\nimport threading\nimport weakref\nimport gc\nimport sys\nfrom functools import lru_cache\nfrom ..utils.logger import get_logger\n\n\nclass MemoryPool:\n    \"\"\"Memory pool for reusing NumPy arrays.\"\"\"\n    \n    def __init__(self, max_size_mb: int = 512):\n        \"\"\"Initialize memory pool.\n        \n        Args:\n            max_size_mb: Maximum pool size in MB\n        \"\"\"\n        self.logger = get_logger('memory_pool')\n        self.max_size_bytes = max_size_mb * 1024 * 1024\n        self.current_size = 0\n        self._pools = {}  # shape -> list of arrays\n        self._lock = threading.Lock()\n        \n        self.logger.info(f\"Initialized memory pool with {max_size_mb} MB limit\")\n    \n    def get_array(self, shape: Tuple[int, ...], dtype: np.dtype = np.float64) -> np.ndarray:\n        \"\"\"Get array from pool or allocate new one.\n        \n        Args:\n            shape: Array shape\n            dtype: Array data type\n            \n        Returns:\n            NumPy array\n        \"\"\"\n        key = (shape, dtype)\n        \n        with self._lock:\n            if key in self._pools and self._pools[key]:\n                array = self._pools[key].pop()\n                array.fill(0)  # Clear previous data\n                return array\n        \n        # Allocate new array\n        return np.zeros(shape, dtype=dtype)\n    \n    def return_array(self, array: np.ndarray) -> None:\n        \"\"\"Return array to pool.\n        \n        Args:\n            array: Array to return\n        \"\"\"\n        if array is None:\n            return\n            \n        key = (array.shape, array.dtype)\n        array_size = array.nbytes\n        \n        with self._lock:\n            # Check if we have space\n            if self.current_size + array_size <= self.max_size_bytes:\n                if key not in self._pools:\n                    self._pools[key] = []\n                \n                self._pools[key].append(array)\n                self.current_size += array_size\n            else:\n                # Pool is full, let array be garbage collected\n                pass\n    \n    def clear(self) -> None:\n        \"\"\"Clear the memory pool.\"\"\"\n        with self._lock:\n            self._pools.clear()\n            self.current_size = 0\n            gc.collect()\n        \n        self.logger.info(\"Memory pool cleared\")\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get pool statistics.\"\"\"\n        with self._lock:\n            total_arrays = sum(len(arrays) for arrays in self._pools.values())\n            return {\n                'current_size_mb': self.current_size / (1024 * 1024),\n                'max_size_mb': self.max_size_bytes / (1024 * 1024),\n                'pool_count': len(self._pools),\n                'total_arrays': total_arrays,\n                'utilization': self.current_size / self.max_size_bytes\n            }\n\n\nclass ConductanceCache:\n    \"\"\"Cache for conductance matrices to avoid recomputation.\"\"\"\n    \n    def __init__(self, max_entries: int = 100):\n        \"\"\"Initialize conductance cache.\n        \n        Args:\n            max_entries: Maximum cache entries\n        \"\"\"\n        self.logger = get_logger('conductance_cache')\n        self.max_entries = max_entries\n        self._cache = {}\n        self._access_order = []\n        self._lock = threading.Lock()\n        \n        self.logger.info(f\"Initialized conductance cache with {max_entries} max entries\")\n    \n    def _make_key(\n        self, \n        matrix_size: int, \n        conductance_range: Tuple[float, float],\n        pde_type: str\n    ) -> str:\n        \"\"\"Create cache key.\"\"\"\n        return f\"{matrix_size}_{conductance_range[0]}_{conductance_range[1]}_{pde_type}\"\n    \n    def get(\n        self, \n        matrix_size: int, \n        conductance_range: Tuple[float, float],\n        pde_type: str\n    ) -> Optional[Tuple[np.ndarray, np.ndarray]]:\n        \"\"\"Get cached conductance matrices.\n        \n        Args:\n            matrix_size: Size of the matrix\n            conductance_range: Min/max conductance values\n            pde_type: Type of PDE\n            \n        Returns:\n            Tuple of (g_positive, g_negative) or None if not cached\n        \"\"\"\n        key = self._make_key(matrix_size, conductance_range, pde_type)\n        \n        with self._lock:\n            if key in self._cache:\n                # Move to end (most recently used)\n                self._access_order.remove(key)\n                self._access_order.append(key)\n                \n                g_pos, g_neg = self._cache[key]\n                return g_pos.copy(), g_neg.copy()\n        \n        return None\n    \n    def put(\n        self, \n        matrix_size: int, \n        conductance_range: Tuple[float, float],\n        pde_type: str,\n        g_positive: np.ndarray,\n        g_negative: np.ndarray\n    ) -> None:\n        \"\"\"Store conductance matrices in cache.\n        \n        Args:\n            matrix_size: Size of the matrix\n            conductance_range: Min/max conductance values\n            pde_type: Type of PDE\n            g_positive: Positive conductance matrix\n            g_negative: Negative conductance matrix\n        \"\"\"\n        key = self._make_key(matrix_size, conductance_range, pde_type)\n        \n        with self._lock:\n            # Remove oldest if at capacity\n            if len(self._cache) >= self.max_entries and key not in self._cache:\n                oldest_key = self._access_order.pop(0)\n                del self._cache[oldest_key]\n            \n            # Store copies to avoid external modifications\n            self._cache[key] = (g_positive.copy(), g_negative.copy())\n            \n            # Update access order\n            if key in self._access_order:\n                self._access_order.remove(key)\n            self._access_order.append(key)\n    \n    def clear(self) -> None:\n        \"\"\"Clear the cache.\"\"\"\n        with self._lock:\n            self._cache.clear()\n            self._access_order.clear()\n        \n        self.logger.info(\"Conductance cache cleared\")\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n        with self._lock:\n            cache_size_mb = sum(\n                (g_pos.nbytes + g_neg.nbytes) \n                for g_pos, g_neg in self._cache.values()\n            ) / (1024 * 1024)\n            \n            return {\n                'entries': len(self._cache),\n                'max_entries': self.max_entries,\n                'cache_size_mb': cache_size_mb,\n                'hit_rate': getattr(self, '_hit_rate', 0.0),\n                'utilization': len(self._cache) / self.max_entries\n            }\n\n\nclass MemoryOptimizedSolver:\n    \"\"\"Memory-optimized analog PDE solver.\"\"\"\n    \n    def __init__(\n        self,\n        crossbar_size: int = 128,\n        conductance_range: tuple = (1e-9, 1e-6),\n        noise_model: str = \"realistic\",\n        memory_pool_size_mb: int = 256,\n        enable_caching: bool = True\n    ):\n        \"\"\"Initialize memory-optimized solver.\n        \n        Args:\n            crossbar_size: Size of crossbar array\n            conductance_range: Min/max conductance values\n            noise_model: Noise modeling approach\n            memory_pool_size_mb: Memory pool size in MB\n            enable_caching: Enable conductance caching\n        \"\"\"\n        self.logger = get_logger('memory_optimized_solver')\n        \n        self.crossbar_size = crossbar_size\n        self.conductance_range = conductance_range\n        self.noise_model = noise_model\n        \n        # Memory management\n        self.memory_pool = MemoryPool(memory_pool_size_mb)\n        self.conductance_cache = ConductanceCache() if enable_caching else None\n        \n        # Statistics\n        self._solve_count = 0\n        self._cache_hits = 0\n        self._cache_misses = 0\n        \n        self.logger.info(\n            f\"Initialized MemoryOptimizedSolver with {memory_pool_size_mb}MB pool, \"\n            f\"caching {'enabled' if enable_caching else 'disabled'}\"\n        )\n    \n    def solve(\n        self,\n        pde,\n        iterations: int = 100,\n        convergence_threshold: float = 1e-6,\n        in_place: bool = True\n    ) -> np.ndarray:\n        \"\"\"Solve PDE with memory optimization.\n        \n        Args:\n            pde: PDE object to solve\n            iterations: Maximum iterations\n            convergence_threshold: Convergence threshold\n            in_place: Use in-place operations where possible\n            \n        Returns:\n            Solution array\n        \"\"\"\n        self._solve_count += 1\n        pde_type = type(pde).__name__\n        \n        # Try to get conductances from cache\n        cached_conductances = None\n        if self.conductance_cache:\n            cached_conductances = self.conductance_cache.get(\n                self.crossbar_size, self.conductance_range, pde_type\n            )\n            \n            if cached_conductances:\n                self._cache_hits += 1\n                self.logger.debug(\"Using cached conductance matrices\")\n            else:\n                self._cache_misses += 1\n        \n        # Get working arrays from memory pool\n        phi = self.memory_pool.get_array((self.crossbar_size,))\n        phi_new = self.memory_pool.get_array((self.crossbar_size,))\n        source = self.memory_pool.get_array((self.crossbar_size,))\n        residual = self.memory_pool.get_array((self.crossbar_size,))\n        \n        try:\n            # Initialize arrays\n            np.random.seed(42)  # For reproducible results\n            phi[:] = np.random.random(self.crossbar_size) * 0.1\n            \n            # Create source term\n            self._create_source_term(pde, source)\n            \n            # Get or compute conductances\n            if cached_conductances:\n                g_positive, g_negative = cached_conductances\n            else:\n                g_positive, g_negative = self._compute_conductances(pde)\n                \n                # Cache for future use\n                if self.conductance_cache:\n                    self.conductance_cache.put(\n                        self.crossbar_size, self.conductance_range, pde_type,\n                        g_positive, g_negative\n                    )\n            \n            # Iterative solver with memory optimization\n            for i in range(iterations):\n                # Compute residual using cached matrices\n                self._compute_residual_optimized(\n                    phi, source, residual, g_positive, g_negative\n                )\n                \n                # Update solution (in-place if requested)\n                if in_place:\n                    phi -= 0.1 * residual\n                    phi_new, phi = phi, phi_new  # Swap references\n                else:\n                    np.subtract(phi, 0.1 * residual, out=phi_new)\n                    phi, phi_new = phi_new, phi  # Swap for next iteration\n                \n                # Apply boundary conditions\n                phi[0] = 0.0\n                phi[-1] = 0.0\n                \n                # Check convergence\n                error = np.linalg.norm(residual)\n                if error < convergence_threshold:\n                    self.logger.debug(f\"Converged after {i+1} iterations\")\n                    break\n            \n            # Create result array (not from pool, as it's returned)\n            result = phi.copy()\n            \n        finally:\n            # Return arrays to pool\n            self.memory_pool.return_array(phi)\n            self.memory_pool.return_array(phi_new)\n            self.memory_pool.return_array(source)\n            self.memory_pool.return_array(residual)\n        \n        return result\n    \n    def _create_source_term(self, pde, source: np.ndarray) -> None:\n        \"\"\"Create source term efficiently.\"\"\"\n        if hasattr(pde, 'source_function') and pde.source_function:\n            x = np.linspace(0, 1, self.crossbar_size)\n            for i, xi in enumerate(x):\n                source[i] = pde.source_function(xi, 0)\n        else:\n            source.fill(0.1)\n    \n    def _compute_conductances(self, pde) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Compute conductance matrices.\"\"\"\n        # Create Laplacian matrix\n        laplacian = self._create_laplacian_matrix_optimized()\n        \n        # Decompose into positive and negative components\n        g_min, g_max = self.conductance_range\n        \n        pos_matrix = np.maximum(laplacian, 0)\n        neg_matrix = np.maximum(-laplacian, 0)\n        \n        # Scale to conductance range\n        g_positive = self._scale_to_conductance(pos_matrix, g_min, g_max)\n        g_negative = self._scale_to_conductance(neg_matrix, g_min, g_max)\n        \n        return g_positive, g_negative\n    \n    def _create_laplacian_matrix_optimized(self) -> np.ndarray:\n        \"\"\"Create Laplacian matrix with memory optimization.\"\"\"\n        size = self.crossbar_size\n        \n        # Use sparse-like approach for memory efficiency\n        laplacian = np.zeros((size, size), dtype=np.float32)  # Use float32 to save memory\n        \n        # Main diagonal\n        np.fill_diagonal(laplacian, -2.0)\n        \n        # Off-diagonals (vectorized)\n        if size > 1:\n            laplacian[np.arange(size-1), np.arange(1, size)] = 1.0\n            laplacian[np.arange(1, size), np.arange(size-1)] = 1.0\n        \n        return laplacian.astype(np.float64)  # Convert back if needed\n    \n    def _scale_to_conductance(\n        self, \n        matrix: np.ndarray, \n        g_min: float, \n        g_max: float\n    ) -> np.ndarray:\n        \"\"\"Scale matrix to conductance range efficiently.\"\"\"\n        if matrix.max() == 0:\n            return np.full_like(matrix, g_min)\n        \n        # Avoid unnecessary copies\n        scaled = matrix / matrix.max()  # Normalize\n        scaled *= (g_max - g_min)       # Scale to range\n        scaled += g_min                 # Shift to minimum\n        \n        return scaled\n    \n    def _compute_residual_optimized(\n        self,\n        phi: np.ndarray,\n        source: np.ndarray,\n        residual: np.ndarray,\n        g_positive: np.ndarray,\n        g_negative: np.ndarray\n    ) -> None:\n        \"\"\"Compute residual with optimized memory usage.\"\"\"\n        # Analog vector-matrix multiplication (simplified)\n        # In practice, this would use the crossbar compute_vmm method\n        \n        # Positive contribution\n        np.dot(g_positive.T, phi, out=residual)\n        \n        # Subtract negative contribution (in-place)\n        temp = np.dot(g_negative.T, phi)\n        residual -= temp\n        \n        # Add source term\n        residual += source\n    \n    def get_memory_stats(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive memory statistics.\"\"\"\n        pool_stats = self.memory_pool.get_stats()\n        \n        stats = {\n            'memory_pool': pool_stats,\n            'solve_count': self._solve_count,\n            'system_memory': self._get_system_memory_info()\n        }\n        \n        if self.conductance_cache:\n            cache_stats = self.conductance_cache.get_stats()\n            cache_stats['hit_rate'] = (\n                self._cache_hits / (self._cache_hits + self._cache_misses)\n                if (self._cache_hits + self._cache_misses) > 0 else 0.0\n            )\n            stats['conductance_cache'] = cache_stats\n        \n        return stats\n    \n    def _get_system_memory_info(self) -> Dict[str, Any]:\n        \"\"\"Get system memory information.\"\"\"\n        try:\n            import psutil\n            mem = psutil.virtual_memory()\n            return {\n                'total_gb': mem.total / (1024**3),\n                'available_gb': mem.available / (1024**3),\n                'used_percent': mem.percent,\n                'python_process_mb': psutil.Process().memory_info().rss / (1024**2)\n            }\n        except ImportError:\n            return {'available': False, 'reason': 'psutil not installed'}\n    \n    def cleanup(self) -> None:\n        \"\"\"Clean up memory resources.\"\"\"\n        self.memory_pool.clear()\n        if self.conductance_cache:\n            self.conductance_cache.clear()\n        gc.collect()\n        \n        self.logger.info(\"Memory cleanup completed\")"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/research/adaptive_solvers.py",
      "line_number": 1,
      "details": "\"\"\"Research-grade adaptive mesh refinement and multigrid solvers.\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom ..core.solver import AnalogPDESolver\nfrom ..utils.logger import get_logger, PerformanceLogger\n\n\n@dataclass\nclass MeshCell:\n    \"\"\"Represents a cell in an adaptive mesh.\"\"\"\n    level: int\n    x_start: float\n    x_end: float\n    y_start: float\n    y_end: float\n    solution: Optional[np.ndarray] = None\n    error_estimate: float = 0.0\n    needs_refinement: bool = False\n    \n    @property\n    def center(self) -> Tuple[float, float]:\n        \"\"\"Get cell center coordinates.\"\"\"\n        return ((self.x_start + self.x_end) / 2, (self.y_start + self.y_end) / 2)\n    \n    @property\n    def size(self) -> Tuple[float, float]:\n        \"\"\"Get cell dimensions.\"\"\"\n        return (self.x_end - self.x_start, self.y_end - self.y_start)\n\n\nclass AdaptiveMeshRefiner:\n    \"\"\"Adaptive mesh refinement for analog PDE solvers.\"\"\"\n    \n    def __init__(\n        self,\n        domain_bounds: Tuple[Tuple[float, float], Tuple[float, float]] = ((0, 1), (0, 1)),\n        max_refinement_levels: int = 4,\n        refinement_threshold: float = 1e-3,\n        coarsening_threshold: float = 1e-5\n    ):\n        \"\"\"Initialize adaptive mesh refiner.\n        \n        Args:\n            domain_bounds: ((x_min, x_max), (y_min, y_max))\n            max_refinement_levels: Maximum refinement levels\n            refinement_threshold: Error threshold for refinement\n            coarsening_threshold: Error threshold for coarsening\n        \"\"\"\n        self.logger = get_logger('adaptive_mesh')\n        self.perf_logger = PerformanceLogger(self.logger)\n        \n        self.domain_bounds = domain_bounds\n        self.max_refinement_levels = max_refinement_levels\n        self.refinement_threshold = refinement_threshold\n        self.coarsening_threshold = coarsening_threshold\n        \n        # Initialize with single coarse cell\n        (x_min, x_max), (y_min, y_max) = domain_bounds\n        self.mesh_cells = [\n            MeshCell(level=0, x_start=x_min, x_end=x_max, y_start=y_min, y_end=y_max)\n        ]\n        \n        self.logger.info(f\"Initialized adaptive mesh with {max_refinement_levels} max levels\")\n    \n    def refine_mesh(self, error_estimates: Dict[int, float]) -> List[MeshCell]:\n        \"\"\"Refine mesh based on error estimates.\n        \n        Args:\n            error_estimates: Error estimates for each cell (by index)\n            \n        Returns:\n            Updated list of mesh cells\n        \"\"\"\n        self.perf_logger.start_timer('mesh_refinement')\n        \n        new_cells = []\n        refinement_count = 0\n        \n        for i, cell in enumerate(self.mesh_cells):\n            error = error_estimates.get(i, 0.0)\n            cell.error_estimate = error\n            \n            # Check if cell needs refinement\n            if (error > self.refinement_threshold and \n                cell.level < self.max_refinement_levels):\n                \n                # Subdivide cell into 4 subcells (2D)\n                subcells = self._subdivide_cell(cell)\n                new_cells.extend(subcells)\n                refinement_count += 1\n                \n            else:\n                new_cells.append(cell)\n        \n        self.mesh_cells = new_cells\n        \n        refinement_time = self.perf_logger.end_timer('mesh_refinement')\n        \n        self.logger.info(\n            f\"Refined mesh: {refinement_count} cells subdivided, \"\n            f\"{len(self.mesh_cells)} total cells\"\n        )\n        \n        return self.mesh_cells\n    \n    def _subdivide_cell(self, cell: MeshCell) -> List[MeshCell]:\n        \"\"\"Subdivide a cell into 4 subcells.\"\"\"\n        x_mid = (cell.x_start + cell.x_end) / 2\n        y_mid = (cell.y_start + cell.y_end) / 2\n        new_level = cell.level + 1\n        \n        subcells = [\n            # Bottom-left\n            MeshCell(new_level, cell.x_start, x_mid, cell.y_start, y_mid),\n            # Bottom-right\n            MeshCell(new_level, x_mid, cell.x_end, cell.y_start, y_mid),\n            # Top-left\n            MeshCell(new_level, cell.x_start, x_mid, y_mid, cell.y_end),\n            # Top-right\n            MeshCell(new_level, x_mid, cell.x_end, y_mid, cell.y_end)\n        ]\n        \n        return subcells\n    \n    def coarsen_mesh(self) -> List[MeshCell]:\n        \"\"\"Coarsen mesh by combining cells with low error.\"\"\"\n        # Group cells by parent and check if all siblings can be coarsened\n        coarsened_cells = []\n        processed_indices = set()\n        coarsening_count = 0\n        \n        for i, cell in enumerate(self.mesh_cells):\n            if i in processed_indices:\n                continue\n                \n            if cell.level > 0 and cell.error_estimate < self.coarsening_threshold:\n                # Try to find siblings for coarsening\n                siblings = self._find_siblings(i, cell)\n                \n                if len(siblings) == 4:  # All siblings found\n                    # Check if all siblings can be coarsened\n                    can_coarsen = all(\n                        self.mesh_cells[j].error_estimate < self.coarsening_threshold\n                        for j in siblings\n                    )\n                    \n                    if can_coarsen:\n                        # Create parent cell\n                        parent_cell = self._create_parent_cell(siblings)\n                        coarsened_cells.append(parent_cell)\n                        \n                        # Mark siblings as processed\n                        processed_indices.update(siblings)\n                        coarsening_count += 1\n                        continue\n            \n            if i not in processed_indices:\n                coarsened_cells.append(cell)\n                processed_indices.add(i)\n        \n        if coarsening_count > 0:\n            self.mesh_cells = coarsened_cells\n            self.logger.info(f\"Coarsened mesh: {coarsening_count} parent cells created\")\n        \n        return self.mesh_cells\n    \n    def _find_siblings(self, cell_index: int, cell: MeshCell) -> List[int]:\n        \"\"\"Find sibling cells that share the same parent.\"\"\"\n        siblings = []\n        \n        # Calculate parent bounds\n        parent_size_x = (cell.x_end - cell.x_start) * 2\n        parent_size_y = (cell.y_end - cell.y_start) * 2\n        \n        # Find potential parent origin\n        parent_x = cell.x_start - (cell.x_start % parent_size_x)\n        parent_y = cell.y_start - (cell.y_start % parent_size_y)\n        \n        # Look for cells that would be siblings\n        for j, other_cell in enumerate(self.mesh_cells):\n            if (other_cell.level == cell.level and\n                other_cell.x_start >= parent_x and \n                other_cell.x_end <= parent_x + parent_size_x and\n                other_cell.y_start >= parent_y and\n                other_cell.y_end <= parent_y + parent_size_y):\n                siblings.append(j)\n        \n        return siblings\n    \n    def _create_parent_cell(self, sibling_indices: List[int]) -> MeshCell:\n        \"\"\"Create parent cell from siblings.\"\"\"\n        siblings = [self.mesh_cells[i] for i in sibling_indices]\n        \n        # Find bounds\n        x_min = min(cell.x_start for cell in siblings)\n        x_max = max(cell.x_end for cell in siblings)\n        y_min = min(cell.y_start for cell in siblings)\n        y_max = max(cell.y_end for cell in siblings)\n        \n        # Average error estimates\n        avg_error = np.mean([cell.error_estimate for cell in siblings])\n        \n        parent = MeshCell(\n            level=siblings[0].level - 1,\n            x_start=x_min, x_end=x_max,\n            y_start=y_min, y_end=y_max\n        )\n        parent.error_estimate = avg_error\n        \n        return parent\n    \n    def get_mesh_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get mesh statistics.\"\"\"\n        level_counts = {}\n        for cell in self.mesh_cells:\n            level_counts[cell.level] = level_counts.get(cell.level, 0) + 1\n        \n        total_cells = len(self.mesh_cells)\n        avg_error = np.mean([cell.error_estimate for cell in self.mesh_cells])\n        max_error = max(cell.error_estimate for cell in self.mesh_cells)\n        \n        return {\n            'total_cells': total_cells,\n            'level_distribution': level_counts,\n            'average_error': avg_error,\n            'maximum_error': max_error,\n            'refinement_efficiency': 1.0 - (total_cells / (4 ** self.max_refinement_levels))\n        }\n\n\nclass MultigridAnalogSolver:\n    \"\"\"Multigrid solver using analog crossbar arrays at different scales.\"\"\"\n    \n    def __init__(\n        self,\n        base_size: int = 128,\n        num_levels: int = 4,\n        conductance_range: tuple = (1e-9, 1e-6),\n        noise_model: str = \"realistic\"\n    ):\n        \"\"\"Initialize multigrid solver.\n        \n        Args:\n            base_size: Size of finest grid\n            num_levels: Number of multigrid levels\n            conductance_range: Conductance range for crossbars\n            noise_model: Noise model for crossbars\n        \"\"\"\n        self.logger = get_logger('multigrid_solver')\n        self.perf_logger = PerformanceLogger(self.logger)\n        \n        self.base_size = base_size\n        self.num_levels = num_levels\n        self.conductance_range = conductance_range\n        self.noise_model = noise_model\n        \n        # Create solvers for each level\n        self.level_solvers = {}\n        for level in range(num_levels):\n            level_size = max(4, base_size // (2 ** level))\n            self.level_solvers[level] = AnalogPDESolver(\n                crossbar_size=level_size,\n                conductance_range=conductance_range,\n                noise_model=noise_model\n            )\n        \n        self.logger.info(\n            f\"Initialized multigrid solver with {num_levels} levels, \"\n            f\"finest grid: {base_size}\u00d7{base_size}\"\n        )\n    \n    def solve_v_cycle(\n        self,\n        pde,\n        num_v_cycles: int = 3,\n        pre_smooth_iterations: int = 2,\n        post_smooth_iterations: int = 2,\n        coarse_solve_iterations: int = 10\n    ) -> np.ndarray:\n        \"\"\"Solve PDE using V-cycle multigrid method.\n        \n        Args:\n            pde: PDE to solve\n            num_v_cycles: Number of V-cycles\n            pre_smooth_iterations: Pre-smoothing iterations\n            post_smooth_iterations: Post-smoothing iterations\n            coarse_solve_iterations: Coarse grid solve iterations\n            \n        Returns:\n            Solution on finest grid\n        \"\"\"\n        self.perf_logger.start_timer('multigrid_v_cycle')\n        \n        # Initialize solution on finest grid\n        finest_size = self.level_solvers[0].crossbar_size\n        solution = np.random.random(finest_size) * 0.1\n        \n        self.logger.info(f\"Starting {num_v_cycles} V-cycles\")\n        \n        for cycle in range(num_v_cycles):\n            self.logger.debug(f\"V-cycle {cycle + 1}/{num_v_cycles}\")\n            \n            # Store solutions at each level\n            solutions = {0: solution.copy()}\n            residuals = {}\n            \n            # Downward sweep (restriction)\n            for level in range(self.num_levels - 1):\n                current_solution = solutions[level]\n                \n                # Pre-smoothing\n                current_solution = self._smooth_level(\n                    level, pde, current_solution, pre_smooth_iterations\n                )\n                solutions[level] = current_solution\n                \n                # Compute residual\n                residual = self._compute_residual(level, pde, current_solution)\n                residuals[level] = residual\n                \n                # Restrict to coarser grid\n                coarse_residual = self._restrict(residual)\n                \n                # Initialize coarse grid correction\n                coarse_size = self.level_solvers[level + 1].crossbar_size\n                coarse_correction = np.zeros(coarse_size)\n                solutions[level + 1] = coarse_correction\n            \n            # Solve on coarsest grid\n            coarsest_level = self.num_levels - 1\n            coarsest_residual = residuals.get(coarsest_level - 1, \n                                            np.ones(self.level_solvers[coarsest_level].crossbar_size) * 0.1)\n            \n            if len(coarsest_residual) != self.level_solvers[coarsest_level].crossbar_size:\n                coarsest_residual = self._restrict(coarsest_residual)\n            \n            solutions[coarsest_level] = self._solve_coarse_grid(\n                coarsest_level, pde, coarsest_residual, coarse_solve_iterations\n            )\n            \n            # Upward sweep (prolongation)\n            for level in range(self.num_levels - 2, -1, -1):\n                # Prolongate correction from coarser grid\n                coarse_correction = solutions[level + 1]\n                fine_correction = self._prolongate(coarse_correction, level)\n                \n                # Add correction\n                solutions[level] += fine_correction\n                \n                # Post-smoothing\n                solutions[level] = self._smooth_level(\n                    level, pde, solutions[level], post_smooth_iterations\n                )\n            \n            solution = solutions[0]\n            \n            # Check convergence\n            residual_norm = np.linalg.norm(self._compute_residual(0, pde, solution))\n            self.logger.debug(f\"V-cycle {cycle + 1} residual norm: {residual_norm:.2e}\")\n        \n        multigrid_time = self.perf_logger.end_timer('multigrid_v_cycle')\n        \n        self.logger.info(f\"Multigrid solve completed in {multigrid_time:.3f}s\")\n        \n        return solution\n    \n    def _smooth_level(\n        self,\n        level: int,\n        pde,\n        solution: np.ndarray,\n        iterations: int\n    ) -> np.ndarray:\n        \"\"\"Smooth solution at given level.\"\"\"\n        solver = self.level_solvers[level]\n        \n        # Create PDE at this level size\n        level_size = solver.crossbar_size\n        if hasattr(pde, 'domain_size'):\n            # Create scaled version of PDE\n            from ..core.equations import PoissonEquation\n            level_pde = PoissonEquation((level_size,))\n        else:\n            level_pde = pde\n        \n        # Smooth using analog solver\n        try:\n            smoothed = solver.solve(level_pde, iterations=iterations, \n                                  convergence_threshold=1e-6)\n            return smoothed\n        except Exception as e:\n            self.logger.warning(f\"Smoothing failed at level {level}: {e}\")\n            return solution\n    \n    def _compute_residual(self, level: int, pde, solution: np.ndarray) -> np.ndarray:\n        \"\"\"Compute residual at given level.\"\"\"\n        # Simplified residual computation\n        # In practice, this would use the actual PDE operator\n        solver = self.level_solvers[level]\n        \n        try:\n            # Use crossbar to compute matrix-vector product\n            residual = solver.crossbar.compute_vmm(solution)\n            \n            # Add source term (simplified)\n            if len(residual) > 0:\n                residual += 0.1 * np.ones_like(residual)\n            \n            return residual\n            \n        except Exception as e:\n            self.logger.warning(f\"Residual computation failed at level {level}: {e}\")\n            return np.zeros_like(solution)\n    \n    def _restrict(self, fine_array: np.ndarray) -> np.ndarray:\n        \"\"\"Restrict fine grid array to coarse grid.\"\"\"\n        fine_size = len(fine_array)\n        coarse_size = fine_size // 2\n        \n        if coarse_size < 2:\n            return np.array([np.mean(fine_array)])\n        \n        # Simple injection restriction\n        coarse_array = np.zeros(coarse_size)\n        for i in range(coarse_size):\n            # Average two fine grid points\n            if 2*i + 1 < fine_size:\n                coarse_array[i] = 0.5 * (fine_array[2*i] + fine_array[2*i + 1])\n            else:\n                coarse_array[i] = fine_array[2*i]\n        \n        return coarse_array\n    \n    def _prolongate(self, coarse_array: np.ndarray, target_level: int) -> np.ndarray:\n        \"\"\"Prolongate coarse grid array to fine grid.\"\"\"\n        coarse_size = len(coarse_array)\n        fine_size = self.level_solvers[target_level].crossbar_size\n        \n        if fine_size <= coarse_size:\n            return coarse_array[:fine_size]\n        \n        # Linear interpolation\n        fine_array = np.zeros(fine_size)\n        scale_factor = coarse_size / fine_size\n        \n        for i in range(fine_size):\n            coarse_index = i * scale_factor\n            left_index = int(np.floor(coarse_index))\n            right_index = min(left_index + 1, coarse_size - 1)\n            \n            if left_index == right_index:\n                fine_array[i] = coarse_array[left_index]\n            else:\n                weight = coarse_index - left_index\n                fine_array[i] = ((1 - weight) * coarse_array[left_index] + \n                               weight * coarse_array[right_index])\n        \n        return fine_array\n    \n    def _solve_coarse_grid(\n        self,\n        level: int,\n        pde,\n        rhs: np.ndarray,\n        iterations: int\n    ) -> np.ndarray:\n        \"\"\"Solve on coarsest grid.\"\"\"\n        solver = self.level_solvers[level]\n        \n        # Create appropriate PDE for this level\n        level_size = solver.crossbar_size\n        from ..core.equations import PoissonEquation\n        level_pde = PoissonEquation((level_size,))\n        \n        try:\n            return solver.solve(level_pde, iterations=iterations)\n        except Exception as e:\n            self.logger.warning(f\"Coarse grid solve failed: {e}\")\n            return np.zeros(level_size)\n    \n    def get_multigrid_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get multigrid solver statistics.\"\"\"\n        level_info = {}\n        \n        for level, solver in self.level_solvers.items():\n            level_info[level] = {\n                'grid_size': solver.crossbar_size,\n                'conductance_range': solver.conductance_range,\n                'noise_model': solver.noise_model\n            }\n        \n        return {\n            'num_levels': self.num_levels,\n            'base_size': self.base_size,\n            'level_info': level_info,\n            'memory_complexity': sum(solver.crossbar_size**2 for solver in self.level_solvers.values()),\n            'computational_complexity': self.base_size**2 * (4/3)  # Theoretical V-cycle complexity\n        }\n\n\nclass ErrorEstimator:\n    \"\"\"Error estimator for adaptive refinement.\"\"\"\n    \n    def __init__(self, method: str = 'gradient'):\n        \"\"\"Initialize error estimator.\"\"\"\n        self.method = method\n        \n    def estimate_error(self, solution: np.ndarray) -> np.ndarray:\n        \"\"\"Estimate local errors.\"\"\"\n        if self.method == 'gradient':\n            return np.abs(np.gradient(solution))\n        else:\n            return np.abs(solution - np.mean(solution))\n\n\nclass MeshRefinement:\n    \"\"\"Mesh refinement system.\"\"\"\n    \n    def __init__(self, initial_size: int = 64):\n        \"\"\"Initialize mesh refinement.\"\"\"\n        self.initial_size = initial_size\n        \n    def refine_mesh(self, error_indicators: np.ndarray) -> np.ndarray:\n        \"\"\"Refine mesh based on error indicators.\"\"\"\n        return error_indicators  # Simplified\n\n\nclass AdaptivePDESolver:\n    \"\"\"Adaptive PDE solver combining error estimation and mesh refinement.\"\"\"\n    \n    def __init__(\n        self,\n        base_solver: AnalogPDESolver,\n        error_estimator: ErrorEstimator = None,\n        mesh_refiner: AdaptiveMeshRefiner = None\n    ):\n        \"\"\"Initialize adaptive PDE solver.\n        \n        Args:\n            base_solver: Base analog PDE solver\n            error_estimator: Error estimator for adaptivity\n            mesh_refiner: Mesh refinement system\n        \"\"\"\n        self.logger = get_logger('adaptive_pde_solver')\n        \n        self.base_solver = base_solver\n        self.error_estimator = error_estimator or ErrorEstimator()\n        self.mesh_refiner = mesh_refiner or AdaptiveMeshRefiner()\n        \n        self.logger.info(\"Initialized adaptive PDE solver\")\n    \n    def solve_adaptive(\n        self,\n        pde,\n        max_iterations: int = 10,\n        tolerance: float = 1e-6\n    ) -> np.ndarray:\n        \"\"\"Solve PDE with adaptive refinement.\n        \n        Args:\n            pde: PDE to solve\n            max_iterations: Maximum adaptive iterations\n            tolerance: Convergence tolerance\n            \n        Returns:\n            Adaptive solution\n        \"\"\"\n        self.logger.info(f\"Starting adaptive solve with {max_iterations} max iterations\")\n        \n        # Initial solve\n        solution = self.base_solver.solve(pde)\n        \n        for iteration in range(max_iterations):\n            # Estimate errors\n            errors = self.error_estimator.estimate_error(solution)\n            \n            # Check convergence\n            max_error = np.max(np.abs(errors))\n            if max_error < tolerance:\n                self.logger.info(f\"Converged after {iteration + 1} iterations\")\n                break\n                \n            # Refine mesh\n            error_dict = {i: errors[i] for i in range(len(errors)) if i < len(errors)}\n            self.mesh_refiner.refine_mesh(error_dict)\n            \n            # Re-solve\n            solution = self.base_solver.solve(pde)\n            \n        return solution\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get adaptive solver statistics.\"\"\"\n        return {\n            'base_solver_size': self.base_solver.crossbar_size,\n            'error_estimation_method': self.error_estimator.method,\n            'mesh_statistics': self.mesh_refiner.get_mesh_statistics()\n        }"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/research/bioneuro_olfactory_fusion.py",
      "line_number": 1,
      "details": "\"\"\"\nBio-Neuromorphic Olfactory Fusion Engine\n\nAdvanced analog computing framework that merges biological olfactory processing\nwith neuromorphic analog circuits for next-generation PDE solving with chemical\ngradient detection and bio-inspired optimization.\n\nResearch Innovation: Combines mammalian olfactory bulb neural dynamics with\nanalog crossbar arrays for ultra-efficient chemical gradient PDE solving.\n\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, List, Tuple, Optional, Any\nfrom dataclasses import dataclass\nfrom scipy.signal import convolve2d\nfrom ..core.solver import AnalogPDESolver\nfrom ..core.equations import PoissonEquation\n\n@dataclass\nclass OlfactoryReceptorConfig:\n    \"\"\"Configuration for bio-inspired olfactory receptor arrays.\"\"\"\n    num_receptors: int = 256\n    sensitivity_range: Tuple[float, float] = (1e-12, 1e-6)  # molar concentration\n    response_time: float = 0.001  # seconds\n    adaptation_rate: float = 0.1\n    noise_level: float = 0.02\n\n@dataclass\nclass MitralCellNetwork:\n    \"\"\"Mitral cell lateral inhibition network for contrast enhancement.\"\"\"\n    num_cells: int = 64\n    inhibition_radius: float = 3.0\n    inhibition_strength: float = 0.5\n    temporal_dynamics: bool = True\n    oscillation_frequency: float = 40.0  # Hz (gamma rhythm)\n\nclass BioneuroOlfactoryFusion:\n    \"\"\"\n    Bio-neuromorphic olfactory fusion engine combining:\n    1. Olfactory receptor analog arrays\n    2. Mitral cell lateral inhibition networks  \n    3. Glomerular processing layers\n    4. Chemical gradient PDE solving\n    \"\"\"\n    \n    def __init__(\n        self,\n        receptor_config: OlfactoryReceptorConfig = None,\n        mitral_config: MitralCellNetwork = None,\n        crossbar_size: int = 128\n    ):\n        \"\"\"Initialize bio-neuromorphic olfactory fusion system.\"\"\"\n        self.logger = logging.getLogger(__name__)\n        \n        # Configuration defaults\n        self.receptor_config = receptor_config or OlfactoryReceptorConfig()\n        self.mitral_config = mitral_config or MitralCellNetwork()\n        \n        # Initialize analog PDE solver\n        self.pde_solver = AnalogPDESolver(\n            crossbar_size=crossbar_size,\n            conductance_range=(1e-9, 1e-6),\n            noise_model=\"realistic\"\n        )\n        \n        # Initialize olfactory processing layers\n        self._initialize_olfactory_layers()\n        \n        self.logger.info(f\"Initialized BioneuroOlfactoryFusion with {self.receptor_config.num_receptors} receptors\")\n    \n    def _initialize_olfactory_layers(self):\n        \"\"\"Initialize bio-inspired olfactory processing layers.\"\"\"\n        # Olfactory receptor layer\n        self.receptor_weights = self._generate_receptor_sensitivity_map()\n        \n        # Glomerular convergence layer  \n        self.glomerular_map = self._create_glomerular_convergence()\n        \n        # Mitral cell lateral inhibition kernel\n        self.inhibition_kernel = self._create_lateral_inhibition_kernel()\n        \n        # Temporal dynamics state\n        self.mitral_state = np.zeros(self.mitral_config.num_cells)\n        self.receptor_adaptation = np.ones(self.receptor_config.num_receptors)\n        \n    def _generate_receptor_sensitivity_map(self) -> np.ndarray:\n        \"\"\"Generate biologically-inspired receptor sensitivity patterns.\"\"\"\n        num_receptors = self.receptor_config.num_receptors\n        \n        # Create log-normal distribution of sensitivities (biological pattern)\n        sensitivities = np.random.lognormal(\n            mean=np.log(1e-9),\n            sigma=2.0,\n            size=num_receptors\n        )\n        \n        # Clip to biological range\n        min_sens, max_sens = self.receptor_config.sensitivity_range\n        sensitivities = np.clip(sensitivities, min_sens, max_sens)\n        \n        return sensitivities.reshape(int(np.sqrt(num_receptors)), -1)\n    \n    def _create_glomerular_convergence(self) -> np.ndarray:\n        \"\"\"Create glomerular convergence pattern from receptors to mitral cells.\"\"\"\n        num_receptors = self.receptor_config.num_receptors\n        num_mitral = self.mitral_config.num_cells\n        \n        # Each mitral cell receives from ~20-50 receptors (biological ratio)\n        convergence_ratio = num_receptors // num_mitral\n        convergence_map = np.zeros((num_mitral, num_receptors))\n        \n        for mitral_idx in range(num_mitral):\n            # Random selection of receptor inputs with distance bias\n            receptor_indices = np.random.choice(\n                num_receptors,\n                size=min(convergence_ratio, num_receptors),\n                replace=False\n            )\n            \n            # Gaussian weights for convergence\n            weights = np.random.normal(1.0, 0.2, len(receptor_indices))\n            weights = np.clip(weights, 0.1, 2.0)\n            \n            convergence_map[mitral_idx, receptor_indices] = weights\n            \n        return convergence_map\n    \n    def _create_lateral_inhibition_kernel(self) -> np.ndarray:\n        \"\"\"Create lateral inhibition kernel for mitral cell interactions.\"\"\"\n        radius = self.mitral_config.inhibition_radius\n        strength = self.mitral_config.inhibition_strength\n        \n        # 2D Gaussian inhibition kernel\n        size = int(2 * radius + 1)\n        center = size // 2\n        y, x = np.ogrid[-center:size-center, -center:size-center]\n        \n        # Gaussian profile with center excitation\n        kernel = np.exp(-(x*x + y*y) / (2 * radius**2))\n        kernel = -strength * kernel  # Inhibitory\n        kernel[center, center] = 1.0  # Self-excitation\n        \n        return kernel / np.sum(np.abs(kernel))  # Normalize\n    \n    def detect_chemical_gradients(self, concentration_field: np.ndarray) -> Dict[str, np.ndarray]:\n        \"\"\"\n        Detect chemical gradients using bio-inspired olfactory processing.\n        \n        Args:\n            concentration_field: 2D array of chemical concentrations\n            \n        Returns:\n            Dictionary containing detected gradients and processed signals\n        \"\"\"\n        self.logger.debug(\"Starting chemical gradient detection\")\n        \n        # Step 1: Olfactory receptor response\n        receptor_response = self._olfactory_receptor_response(concentration_field)\n        \n        # Step 2: Glomerular convergence\n        glomerular_output = self._glomerular_processing(receptor_response)\n        \n        # Step 3: Mitral cell lateral inhibition\n        mitral_output = self._mitral_cell_processing(glomerular_output)\n        \n        # Step 4: Gradient computation using analog PDE solving\n        gradients = self._compute_gradients_analog(concentration_field, mitral_output)\n        \n        return {\n            'receptor_response': receptor_response,\n            'glomerular_output': glomerular_output,\n            'mitral_output': mitral_output,\n            'gradients': gradients,\n            'gradient_magnitude': np.linalg.norm(gradients, axis=2),\n            'gradient_direction': np.arctan2(gradients[:,:,1], gradients[:,:,0])\n        }\n    \n    def _olfactory_receptor_response(self, concentration: np.ndarray) -> np.ndarray:\n        \"\"\"Simulate olfactory receptor response to chemical concentrations.\"\"\"\n        # Resize concentration field to match receptor array\n        receptor_grid_size = int(np.sqrt(self.receptor_config.num_receptors))\n        if concentration.shape != (receptor_grid_size, receptor_grid_size):\n            from scipy.ndimage import zoom\n            scale_factor = receptor_grid_size / concentration.shape[0]\n            concentration_resized = zoom(concentration, scale_factor)\n        else:\n            concentration_resized = concentration\n        \n        # Apply receptor sensitivity with adaptation\n        response = (concentration_resized * self.receptor_weights * \n                   self.receptor_adaptation.reshape(receptor_grid_size, -1))\n        \n        # Hill equation for receptor saturation\n        hill_coeff = 2.0\n        half_max = 1e-8\n        response = (response**hill_coeff) / (response**hill_coeff + half_max**hill_coeff)\n        \n        # Add receptor noise\n        noise = np.random.normal(0, self.receptor_config.noise_level, response.shape)\n        response += noise\n        \n        # Update adaptation (slow negative feedback)\n        adaptation_decay = np.exp(-self.receptor_config.adaptation_rate)\n        self.receptor_adaptation *= adaptation_decay\n        self.receptor_adaptation += (1 - adaptation_decay) * response.flatten()\n        \n        return np.clip(response, 0, 1)\n    \n    def _glomerular_processing(self, receptor_response: np.ndarray) -> np.ndarray:\n        \"\"\"Process receptor signals through glomerular convergence.\"\"\"\n        # Flatten receptor response for matrix multiplication\n        receptor_flat = receptor_response.flatten()\n        \n        # Glomerular convergence: many-to-one mapping\n        glomerular_output = np.dot(self.glomerular_map, receptor_flat)\n        \n        # Apply sigmoid activation (glomerular nonlinearity)\n        glomerular_output = 1 / (1 + np.exp(-5 * (glomerular_output - 0.5)))\n        \n        return glomerular_output\n    \n    def _mitral_cell_processing(self, glomerular_input: np.ndarray) -> np.ndarray:\n        \"\"\"Process signals through mitral cell lateral inhibition network.\"\"\"\n        # Reshape to 2D grid for convolution\n        grid_size = int(np.sqrt(self.mitral_config.num_cells))\n        if len(glomerular_input) != grid_size * grid_size:\n            # Pad or truncate to fit grid\n            if len(glomerular_input) < grid_size * grid_size:\n                padded = np.zeros(grid_size * grid_size)\n                padded[:len(glomerular_input)] = glomerular_input\n                glomerular_input = padded\n            else:\n                glomerular_input = glomerular_input[:grid_size * grid_size]\n        \n        mitral_grid = glomerular_input.reshape(grid_size, grid_size)\n        \n        # Apply lateral inhibition via convolution\n        inhibited = convolve2d(mitral_grid, self.inhibition_kernel, \n                              mode='same', boundary='symm')\n        \n        # Temporal dynamics (gamma oscillations)\n        if self.mitral_config.temporal_dynamics:\n            oscillation = np.sin(2 * np.pi * self.mitral_config.oscillation_frequency * \n                               np.random.random())\n            inhibited *= (1 + 0.1 * oscillation)\n        \n        # Update mitral state with decay\n        decay_rate = 0.9\n        self.mitral_state = decay_rate * self.mitral_state + (1 - decay_rate) * inhibited.flatten()\n        \n        return np.clip(self.mitral_state.reshape(grid_size, grid_size), 0, 1)\n    \n    def _compute_gradients_analog(self, concentration: np.ndarray, \n                                 mitral_output: np.ndarray) -> np.ndarray:\n        \"\"\"Compute chemical gradients using analog PDE solver.\"\"\"\n        # Create modified Poisson equation for gradient computation\n        # \u2207\u00b2\u03c6 = -\u03c1 where \u03c1 is the processed concentration field\n        \n        class GradientPoissonEquation:\n            def __init__(self, concentration_field, mitral_weights):\n                self.domain_size = concentration_field.shape\n                self.source_field = concentration_field * mitral_weights\n                \n            def source_function(self, x, y):\n                i, j = int(x * self.domain_size[0]), int(y * self.domain_size[1])\n                i = np.clip(i, 0, self.domain_size[0] - 1)\n                j = np.clip(j, 0, self.domain_size[1] - 1)\n                return self.source_field[i, j]\n        \n        # Resize mitral output to match concentration field\n        if mitral_output.shape != concentration.shape:\n            from scipy.ndimage import zoom\n            scale_factor = concentration.shape[0] / mitral_output.shape[0]\n            mitral_resized = zoom(mitral_output, scale_factor)\n        else:\n            mitral_resized = mitral_output\n            \n        pde = GradientPoissonEquation(concentration, mitral_resized)\n        \n        # Solve for potential field\n        try:\n            potential = self.pde_solver.solve(pde, iterations=50, convergence_threshold=1e-4)\n            \n            # Compute gradients via finite differences\n            grad_x = np.gradient(potential.reshape(concentration.shape), axis=1)\n            grad_y = np.gradient(potential.reshape(concentration.shape), axis=0)\n            \n            gradients = np.stack([grad_x, grad_y], axis=2)\n            \n        except Exception as e:\n            self.logger.warning(f\"Analog gradient computation failed: {e}\")\n            # Fallback to simple finite difference\n            grad_x = np.gradient(concentration, axis=1)\n            grad_y = np.gradient(concentration, axis=0)\n            gradients = np.stack([grad_x, grad_y], axis=2)\n        \n        return gradients\n    \n    def fuse_multimodal_signals(self, \n                               chemical_signals: List[np.ndarray],\n                               signal_weights: Optional[List[float]] = None) -> np.ndarray:\n        \"\"\"\n        Fuse multiple chemical signals using bio-inspired processing.\n        \n        Args:\n            chemical_signals: List of 2D chemical concentration fields\n            signal_weights: Optional weights for each signal type\n            \n        Returns:\n            Fused chemical gradient map\n        \"\"\"\n        if not chemical_signals:\n            raise ValueError(\"No chemical signals provided\")\n            \n        if signal_weights is None:\n            signal_weights = [1.0] * len(chemical_signals)\n            \n        if len(signal_weights) != len(chemical_signals):\n            raise ValueError(\"Number of weights must match number of signals\")\n        \n        # Process each signal through olfactory pathway\n        processed_signals = []\n        for signal, weight in zip(chemical_signals, signal_weights):\n            result = self.detect_chemical_gradients(signal)\n            processed_signals.append(weight * result['gradient_magnitude'])\n        \n        # Fusion strategy: weighted combination with competitive dynamics\n        fused_signal = np.zeros_like(processed_signals[0])\n        total_weight = 0\n        \n        for signal, weight in zip(processed_signals, signal_weights):\n            # Winner-take-all with soft competition\n            competition_factor = np.exp(5 * signal) / (1 + np.exp(5 * signal))\n            fused_signal += weight * signal * competition_factor\n            total_weight += weight\n            \n        # Normalize\n        if total_weight > 0:\n            fused_signal /= total_weight\n            \n        return fused_signal\n    \n    def adapt_to_environment(self, \n                           training_signals: List[np.ndarray],\n                           learning_rate: float = 0.01) -> None:\n        \"\"\"\n        Adapt olfactory processing to environmental statistics.\n        \n        Args:\n            training_signals: List of representative chemical fields\n            learning_rate: Adaptation learning rate\n        \"\"\"\n        self.logger.info(f\"Adapting to environment with {len(training_signals)} training signals\")\n        \n        # Compute signal statistics\n        signal_means = []\n        signal_stds = []\n        \n        for signal in training_signals:\n            signal_means.append(np.mean(signal))\n            signal_stds.append(np.std(signal))\n        \n        env_mean = np.mean(signal_means)\n        env_std = np.mean(signal_stds)\n        \n        # Adapt receptor sensitivities\n        adaptation_factor = learning_rate * (env_std / (env_mean + 1e-8))\n        \n        self.receptor_weights *= (1 + adaptation_factor * \n                                 np.random.normal(0, 0.1, self.receptor_weights.shape))\n        \n        # Adapt mitral cell inhibition strength\n        self.mitral_config.inhibition_strength *= (1 + adaptation_factor * 0.1)\n        self.mitral_config.inhibition_strength = np.clip(\n            self.mitral_config.inhibition_strength, 0.1, 1.0\n        )\n        \n        # Update inhibition kernel\n        self.inhibition_kernel = self._create_lateral_inhibition_kernel()\n        \n        self.logger.info(f\"Environment adaptation complete. New inhibition strength: \"\n                        f\"{self.mitral_config.inhibition_strength:.3f}\")\n    \n    def get_processing_metrics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive metrics about olfactory processing performance.\"\"\"\n        return {\n            'receptor_metrics': {\n                'num_receptors': self.receptor_config.num_receptors,\n                'sensitivity_range': self.receptor_config.sensitivity_range,\n                'adaptation_state': {\n                    'mean': np.mean(self.receptor_adaptation),\n                    'std': np.std(self.receptor_adaptation),\n                    'min': np.min(self.receptor_adaptation),\n                    'max': np.max(self.receptor_adaptation)\n                }\n            },\n            'mitral_metrics': {\n                'num_cells': self.mitral_config.num_cells,\n                'inhibition_strength': self.mitral_config.inhibition_strength,\n                'current_state': {\n                    'mean': np.mean(self.mitral_state),\n                    'std': np.std(self.mitral_state),\n                    'active_fraction': np.mean(self.mitral_state > 0.1)\n                }\n            },\n            'glomerular_metrics': {\n                'convergence_ratio': self.receptor_config.num_receptors / self.mitral_config.num_cells,\n                'connection_density': np.mean(self.glomerular_map > 0)\n            }\n        }\n\n# Research benchmark function\ndef benchmark_bioneuro_olfactory_performance():\n    \"\"\"Benchmark bio-neuromorphic olfactory fusion performance.\"\"\"\n    print(\"\ud83e\udde0 Bio-Neuromorphic Olfactory Fusion Benchmark\")\n    print(\"=\" * 50)\n    \n    # Initialize system\n    fusion_engine = BioneuroOlfactoryFusion(\n        receptor_config=OlfactoryReceptorConfig(num_receptors=256),\n        mitral_config=MitralCellNetwork(num_cells=64)\n    )\n    \n    # Create test chemical fields\n    size = 32\n    x, y = np.meshgrid(np.linspace(0, 10, size), np.linspace(0, 10, size))\n    \n    # Chemical plume with gradient\n    chemical_field1 = np.exp(-((x-5)**2 + (y-7)**2) / 4)\n    \n    # Secondary chemical source\n    chemical_field2 = np.exp(-((x-3)**2 + (y-2)**2) / 2)\n    \n    # Process signals\n    print(\"Processing chemical field 1...\")\n    result1 = fusion_engine.detect_chemical_gradients(chemical_field1)\n    \n    print(\"Processing chemical field 2...\")\n    result2 = fusion_engine.detect_chemical_gradients(chemical_field2)\n    \n    # Multi-modal fusion\n    print(\"Performing multi-modal fusion...\")\n    fused = fusion_engine.fuse_multimodal_signals(\n        [chemical_field1, chemical_field2], \n        signal_weights=[0.7, 0.3]\n    )\n    \n    # Environmental adaptation\n    print(\"Adapting to environment...\")\n    fusion_engine.adapt_to_environment([chemical_field1, chemical_field2])\n    \n    # Get metrics\n    metrics = fusion_engine.get_processing_metrics()\n    \n    print(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/research/integrated_solver_framework.py",
      "line_number": 1,
      "details": "\"\"\"Integrated Advanced Solver Framework.\n\nThis module provides a unified interface to all advanced algorithms,\nenabling seamless integration and automatic algorithm selection based on\nproblem characteristics.\n\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple, Union, Callable\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom collections import defaultdict\nimport time\nimport threading\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nfrom ..core.solver import AnalogPDESolver\nfrom ..core.crossbar import AnalogCrossbarArray\nfrom ..utils.logger import get_logger, PerformanceLogger\nfrom .validation_layer import validate_algorithm_result, ValidationLevel, ValidationResult\n\nfrom .ml_acceleration import MLAcceleratedPDESolver\nfrom .advanced_analog_algorithms import (\n    AnalogPhysicsInformedCrossbar,\n    TemporalCrossbarCascade,\n    HeterogeneousPrecisionAnalogComputing,\n    PrecisionLevel,\n    PhysicsConstraint\n)\nfrom .multi_physics_coupling import (\n    AnalogMultiPhysicsCoupler,\n    PhysicsDomain,\n    PhysicsDomainConfig,\n    CouplingInterface\n)\nfrom .neuromorphic_acceleration import (\n    NeuromorphicPDESolver,\n    NeuromorphicSpikeEncoder,\n    NeuromorphicSpikeDecoder,\n    SpikeEncoding\n)\n\n\nclass AlgorithmType(Enum):\n    \"\"\"Types of advanced algorithms available.\"\"\"\n    BASE_ANALOG = \"base_analog\"\n    ML_ACCELERATED = \"ml_accelerated\"\n    PHYSICS_INFORMED = \"physics_informed\"\n    TEMPORAL_CASCADE = \"temporal_cascade\"\n    HETEROGENEOUS_PRECISION = \"heterogeneous_precision\"\n    MULTI_PHYSICS = \"multi_physics\"\n    NEUROMORPHIC = \"neuromorphic\"\n    ADAPTIVE_HYBRID = \"adaptive_hybrid\"\n\n\n@dataclass\nclass ProblemCharacteristics:\n    \"\"\"Characteristics of PDE problem for algorithm selection.\"\"\"\n    problem_size: Tuple[int, ...]\n    sparsity_level: float\n    time_dependent: bool\n    multi_physics: bool\n    conservation_required: bool\n    accuracy_requirement: float\n    energy_budget: Optional[float]\n    real_time_requirement: bool\n    physics_constraints: List[str]\n    boundary_complexity: str  # 'simple', 'complex', 'time_varying'\n\n\n@dataclass\nclass AlgorithmRecommendation:\n    \"\"\"Algorithm recommendation with performance estimates.\"\"\"\n    algorithm_type: AlgorithmType\n    confidence: float  # 0-1\n    estimated_speedup: float\n    estimated_energy_savings: float\n    estimated_accuracy: float\n    reasoning: str\n    configuration: Dict[str, Any]\n\n\nclass AdvancedSolverFramework:\n    \"\"\"Unified framework for advanced analog PDE solving algorithms.\n    \n    Automatically selects optimal algorithms based on problem characteristics\n    and provides seamless integration of all advanced techniques.\n    \"\"\"\n    \n    def __init__(\n        self,\n        base_crossbar_size: int = 128,\n        enable_ml_acceleration: bool = True,\n        enable_neuromorphic: bool = True,\n        enable_multi_physics: bool = True,\n        performance_mode: str = 'balanced'  # 'speed', 'accuracy', 'energy', 'balanced'\n    ):\n        \"\"\"Initialize advanced solver framework.\n        \n        Args:\n            base_crossbar_size: Size of base crossbar array\n            enable_ml_acceleration: Enable ML acceleration\n            enable_neuromorphic: Enable neuromorphic acceleration\n            enable_multi_physics: Enable multi-physics coupling\n            performance_mode: Performance optimization mode\n        \"\"\"\n        self.logger = get_logger('advanced_framework')\n        self.perf_logger = PerformanceLogger(self.logger)\n        \n        self.base_crossbar_size = base_crossbar_size\n        self.performance_mode = performance_mode\n        \n        # Initialize base components\n        self.base_crossbar = AnalogCrossbarArray(base_crossbar_size, base_crossbar_size)\n        self.base_solver = AnalogPDESolver(crossbar_size=base_crossbar_size)\n        \n        # Initialize advanced algorithm components\n        self.algorithms = {}\n        self._initialize_algorithms(enable_ml_acceleration, enable_neuromorphic, enable_multi_physics)\n        \n        # Algorithm selection and performance tracking\n        self.algorithm_selector = AlgorithmSelector(self.performance_mode)\n        self.performance_tracker = PerformanceTracker()\n        \n        # Problem history for adaptive learning\n        self.problem_history = []\n        self.performance_history = {}\n        \n        self.logger.info(f\"Initialized Advanced Solver Framework with {len(self.algorithms)} algorithms\")\n    \n    def _initialize_algorithms(\n        self,\n        enable_ml: bool,\n        enable_neuromorphic: bool,\n        enable_multi_physics: bool\n    ) -> None:\n        \"\"\"Initialize all advanced algorithms.\"\"\"\n        \n        # Base analog solver (always available)\n        self.algorithms[AlgorithmType.BASE_ANALOG] = self.base_solver\n        \n        # ML-accelerated solver\n        if enable_ml:\n            try:\n                self.algorithms[AlgorithmType.ML_ACCELERATED] = MLAcceleratedPDESolver(\n                    self.base_solver,\n                    surrogate_type='neural_network'\n                )\n                \n                # Physics-informed variant\n                physics_ml_solver = MLAcceleratedPDESolver(\n                    self.base_solver,\n                    surrogate_type='physics_informed'\n                )\n                self.algorithms[AlgorithmType.PHYSICS_INFORMED] = physics_ml_solver\n                \n            except Exception as e:\n                self.logger.warning(f\"Failed to initialize ML algorithms: {e}\")\n        \n        # Advanced analog algorithms\n        try:\n            # Physics-informed crossbar\n            physics_constraints = [\n                PhysicsConstraint(\n                    constraint_type='conservation',\n                    constraint_function=lambda x: np.sum(x),\n                    weight=1.0,\n                    conductance_mapping=None,\n                    active_regions=[(0, 32, 0, 32)],  # Example region\n                    conservation_required=True,\n                    bidirectional=False\n                )\n            ]\n            \n            physics_crossbar = AnalogPhysicsInformedCrossbar(\n                self.base_crossbar,\n                physics_constraints\n            )\n            self.algorithms[AlgorithmType.PHYSICS_INFORMED] = physics_crossbar\n            \n            # Temporal cascade\n            cascade_crossbars = [\n                AnalogCrossbarArray(self.base_crossbar_size, self.base_crossbar_size)\n                for _ in range(4)\n            ]\n            temporal_cascade = TemporalCrossbarCascade(\n                cascade_crossbars,\n                time_step=0.001\n            )\n            self.algorithms[AlgorithmType.TEMPORAL_CASCADE] = temporal_cascade\n            \n            # Heterogeneous precision\n            hetero_precision = HeterogeneousPrecisionAnalogComputing(\n                self.base_crossbar\n            )\n            self.algorithms[AlgorithmType.HETEROGENEOUS_PRECISION] = hetero_precision\n            \n        except Exception as e:\n            self.logger.warning(f\"Failed to initialize advanced analog algorithms: {e}\")\n        \n        # Neuromorphic acceleration\n        if enable_neuromorphic:\n            try:\n                neuromorphic_solver = NeuromorphicPDESolver(\n                    self.base_solver,\n                    sparsity_threshold=0.9\n                )\n                self.algorithms[AlgorithmType.NEUROMORPHIC] = neuromorphic_solver\n                \n            except Exception as e:\n                self.logger.warning(f\"Failed to initialize neuromorphic algorithms: {e}\")\n        \n        # Multi-physics coupling\n        if enable_multi_physics:\n            try:\n                # Example multi-physics configuration\n                thermal_domain = PhysicsDomainConfig(\n                    domain_type=PhysicsDomain.THERMAL,\n                    governing_equations=['heat_equation'],\n                    crossbar_allocation=(0, 64, 0, 64),\n                    boundary_conditions={'dirichlet': True},\n                    material_properties={'conductivity': 1.0},\n                    source_terms=None,\n                    time_scale=1.0,\n                    length_scale=1.0\n                )\n                \n                fluid_domain = PhysicsDomainConfig(\n                    domain_type=PhysicsDomain.FLUID,\n                    governing_equations=['navier_stokes'],\n                    crossbar_allocation=(64, 128, 0, 64),\n                    boundary_conditions={'dirichlet': True},\n                    material_properties={'viscosity': 1e-3},\n                    source_terms=None,\n                    time_scale=0.1,\n                    length_scale=1.0\n                )\n                \n                coupling_interface = CouplingInterface(\n                    source_domain=PhysicsDomain.THERMAL,\n                    target_domain=PhysicsDomain.FLUID,\n                    coupling_type='source_term',\n                    coupling_strength=0.1,\n                    coupling_function=lambda x: 0.1 * x,  # Simple linear coupling\n                    interface_regions=[(32, 96, 32, 96)],\n                    conservation_required=True,\n                    bidirectional=True\n                )\n                \n                multi_physics_coupler = AnalogMultiPhysicsCoupler(\n                    self.base_crossbar,\n                    [thermal_domain, fluid_domain],\n                    [coupling_interface]\n                )\n                \n                self.algorithms[AlgorithmType.MULTI_PHYSICS] = multi_physics_coupler\n                \n            except Exception as e:\n                self.logger.warning(f\"Failed to initialize multi-physics algorithms: {e}\")\n    \n    def solve_pde(\n        self,\n        pde,\n        problem_characteristics: Optional[ProblemCharacteristics] = None,\n        algorithm_preference: Optional[AlgorithmType] = None,\n        **kwargs\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Solve PDE using optimal algorithm selection.\n        \n        Args:\n            pde: PDE problem to solve\n            problem_characteristics: Problem characteristics for algorithm selection\n            algorithm_preference: Preferred algorithm (overrides automatic selection)\n            **kwargs: Additional solver parameters\n            \n        Returns:\n            Tuple of (solution, solve_info)\n        \"\"\"\n        self.perf_logger.start_timer('framework_solve')\n        \n        # Analyze problem if characteristics not provided\n        if problem_characteristics is None:\n            problem_characteristics = self._analyze_problem(pde, **kwargs)\n        \n        # Select optimal algorithm\n        if algorithm_preference is None:\n            recommendation = self.algorithm_selector.recommend_algorithm(\n                problem_characteristics,\n                self.algorithms,\n                self.performance_history\n            )\n            selected_algorithm = recommendation.algorithm_type\n        else:\n            selected_algorithm = algorithm_preference\n            recommendation = AlgorithmRecommendation(\n                algorithm_type=selected_algorithm,\n                confidence=1.0,\n                estimated_speedup=1.0,\n                estimated_energy_savings=0.0,\n                estimated_accuracy=1e-6,\n                reasoning=\"User specified\",\n                configuration={}\n            )\n        \n        self.logger.info(f\"Selected algorithm: {selected_algorithm.value} (confidence: {recommendation.confidence:.2f})\")\n        \n        # Execute solve with selected algorithm\n        solution, solve_info = self._execute_solve(\n            selected_algorithm,\n            pde,\n            problem_characteristics,\n            recommendation.configuration,\n            **kwargs\n        )\n        \n        total_time = self.perf_logger.end_timer('framework_solve')\n        \n        # Update performance tracking\n        self._update_performance_history(\n            selected_algorithm,\n            problem_characteristics,\n            solve_info,\n            total_time\n        )\n        \n        # Validate algorithm results\n        validation_metadata = {\n            'tolerance': problem_characteristics.accuracy_requirement,\n            'domain_shape': problem_characteristics.problem_size,\n            'conservation_type': 'mass' if problem_characteristics.conservation_required else None,\n            'boundary_spec': {\n                'dirichlet': hasattr(pde, 'boundary_conditions') and 'dirichlet' in str(pde.boundary_conditions).lower(),\n                'dirichlet_value': 0.0,  # Default assumption\n                'tolerance': problem_characteristics.accuracy_requirement\n            } if hasattr(pde, 'boundary_conditions') else {}\n        }\n        \n        # Add convergence history if available\n        if 'convergence_history' in solve_info or 'residual_history' in solve_info:\n            validation_metadata['convergence_history'] = (\n                solve_info.get('convergence_history') or \n                solve_info.get('residual_history') or\n                []\n            )\n        \n        # Run validation\n        try:\n            validation_report = validate_algorithm_result(\n                algorithm_name=selected_algorithm.value,\n                inputs={'pde': pde, 'characteristics': problem_characteristics},\n                outputs={'solution': solution},\n                metadata=validation_metadata,\n                validation_level=ValidationLevel.STANDARD\n            )\n            \n            # Log validation results\n            if validation_report.has_critical_issues:\n                self.logger.error(f\"Critical validation issues found: {len(validation_report.issues)}\")\n                for issue in validation_report.issues:\n                    if issue.level == ValidationResult.CRITICAL:\n                        self.logger.error(f\"  - {issue.category}: {issue.message}\")\n            elif validation_report.has_failures:\n                self.logger.warning(f\"Validation failures found: {validation_report.metrics['failed_issues']}\")\n            elif validation_report.issues:\n                self.logger.info(f\"Validation completed with {len(validation_report.issues)} warnings\")\n            else:\n                self.logger.debug(\"Validation passed successfully\")\n            \n            # Add validation report to solve info\n            solve_info['validation_report'] = {\n                'overall_result': validation_report.overall_result.value,\n                'issues_count': len(validation_report.issues),\n                'critical_issues': validation_report.metrics.get('critical_issues', 0),\n                'failed_issues': validation_report.metrics.get('failed_issues', 0),\n                'warning_issues': validation_report.metrics.get('warning_issues', 0)\n            }\n            \n        except Exception as e:\n            self.logger.warning(f\"Validation failed: {e}\")\n            solve_info['validation_report'] = {\n                'overall_result': 'validation_error',\n                'error': str(e)\n            }\n        \n        # Enhanced solve info with framework details\n        solve_info.update({\n            'framework_version': '2.0.0',\n            'selected_algorithm': selected_algorithm.value,\n            'algorithm_recommendation': {\n                'confidence': recommendation.confidence,\n                'reasoning': recommendation.reasoning,\n                'estimated_speedup': recommendation.estimated_speedup,\n                'estimated_energy_savings': recommendation.estimated_energy_savings\n            },\n            'problem_characteristics': problem_characteristics,\n            'total_framework_time': total_time\n        })\n        \n        return solution, solve_info\n    \n    def _analyze_problem(self, pde, **kwargs) -> ProblemCharacteristics:\n        \"\"\"Analyze PDE problem to extract characteristics.\"\"\"\n        \n        # Extract problem size\n        if hasattr(pde, 'domain_size'):\n            if isinstance(pde.domain_size, tuple):\n                problem_size = pde.domain_size\n            else:\n                problem_size = (pde.domain_size,)\n        else:\n            problem_size = (self.base_crossbar_size,)\n        \n        # Analyze sparsity (if solution or matrix available)\n        sparsity_level = 0.0\n        if hasattr(pde, 'get_matrix'):\n            try:\n                matrix = pde.get_matrix()\n                sparsity_level = 1.0 - (np.count_nonzero(matrix) / matrix.size)\n            except:\n                pass\n        \n        # Check for time dependence\n        time_dependent = (\n            hasattr(pde, 'time_dependent') and pde.time_dependent or\n            'time' in kwargs or\n            'dt' in kwargs or\n            'num_time_steps' in kwargs\n        )\n        \n        # Check for multi-physics\n        multi_physics = (\n            len(getattr(pde, 'coupled_equations', [])) > 1 or\n            hasattr(pde, 'physics_domains')\n        )\n        \n        # Check conservation requirements\n        conservation_required = (\n            hasattr(pde, 'conservation_laws') or\n            'conservation' in str(type(pde)).lower() or\n            'navier' in str(type(pde)).lower()  # Navier-Stokes typically requires conservation\n        )\n        \n        # Extract accuracy requirement\n        accuracy_requirement = kwargs.get('convergence_threshold', 1e-6)\n        \n        # Check for real-time requirements\n        real_time_requirement = kwargs.get('real_time', False)\n        \n        # Extract physics constraints\n        physics_constraints = []\n        if hasattr(pde, 'constraints'):\n            physics_constraints = list(pde.constraints.keys())\n        \n        # Determine boundary complexity\n        boundary_complexity = 'simple'  # Default\n        if hasattr(pde, 'boundary_conditions'):\n            bc = pde.boundary_conditions\n            if isinstance(bc, dict) and len(bc) > 2:\n                boundary_complexity = 'complex'\n            elif callable(bc) or any(callable(v) for v in bc.values() if isinstance(bc, dict)):\n                boundary_complexity = 'time_varying'\n        \n        return ProblemCharacteristics(\n            problem_size=problem_size,\n            sparsity_level=sparsity_level,\n            time_dependent=time_dependent,\n            multi_physics=multi_physics,\n            conservation_required=conservation_required,\n            accuracy_requirement=accuracy_requirement,\n            energy_budget=kwargs.get('energy_budget'),\n            real_time_requirement=real_time_requirement,\n            physics_constraints=physics_constraints,\n            boundary_complexity=boundary_complexity\n        )\n    \n    def _execute_solve(\n        self,\n        algorithm_type: AlgorithmType,\n        pde,\n        characteristics: ProblemCharacteristics,\n        configuration: Dict[str, Any],\n        **kwargs\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Execute solve with specified algorithm.\"\"\"\n        \n        if algorithm_type not in self.algorithms:\n            self.logger.warning(f\"Algorithm {algorithm_type.value} not available, falling back to base solver\")\n            algorithm_type = AlgorithmType.BASE_ANALOG\n        \n        algorithm = self.algorithms[algorithm_type]\n        solve_info = {'algorithm_used': algorithm_type.value}\n        \n        try:\n            if algorithm_type == AlgorithmType.BASE_ANALOG:\n                # Base analog solver\n                solution = algorithm.solve(\n                    pde,\n                    iterations=kwargs.get('iterations', 100),\n                    convergence_threshold=kwargs.get('convergence_threshold', 1e-6)\n                )\n                \n            elif algorithm_type == AlgorithmType.ML_ACCELERATED:\n                # ML-accelerated solver\n                solution, ml_info = algorithm.solve(\n                    pde,\n                    iterations=kwargs.get('iterations', 100),\n                    convergence_threshold=kwargs.get('convergence_threshold', 1e-6)\n                )\n                solve_info.update(ml_info)\n                \n            elif algorithm_type == AlgorithmType.NEUROMORPHIC:\n                # Neuromorphic solver\n                if characteristics.time_dependent:\n                    time_span = kwargs.get('time_span', (0.0, 1.0))\n                    num_time_steps = kwargs.get('num_time_steps', 100)\n                    initial_solution = kwargs.get('initial_solution', np.random.random(characteristics.problem_size[0]))\n                    \n                    solution, neuro_info = algorithm.solve_sparse_pde(\n                        pde,\n                        initial_solution,\n                        time_span,\n                        num_time_steps\n                    )\n                    solve_info.update(neuro_info)\n                else:\n                    # Fall back to base solver for non-time-dependent\n                    solution = self.base_solver.solve(pde, iterations=kwargs.get('iterations', 100))\n                    \n            elif algorithm_type == AlgorithmType.MULTI_PHYSICS:\n                # Multi-physics coupling\n                if characteristics.multi_physics:\n                    initial_conditions = kwargs.get('initial_conditions', {\n                        PhysicsDomain.THERMAL: np.random.random(64),\n                        PhysicsDomain.FLUID: np.random.random(64)\n                    })\n                    time_span = kwargs.get('time_span', (0.0, 1.0))\n                    num_time_steps = kwargs.get('num_time_steps', 100)\n                    \n                    solutions, coupling_info = algorithm.solve_coupled_system(\n                        initial_conditions,\n                        time_span,\n                        num_time_steps\n                    )\n                    \n                    # Combine solutions (simplified)\n                    solution = np.concatenate([s for s in solutions.values()])\n                    solve_info.update(coupling_info)\n                else:\n                    # Fall back to base solver\n                    solution = self.base_solver.solve(pde, iterations=kwargs.get('iterations', 100))\n                    \n            elif algorithm_type == AlgorithmType.TEMPORAL_CASCADE:\n                # Temporal cascade\n                if characteristics.time_dependent:\n                    # Setup temporal pipeline\n                    spatial_operator = np.random.random((characteristics.problem_size[0], characteristics.problem_size[0]))  # Placeholder\n                    boundary_conditions = {'dirichlet': True}\n                    \n                    algorithm.setup_temporal_pipeline(spatial_operator, boundary_conditions)\n                    \n                    # Evolve solution\n                    initial_state = kwargs.get('initial_solution', np.random.random(characteristics.problem_size[0]))\n                    num_time_steps = kwargs.get('num_time_steps', 100)\n                    \n                    solution, temporal_info = algorithm.evolve_temporal_pipeline(\n                        initial_state,\n                        num_time_steps\n                    )\n                    solve_info.update(temporal_info)\n                else:\n                    # Fall back to base solver\n                    solution = self.base_solver.solve(pde, iterations=kwargs.get('iterations', 100))\n                    \n            elif algorithm_type == AlgorithmType.HETEROGENEOUS_PRECISION:\n                # Heterogeneous precision\n                # Adapt precision based on current solution estimate\n                initial_solution = kwargs.get('initial_solution', np.random.random(characteristics.problem_size[0]))\n                \n                adaptation_metrics = algorithm.adapt_precision_allocation(\n                    initial_solution,\n                    characteristics.accuracy_requirement\n                )\n                \n                # Compute solution with adapted precision\n                solution, computation_metrics = algorithm.compute_heterogeneous_vmm(\n                    initial_solution\n                )\n                \n                solve_info.update({\n                    'adaptation_metrics': adaptation_metrics,\n                    'computation_metrics': computation_metrics\n                })\n                \n            else:\n                # Default to base solver\n                solution = self.base_solver.solve(\n                    pde,\n                    iterations=kwargs.get('iterations', 100),\n                    convergence_threshold=kwargs.get('convergence_threshold', 1e-6)\n                )\n            \n        except Exception as e:\n            self.logger.error(f\"Algorithm {algorithm_type.value} failed: {e}\")\n            # Fall back to base solver\n            solution = self.base_solver.solve(\n                pde,\n                iterations=kwargs.get('iterations', 100),\n                convergence_threshold=kwargs.get('convergence_threshold', 1e-6)\n            )\n            solve_info['fallback_used'] = True\n            solve_info['error'] = str(e)\n        \n        return solution, solve_info\n    \n    def _update_performance_history(\n        self,\n        algorithm_type: AlgorithmType,\n        characteristics: ProblemCharacteristics,\n        solve_info: Dict[str, Any],\n        total_time: float\n    ) -> None:\n        \"\"\"Update performance history for future algorithm selection.\"\"\"\n        \n        if algorithm_type not in self.performance_history:\n            self.performance_history[algorithm_type] = []\n        \n        performance_record = {\n            'problem_size': characteristics.problem_size,\n            'sparsity_level': characteristics.sparsity_level,\n            'time_dependent': characteristics.time_dependent,\n            'multi_physics': characteristics.multi_physics,\n            'solve_time': total_time,\n            'solve_info': solve_info,\n            'timestamp': time.time()\n        }\n        \n        self.performance_history[algorithm_type].append(performance_record)\n        \n        # Keep only recent records (last 1000)\n        if len(self.performance_history[algorithm_type]) > 1000:\n            self.performance_history[algorithm_type] = self.performance_history[algorithm_type][-1000:]\n    \n    def get_algorithm_performance_summary(self) -> Dict[str, Any]:\n        \"\"\"Get performance summary for all algorithms.\"\"\"\n        summary = {}\n        \n        for algorithm_type, records in self.performance_history.items():\n            if records:\n                solve_times = [r['solve_time'] for r in records]\n                summary[algorithm_type.value] = {\n                    'num_uses': len(records),\n                    'avg_solve_time': np.mean(solve_times),\n                    'min_solve_time': np.min(solve_times),\n                    'max_solve_time': np.max(solve_times),\n                    'std_solve_time': np.std(solve_times),\n                    'recent_uses': len([r for r in records if time.time() - r['timestamp'] < 3600])  # Last hour\n                }\n        \n        return summary\n    \n    def benchmark_algorithms(\n        self,\n        test_problems: List[Tuple[Any, Dict[str, Any]]],\n        algorithms_to_test: Optional[List[AlgorithmType]] = None\n    ) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Benchmark multiple algorithms on test problems.\n        \n        Args:\n            test_problems: List of (pde, kwargs) tuples\n            algorithms_to_test: Specific algorithms to test (default: all available)\n            \n        Returns:\n            Benchmark results\n        \"\"\"\n        if algorithms_to_test is None:\n            algorithms_to_test = list(self.algorithms.keys())\n        \n        benchmark_results = {}\n        \n        self.logger.info(f\"Starting benchmark with {len(test_problems)} problems and {len(algorithms_to_test)} algorithms\")\n        \n        for algorithm_type in algorithms_to_test:\n            if algorithm_type not in self.algorithms:\n                continue\n                \n            algorithm_results = {\n                'solve_times': [],\n                'errors': [],\n                'successes': 0,\n                'failures': 0\n            }\n            \n            for i, (pde, kwargs) in enumerate(test_problems):\n                try:\n                    self.perf_logger.start_timer(f'benchmark_{algorithm_type.value}_{i}')\n                    \n                    solution, solve_info = self._execute_solve(\n                        algorithm_type,\n                        pde,\n                        self._analyze_problem(pde, **kwargs),\n                        {},\n                        **kwargs\n                    )\n                    \n                    solve_time = self.perf_logger.end_timer(f'benchmark_{algorithm_type.value}_{i}')\n                    \n                    algorithm_results['solve_times'].append(solve_time)\n                    algorithm_results['successes'] += 1\n                    \n                    # Compute error if reference solution available\n                    if 'reference_solution' in kwargs:\n                        error = np.linalg.norm(solution - kwargs['reference_solution'])\n                        algorithm_results['errors'].append(error)\n                    \n                except Exception as e:\n                    self.logger.warning(f\"Benchmark failed for {algorithm_type.value} on problem {i}: {e}\")\n                    algorithm_results['failures'] += 1\n            \n            # Compute summary statistics\n            if algorithm_results['solve_times']:\n                algorithm_results.update({\n                    'avg_solve_time': np.mean(algorithm_results['solve_times']),\n                    'min_solve_time': np.min(algorithm_results['solve_times']),\n                    'max_solve_time': np.max(algorithm_results['solve_times']),\n                    'success_rate': algorithm_results['successes'] / len(test_problems)\n                })\n                \n                if algorithm_results['errors']:\n                    algorithm_results.update({\n                        'avg_error': np.mean(algorithm_results['errors']),\n                        'max_error': np.max(algorithm_results['errors'])\n                    })\n            \n            benchmark_results[algorithm_type.value] = algorithm_results\n        \n        self.logger.info(\"Benchmark completed\")\n        return benchmark_results\n\n\nclass AlgorithmSelector:\n    \"\"\"Intelligent algorithm selection based on problem characteristics.\"\"\"\n    \n    def __init__(self, performance_mode: str = 'balanced'):\n        \"\"\"Initialize algorithm selector.\n        \n        Args:\n            performance_mode: Optimization mode ('speed', 'accuracy', 'energy', 'balanced')\n        \"\"\"\n        self.logger = get_logger('algorithm_selector')\n        self.performance_mode = performance_mode\n        \n        # Performance mode weights (speed, accuracy, energy)\n        self.mode_weights = {\n            'speed': (0.8, 0.1, 0.1),\n            'accuracy': (0.1, 0.8, 0.1),\n            'energy': (0.1, 0.1, 0.8),\n            'balanced': (0.33, 0.33, 0.34)\n        }\n    \n    def recommend_algorithm(\n        self,\n        characteristics: ProblemCharacteristics,\n        available_algorithms: Dict[AlgorithmType, Any],\n        performance_history: Dict[AlgorithmType, List[Dict[str, Any]]]\n    ) -> AlgorithmRecommendation:\n        \"\"\"Recommend optimal algorithm based on problem characteristics.\n        \n        Args:\n            characteristics: Problem characteristics\n            available_algorithms: Available algorithm instances\n            performance_history: Historical performance data\n            \n        Returns:\n            Algorithm recommendation\n        \"\"\"\n        \n        # Score each available algorithm\n        algorithm_scores = {}\n        \n        for algorithm_type in available_algorithms.keys():\n            score = self._score_algorithm(algorithm_type, characteristics, performance_history)\n            algorithm_scores[algorithm_type] = score\n        \n        # Select highest scoring algorithm\n        if algorithm_scores:\n            best_algorithm = max(algorithm_scores.keys(), key=lambda alg: algorithm_scores[alg]['total_score'])\n            best_score = algorithm_scores[best_algorithm]\n            \n            return AlgorithmRecommendation(\n                algorithm_type=best_algorithm,\n                confidence=best_score['confidence'],\n                estimated_speedup=best_score['estimated_speedup'],\n                estimated_energy_savings=best_score['estimated_energy_savings'],\n                estimated_accuracy=best_score['estimated_accuracy'],\n                reasoning=best_score['reasoning'],\n                configuration=best_score['configuration']\n            )\n        else:\n            # Fallback to base algorithm\n            return AlgorithmRecommendation(\n                algorithm_type=AlgorithmType.BASE_ANALOG,\n                confidence=0.5,\n                estimated_speedup=1.0,\n                estimated_energy_savings=0.0,\n                estimated_accuracy=characteristics.accuracy_requirement,\n                reasoning=\"No algorithms available, using base solver\",\n                configuration={}\n            )\n    \n    def _score_algorithm(\n        self,\n        algorithm_type: AlgorithmType,\n        characteristics: ProblemCharacteristics,\n        performance_history: Dict[AlgorithmType, List[Dict[str, Any]]]\n    ) -> Dict[str, Any]:\n        \"\"\"Score algorithm suitability for given problem characteristics.\"\"\"\n        \n        # Base scores\n        speed_score = 0.5\n        accuracy_score = 0.5\n        energy_score = 0.5\n        confidence = 0.5\n        estimated_speedup = 1.0\n        estimated_energy_savings = 0.0\n        estimated_accuracy = characteristics.accuracy_requirement\n        reasoning_parts = []\n        \n        # Algorithm-specific scoring\n        if algorithm_type == AlgorithmType.NEUROMORPHIC:\n            # Neuromorphic excels for sparse problems\n            if characteristics.sparsity_level > 0.9:\n                speed_score = 0.95\n                energy_score = 0.98\n                confidence = 0.9\n                estimated_speedup = 1000.0 * characteristics.sparsity_level\n                estimated_energy_savings = 0.999 * characteristics.sparsity_level\n                reasoning_parts.append(\"High sparsity favors neuromorphic\")\n            elif characteristics.sparsity_level > 0.5:\n                speed_score = 0.7\n                energy_score = 0.8\n                confidence = 0.7\n                estimated_speedup = 10.0 * characteristics.sparsity_level\n                estimated_energy_savings = 0.5 * characteristics.sparsity_level\n                reasoning_parts.append(\"Moderate sparsity benefits from neuromorphic\")\n            else:\n                speed_score = 0.2\n                energy_score = 0.3\n                confidence = 0.3\n                reasoning_parts.append(\"Low sparsity not suitable for neuromorphic\")\n                \n        elif algorithm_type == AlgorithmType.TEMPORAL_CASCADE:\n            # Temporal cascade excels for time-dependent problems\n            if characteristics.time_dependent:\n                speed_score = 0.9\n                confidence = 0.85\n                estimated_speedup = 100.0\n                reasoning_parts.append(\"Time-dependent problem ideal for temporal cascade\")\n                \n                # Better for larger problems\n                problem_size = np.prod(characteristics.problem_size)\n                if problem_size > 1000:\n                    speed_score = 0.95\n                    confidence = 0.9\n                    reasoning_parts.append(\"Large time-dependent problem\")\n            else:\n                speed_score = 0.3\n                confidence = 0.2\n                reasoning_parts.append(\"Not time-dependent, temporal cascade not beneficial\")\n                \n        elif algorithm_type == AlgorithmType.MULTI_PHYSICS:\n            # Multi-physics coupling for coupled problems\n            if characteristics.multi_physics:\n                speed_score = 0.8\n                accuracy_score = 0.9\n                confidence = 0.8\n                estimated_speedup = 10.0\n                reasoning_parts.append(\"Multi-physics problem benefits from direct coupling\")\n                \n                if characteristics.conservation_required:\n                    accuracy_score = 0.95\n                    confidence = 0.85\n                    reasoning_parts.append(\"Conservation requirements well-handled\")\n            else:\n                speed_score = 0.1\n                confidence = 0.1\n                reasoning_parts.append(\"Single-physics problem doesn't need multi-physics coupling\")\n                \n        elif algorithm_type == AlgorithmType.ML_ACCELERATED:\n            # ML acceleration good for repeated similar problems\n            if algorithm_type in performance_history and len(performance_history[algorithm_type]) > 10:\n                speed_score = 0.8\n                confidence = 0.7\n                estimated_speedup = 50.0\n                reasoning_parts.append(\"ML surrogate trained on similar problems\")\n            else:\n                speed_score = 0.4\n                confidence = 0.3\n                reasoning_parts.append(\"ML surrogate not yet trained\")\n                \n        elif algorithm_type == AlgorithmType.HETEROGENEOUS_PRECISION:\n            # Heterogeneous precision good for multi-scale problems\n            problem_size = np.prod(characteristics.problem_size)\n            if problem_size > 10000:  # Large problems benefit more\n                energy_score = 0.8\n                speed_score = 0.7\n                confidence = 0.7\n                estimated_energy_savings = 0.5\n                reasoning_parts.append(\"Large problem benefits from precision adaptation\")\n            else:\n                energy_score = 0.6\n                speed_score = 0.5\n                confidence = 0.5\n                reasoning_parts.append(\"Moderate benefits for smaller problems\")\n                \n        elif algorithm_type == AlgorithmType.PHYSICS_INFORMED:\n            # Physics-informed good when physics constraints are important\n            if len(characteristics.physics_constraints) > 0:\n                accuracy_score = 0.9\n                confidence = 0.8\n                reasoning_parts.append(\"Physics constraints important\")\n            \n            if characteristics.conservation_required:\n                accuracy_score = 0.95\n                confidence = 0.85\n                reasoning_parts.append(\"Conservation laws enforced in hardware\")\n                \n        # Apply performance mode weights\n        speed_weight, accuracy_weight, energy_weight = self.mode_weights[self.performance_mode]\n        total_score = (speed_weight * speed_score + \n                      accuracy_weight * accuracy_score + \n                      energy_weight * energy_score)\n        \n        # Adjust based on historical performance\n        if algorithm_type in performance_history:\n            recent_records = [r for r in performance_history[algorithm_type] \n                            if time.time() - r['timestamp'] < 3600]  # Last hour\n            \n            if recent_records:\n                avg_time = np.mean([r['solve_time'] for r in recent_records])\n                if avg_time < 1.0:  # Fast recent performance\n                    total_score += 0.1\n                    confidence += 0.1\n                    reasoning_parts.append(\"Good recent performance\")\n        \n        return {\n            'total_score': total_score,\n            'speed_score': speed_score,\n            'accuracy_score': accuracy_score,\n            'energy_score': energy_score,\n            'confidence': min(confidence, 1.0),\n            'estimated_speedup': estimated_speedup,\n            'estimated_energy_savings': estimated_energy_savings,\n            'estimated_accuracy': estimated_accuracy,\n            'reasoning': '; '.join(reasoning_parts) if reasoning_parts else \"Default scoring\",\n            'configuration': {}  # Algorithm-specific configuration\n        }\n\n\nclass PerformanceTracker:\n    \"\"\"Track performance metrics across all algorithms.\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize performance tracker.\"\"\"\n        self.logger = get_logger('performance_tracker')\n        self.metrics = defaultdict(list)\n        self.start_times = {}\n        self.thread_lock = threading.Lock()\n    \n    def start_tracking(self, identifier: str) -> None:\n        \"\"\"Start tracking performance for identifier.\"\"\"\n        with self.thread_lock:\n            self.start_times[identifier] = time.time()\n    \n    def end_tracking(self, identifier: str, additional_metrics: Dict[str, Any] = None) -> float:\n        \"\"\"End tracking and record metrics.\"\"\"\n        end_time = time.time()\n        \n        with self.thread_lock:\n            if identifier in self.start_times:\n                duration = end_time - self.start_times[identifier]\n                \n                metric_record = {\n                    'timestamp': end_time,\n                    'duration': duration\n                }\n                \n                if additional_metrics:\n                    metric_record.update(additional_metrics)\n                \n                self.metrics[identifier].append(metric_record)\n                \n                # Clean up\n                del self.start_times[identifier]\n                \n                return duration\n            else:\n                self.logger.warning(f\"No start time found for identifier: {identifier}\")\n                return 0.0\n    \n    def get_performance_summary(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"Get performance summary for all tracked identifiers.\"\"\"\n        summary = {}\n        \n        with self.thread_lock:\n            for identifier, records in self.metrics.items():\n                if records:\n                    durations = [r['duration'] for r in records]\n                    summary[identifier] = {\n                        'count': len(records),\n                        'avg_duration': np.mean(durations),\n                        'min_duration': np.min(durations),\n                        'max_duration': np.max(durations),\n                        'std_duration': np.std(durations),\n                        'total_duration': np.sum(durations)\n                    }\n        \n        return summary"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/research/neuromorphic_acceleration.py",
      "line_number": 1,
      "details": "\"\"\"Neuromorphic PDE Acceleration (NPA) for ultra-low power sparse PDE solving.\n\nThis module implements spike-based neuromorphic architectures for sparse PDE solving\nwith extreme energy efficiency for sparse problems.\n\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple, Callable, Union\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\nfrom collections import defaultdict, deque\nfrom ..core.solver import AnalogPDESolver\nfrom ..core.crossbar import AnalogCrossbarArray\nfrom ..utils.logger import get_logger, PerformanceLogger\n\n\nclass SpikeEncoding(Enum):\n    \"\"\"Spike encoding schemes for neuromorphic computation.\"\"\"\n    RATE = \"rate\"           # Rate-based encoding\n    TEMPORAL = \"temporal\"   # Temporal encoding\n    POPULATION = \"population\"  # Population encoding\n    DELTA = \"delta\"         # Delta modulation\n    RANK_ORDER = \"rank_order\"  # Rank order encoding\n\n\n@dataclass\nclass SpikeEvent:\n    \"\"\"Individual spike event in neuromorphic system.\"\"\"\n    timestamp: float\n    neuron_id: int\n    spike_value: float\n    metadata: Optional[Dict[str, Any]] = None\n\n\n@dataclass\nclass NeuronState:\n    \"\"\"State of a neuromorphic neuron.\"\"\"\n    membrane_potential: float\n    threshold: float\n    leak_rate: float\n    refractory_period: float\n    last_spike_time: float\n    spike_count: int\n    accumulated_input: float\n\n\nclass SparseEventBuffer:\n    \"\"\"Event-driven sparse data buffer for neuromorphic processing.\"\"\"\n    \n    def __init__(self, capacity: int = 10000):\n        \"\"\"Initialize sparse event buffer.\n        \n        Args:\n            capacity: Maximum number of events to store\n        \"\"\"\n        self.capacity = capacity\n        self.events = deque(maxlen=capacity)\n        self.active_neurons = set()\n        self.event_statistics = defaultdict(int)\n        \n    def add_event(self, event: SpikeEvent) -> None:\n        \"\"\"Add spike event to buffer.\"\"\"\n        self.events.append(event)\n        self.active_neurons.add(event.neuron_id)\n        self.event_statistics['total_events'] += 1\n        self.event_statistics[f'neuron_{event.neuron_id}'] += 1\n    \n    def get_events_in_window(\n        self,\n        start_time: float,\n        end_time: float,\n        neuron_ids: Optional[List[int]] = None\n    ) -> List[SpikeEvent]:\n        \"\"\"Get events within time window.\"\"\"\n        filtered_events = []\n        \n        for event in self.events:\n            if start_time <= event.timestamp <= end_time:\n                if neuron_ids is None or event.neuron_id in neuron_ids:\n                    filtered_events.append(event)\n        \n        return filtered_events\n    \n    def get_sparsity_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get sparsity statistics.\"\"\"\n        if not self.events:\n            return {'sparsity': 1.0, 'active_fraction': 0.0, 'event_rate': 0.0}\n        \n        total_possible_neurons = max(self.active_neurons) + 1 if self.active_neurons else 1\n        active_fraction = len(self.active_neurons) / total_possible_neurons\n        \n        time_span = max(event.timestamp for event in self.events) - min(event.timestamp for event in self.events)\n        event_rate = len(self.events) / max(time_span, 1e-6)\n        \n        return {\n            'sparsity': 1.0 - active_fraction,\n            'active_fraction': active_fraction,\n            'event_rate': event_rate,\n            'total_events': len(self.events),\n            'active_neurons': len(self.active_neurons)\n        }\n\n\nclass NeuromorphicSpikeEncoder:\n    \"\"\"Encoder for converting PDE data to spike trains.\"\"\"\n    \n    def __init__(\n        self,\n        encoding_scheme: SpikeEncoding = SpikeEncoding.RATE,\n        time_window: float = 1.0,\n        spike_threshold: float = 0.5,\n        max_spike_rate: float = 1000.0\n    ):\n        \"\"\"Initialize spike encoder.\n        \n        Args:\n            encoding_scheme: Spike encoding method\n            time_window: Time window for encoding\n            spike_threshold: Threshold for spike generation\n            max_spike_rate: Maximum spike rate (Hz)\n        \"\"\"\n        self.logger = get_logger('spike_encoder')\n        \n        self.encoding_scheme = encoding_scheme\n        self.time_window = time_window\n        self.spike_threshold = spike_threshold\n        self.max_spike_rate = max_spike_rate\n        \n        self.encoding_statistics = defaultdict(float)\n    \n    def encode_data(\n        self,\n        data: np.ndarray,\n        current_time: float\n    ) -> List[SpikeEvent]:\n        \"\"\"Encode data array into spike events.\n        \n        Args:\n            data: Input data to encode\n            current_time: Current simulation time\n            \n        Returns:\n            List of spike events\n        \"\"\"\n        if self.encoding_scheme == SpikeEncoding.RATE:\n            return self._rate_encode(data, current_time)\n        elif self.encoding_scheme == SpikeEncoding.TEMPORAL:\n            return self._temporal_encode(data, current_time)\n        elif self.encoding_scheme == SpikeEncoding.POPULATION:\n            return self._population_encode(data, current_time)\n        elif self.encoding_scheme == SpikeEncoding.DELTA:\n            return self._delta_encode(data, current_time)\n        elif self.encoding_scheme == SpikeEncoding.RANK_ORDER:\n            return self._rank_order_encode(data, current_time)\n        else:\n            return self._rate_encode(data, current_time)  # Default\n    \n    def _rate_encode(self, data: np.ndarray, current_time: float) -> List[SpikeEvent]:\n        \"\"\"Rate-based spike encoding.\"\"\"\n        events = []\n        flattened_data = data.flatten()\n        \n        # Normalize data to [0, 1] range\n        if np.max(np.abs(flattened_data)) > 0:\n            normalized_data = np.abs(flattened_data) / np.max(np.abs(flattened_data))\n        else:\n            normalized_data = flattened_data\n        \n        for neuron_id, value in enumerate(normalized_data):\n            # Skip very small values (sparsity)\n            if value < self.spike_threshold * 0.1:\n                continue\n            \n            # Convert value to spike rate\n            spike_rate = value * self.max_spike_rate\n            \n            # Generate spikes based on Poisson process\n            dt = 1.0 / self.max_spike_rate  # Time resolution\n            num_spikes = int(spike_rate * self.time_window)\n            \n            for spike_idx in range(num_spikes):\n                # Random spike timing within window\n                spike_time = current_time + np.random.random() * self.time_window\n                \n                event = SpikeEvent(\n                    timestamp=spike_time,\n                    neuron_id=neuron_id,\n                    spike_value=value,\n                    metadata={'encoding': 'rate', 'original_value': flattened_data[neuron_id]}\n                )\n                events.append(event)\n        \n        self.encoding_statistics['rate_events'] += len(events)\n        return events\n    \n    def _temporal_encode(self, data: np.ndarray, current_time: float) -> List[SpikeEvent]:\n        \"\"\"Temporal spike encoding - timing carries information.\"\"\"\n        events = []\n        flattened_data = data.flatten()\n        \n        # Sort values to determine spike timing order\n        sorted_indices = np.argsort(np.abs(flattened_data))[::-1]  # Descending order\n        \n        for rank, neuron_id in enumerate(sorted_indices):\n            value = flattened_data[neuron_id]\n            \n            # Skip very small values\n            if np.abs(value) < self.spike_threshold:\n                continue\n            \n            # Earlier spikes for larger values\n            spike_time = current_time + (rank / len(sorted_indices)) * self.time_window\n            \n            event = SpikeEvent(\n                timestamp=spike_time,\n                neuron_id=neuron_id,\n                spike_value=value,\n                metadata={'encoding': 'temporal', 'rank': rank}\n            )\n            events.append(event)\n        \n        self.encoding_statistics['temporal_events'] += len(events)\n        return events\n    \n    def _population_encode(self, data: np.ndarray, current_time: float) -> List[SpikeEvent]:\n        \"\"\"Population encoding using multiple neurons per value.\"\"\"\n        events = []\n        flattened_data = data.flatten()\n        neurons_per_value = 4  # Use 4 neurons to represent each value\n        \n        for value_idx, value in enumerate(flattened_data):\n            if np.abs(value) < self.spike_threshold:\n                continue\n            \n            # Normalize value to [0, 1]\n            normalized_value = (value + np.max(np.abs(flattened_data))) / (2 * np.max(np.abs(flattened_data)))\n            \n            for neuron_offset in range(neurons_per_value):\n                neuron_id = value_idx * neurons_per_value + neuron_offset\n                \n                # Each neuron has different activation profile\n                activation_center = neuron_offset / neurons_per_value\n                activation_width = 1.0 / neurons_per_value\n                \n                # Gaussian activation\n                activation = np.exp(-((normalized_value - activation_center) / activation_width)**2)\n                \n                if activation > self.spike_threshold:\n                    # Generate spikes based on activation\n                    num_spikes = int(activation * 10)  # Max 10 spikes\n                    \n                    for spike_idx in range(num_spikes):\n                        spike_time = current_time + np.random.random() * self.time_window\n                        \n                        event = SpikeEvent(\n                            timestamp=spike_time,\n                            neuron_id=neuron_id,\n                            spike_value=activation,\n                            metadata={'encoding': 'population', 'value_idx': value_idx, 'neuron_offset': neuron_offset}\n                        )\n                        events.append(event)\n        \n        self.encoding_statistics['population_events'] += len(events)\n        return events\n    \n    def _delta_encode(self, data: np.ndarray, current_time: float) -> List[SpikeEvent]:\n        \"\"\"Delta encoding - only encode changes.\"\"\"\n        events = []\n        flattened_data = data.flatten()\n        \n        # Store previous data for delta computation\n        if not hasattr(self, '_previous_data'):\n            self._previous_data = np.zeros_like(flattened_data)\n        \n        # Compute delta\n        delta = flattened_data - self._previous_data\n        self._previous_data = flattened_data.copy()\n        \n        for neuron_id, delta_value in enumerate(delta):\n            if np.abs(delta_value) > self.spike_threshold:\n                # Positive delta -> positive spike, negative delta -> negative spike\n                spike_time = current_time + np.random.random() * self.time_window\n                \n                event = SpikeEvent(\n                    timestamp=spike_time,\n                    neuron_id=neuron_id,\n                    spike_value=delta_value,\n                    metadata={'encoding': 'delta', 'delta_magnitude': np.abs(delta_value)}\n                )\n                events.append(event)\n        \n        self.encoding_statistics['delta_events'] += len(events)\n        return events\n    \n    def _rank_order_encode(self, data: np.ndarray, current_time: float) -> List[SpikeEvent]:\n        \"\"\"Rank order encoding - first spike timing indicates magnitude.\"\"\"\n        events = []\n        flattened_data = data.flatten()\n        \n        # Sort by magnitude\n        sorted_indices = np.argsort(np.abs(flattened_data))[::-1]\n        \n        for rank, neuron_id in enumerate(sorted_indices):\n            value = flattened_data[neuron_id]\n            \n            if np.abs(value) < self.spike_threshold:\n                break  # Skip remaining smaller values\n            \n            # First spike time inversely related to magnitude\n            spike_delay = (rank / len(sorted_indices)) * self.time_window\n            spike_time = current_time + spike_delay\n            \n            event = SpikeEvent(\n                timestamp=spike_time,\n                neuron_id=neuron_id,\n                spike_value=value,\n                metadata={'encoding': 'rank_order', 'rank': rank}\n            )\n            events.append(event)\n        \n        self.encoding_statistics['rank_order_events'] += len(events)\n        return events\n\n\nclass NeuromorphicSpikeDecoder:\n    \"\"\"Decoder for converting spike trains back to PDE data.\"\"\"\n    \n    def __init__(\n        self,\n        decoding_scheme: SpikeEncoding = SpikeEncoding.RATE,\n        time_window: float = 1.0,\n        output_size: int = 64\n    ):\n        \"\"\"Initialize spike decoder.\n        \n        Args:\n            decoding_scheme: Spike decoding method\n            time_window: Time window for decoding\n            output_size: Size of output data array\n        \"\"\"\n        self.logger = get_logger('spike_decoder')\n        \n        self.decoding_scheme = decoding_scheme\n        self.time_window = time_window\n        self.output_size = output_size\n        \n        self.decoding_statistics = defaultdict(float)\n    \n    def decode_events(\n        self,\n        events: List[SpikeEvent],\n        current_time: float\n    ) -> np.ndarray:\n        \"\"\"Decode spike events into data array.\n        \n        Args:\n            events: List of spike events to decode\n            current_time: Current simulation time\n            \n        Returns:\n            Decoded data array\n        \"\"\"\n        if self.decoding_scheme == SpikeEncoding.RATE:\n            return self._rate_decode(events, current_time)\n        elif self.decoding_scheme == SpikeEncoding.TEMPORAL:\n            return self._temporal_decode(events, current_time)\n        elif self.decoding_scheme == SpikeEncoding.POPULATION:\n            return self._population_decode(events, current_time)\n        elif self.decoding_scheme == SpikeEncoding.DELTA:\n            return self._delta_decode(events, current_time)\n        elif self.decoding_scheme == SpikeEncoding.RANK_ORDER:\n            return self._rank_order_decode(events, current_time)\n        else:\n            return self._rate_decode(events, current_time)  # Default\n    \n    def _rate_decode(self, events: List[SpikeEvent], current_time: float) -> np.ndarray:\n        \"\"\"Rate-based spike decoding.\"\"\"\n        output = np.zeros(self.output_size)\n        spike_counts = defaultdict(int)\n        \n        # Count spikes in time window\n        window_start = current_time - self.time_window\n        \n        for event in events:\n            if window_start <= event.timestamp <= current_time:\n                if event.neuron_id < self.output_size:\n                    spike_counts[event.neuron_id] += 1\n        \n        # Convert spike counts to values\n        for neuron_id, count in spike_counts.items():\n            # Normalize by time window\n            rate = count / self.time_window\n            output[neuron_id] = rate\n        \n        return output\n    \n    def _temporal_decode(self, events: List[SpikeEvent], current_time: float) -> np.ndarray:\n        \"\"\"Temporal spike decoding.\"\"\"\n        output = np.zeros(self.output_size)\n        \n        # Group events by neuron\n        neuron_events = defaultdict(list)\n        window_start = current_time - self.time_window\n        \n        for event in events:\n            if window_start <= event.timestamp <= current_time and event.neuron_id < self.output_size:\n                neuron_events[event.neuron_id].append(event)\n        \n        # Decode based on first spike timing\n        for neuron_id, neuron_event_list in neuron_events.items():\n            if neuron_event_list:\n                # Find earliest spike\n                earliest_event = min(neuron_event_list, key=lambda e: e.timestamp)\n                \n                # Convert timing to magnitude (earlier = larger)\n                relative_time = earliest_event.timestamp - window_start\n                normalized_time = relative_time / self.time_window\n                \n                # Invert: earlier spikes (smaller time) -> larger values\n                output[neuron_id] = 1.0 - normalized_time\n        \n        return output\n    \n    def _population_decode(self, events: List[SpikeEvent], current_time: float) -> np.ndarray:\n        \"\"\"Population spike decoding.\"\"\"\n        neurons_per_value = 4\n        num_values = self.output_size\n        output = np.zeros(num_values)\n        \n        window_start = current_time - self.time_window\n        \n        # Group spikes by value index\n        for value_idx in range(num_values):\n            population_activity = []\n            \n            for neuron_offset in range(neurons_per_value):\n                neuron_id = value_idx * neurons_per_value + neuron_offset\n                \n                # Count spikes for this neuron in time window\n                spike_count = sum(1 for event in events \n                                if (window_start <= event.timestamp <= current_time and \n                                    event.neuron_id == neuron_id))\n                \n                population_activity.append(spike_count)\n            \n            # Decode population activity to single value\n            if population_activity:\n                # Weighted average based on neuron position\n                weights = np.array([i / neurons_per_value for i in range(neurons_per_value)])\n                weighted_activity = np.array(population_activity) * weights\n                \n                if np.sum(population_activity) > 0:\n                    output[value_idx] = np.sum(weighted_activity) / np.sum(population_activity)\n        \n        return output\n    \n    def _delta_decode(self, events: List[SpikeEvent], current_time: float) -> np.ndarray:\n        \"\"\"Delta spike decoding.\"\"\"\n        delta_output = np.zeros(self.output_size)\n        window_start = current_time - self.time_window\n        \n        # Accumulate delta values\n        for event in events:\n            if window_start <= event.timestamp <= current_time and event.neuron_id < self.output_size:\n                delta_output[event.neuron_id] += event.spike_value\n        \n        # Integrate delta to get absolute values\n        if not hasattr(self, '_integrated_output'):\n            self._integrated_output = np.zeros(self.output_size)\n        \n        self._integrated_output += delta_output\n        \n        return self._integrated_output.copy()\n    \n    def _rank_order_decode(self, events: List[SpikeEvent], current_time: float) -> np.ndarray:\n        \"\"\"Rank order spike decoding.\"\"\"\n        output = np.zeros(self.output_size)\n        window_start = current_time - self.time_window\n        \n        # Find first spike for each neuron\n        first_spikes = {}\n        \n        for event in events:\n            if window_start <= event.timestamp <= current_time and event.neuron_id < self.output_size:\n                if event.neuron_id not in first_spikes or event.timestamp < first_spikes[event.neuron_id].timestamp:\n                    first_spikes[event.neuron_id] = event\n        \n        # Sort by spike timing to get rank order\n        sorted_spikes = sorted(first_spikes.values(), key=lambda e: e.timestamp)\n        \n        # Assign values based on rank (earlier = higher value)\n        for rank, event in enumerate(sorted_spikes):\n            # Higher rank (later timing) gets lower value\n            value = 1.0 - (rank / len(sorted_spikes)) if sorted_spikes else 0.0\n            output[event.neuron_id] = value\n        \n        return output\n\n\nclass NeuromorphicPDESolver:\n    \"\"\"Neuromorphic PDE Acceleration (NPA) system.\n    \n    Ultra-low power sparse PDE solving using spike-based neuromorphic architectures\n    with extreme energy efficiency for sparse problems.\n    \"\"\"\n    \n    def __init__(\n        self,\n        base_solver: AnalogPDESolver,\n        spike_encoder: NeuromorphicSpikeEncoder = None,\n        spike_decoder: NeuromorphicSpikeDecoder = None,\n        sparsity_threshold: float = 0.9,\n        max_neurons: int = 1024\n    ):\n        \"\"\"Initialize NPA system.\n        \n        Args:\n            base_solver: Base analog PDE solver\n            spike_encoder: Spike encoding system\n            spike_decoder: Spike decoding system\n            sparsity_threshold: Minimum sparsity to activate neuromorphic mode\n            max_neurons: Maximum number of neurons\n        \"\"\"\n        self.logger = get_logger('npa')\n        self.perf_logger = PerformanceLogger(self.logger)\n        \n        self.base_solver = base_solver\n        self.sparsity_threshold = sparsity_threshold\n        self.max_neurons = max_neurons\n        \n        # Initialize encoder/decoder if not provided\n        self.spike_encoder = spike_encoder or NeuromorphicSpikeEncoder()\n        self.spike_decoder = spike_decoder or NeuromorphicSpikeDecoder()\n        \n        # Neuromorphic components\n        self.event_buffer = SparseEventBuffer()\n        self.neuron_states = {i: NeuronState(0.0, 1.0, 0.1, 0.001, 0.0, 0, 0.0) \n                             for i in range(max_neurons)}\n        \n        # Performance tracking\n        self.energy_savings = []\n        self.sparsity_levels = []\n        self.neuromorphic_activations = 0\n        self.analog_fallbacks = 0\n        \n        self.logger.info(f\"Initialized NPA with {max_neurons} neurons, sparsity threshold: {sparsity_threshold}\")\n    \n    def solve_sparse_pde(\n        self,\n        pde,\n        initial_solution: np.ndarray,\n        time_span: Tuple[float, float],\n        num_time_steps: int,\n        adaptive_mode: bool = True\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Solve sparse PDE using neuromorphic acceleration.\n        \n        Args:\n            pde: PDE problem to solve\n            initial_solution: Initial solution state\n            time_span: Time integration span\n            num_time_steps: Number of time steps\n            adaptive_mode: Whether to adaptively switch between neuromorphic and analog\n            \n        Returns:\n            Tuple of (final_solution, neuromorphic_metrics)\n        \"\"\"\n        self.perf_logger.start_timer('neuromorphic_solve')\n        \n        t_start, t_end = time_span\n        dt = (t_end - t_start) / num_time_steps\n        current_time = t_start\n        current_solution = initial_solution.copy()\n        \n        neuromorphic_metrics = {\n            'neuromorphic_steps': 0,\n            'analog_fallback_steps': 0,\n            'average_sparsity': 0.0,\n            'total_energy_savings': 0.0,\n            'spike_statistics': {},\n            'adaptation_decisions': []\n        }\n        \n        sparsity_history = []\n        energy_savings_history = []\n        \n        for step in range(num_time_steps):\n            # Analyze current solution sparsity\n            sparsity_level = self._compute_sparsity(current_solution)\n            sparsity_history.append(sparsity_level)\n            \n            # Decide whether to use neuromorphic or analog solver\n            use_neuromorphic = sparsity_level >= self.sparsity_threshold\n            \n            if adaptive_mode:\n                # Additional criteria for neuromorphic activation\n                solution_magnitude = np.linalg.norm(current_solution)\n                use_neuromorphic = (use_neuromorphic and \n                                  solution_magnitude > 1e-6 and\n                                  len(self.event_buffer.active_neurons) < self.max_neurons * 0.8)\n            \n            neuromorphic_metrics['adaptation_decisions'].append({\n                'step': step,\n                'sparsity': sparsity_level,\n                'use_neuromorphic': use_neuromorphic,\n                'active_neurons': len(self.event_buffer.active_neurons)\n            })\n            \n            if use_neuromorphic:\n                # Solve using neuromorphic acceleration\n                new_solution, step_metrics = self._solve_neuromorphic_step(\n                    pde, current_solution, dt, current_time\n                )\n                neuromorphic_metrics['neuromorphic_steps'] += 1\n                self.neuromorphic_activations += 1\n                \n                # Estimate energy savings\n                energy_saving = self._estimate_energy_savings(sparsity_level)\n                energy_savings_history.append(energy_saving)\n                \n            else:\n                # Fall back to analog solver\n                new_solution = self._solve_analog_step(\n                    pde, current_solution, dt, current_time\n                )\n                step_metrics = {'method': 'analog_fallback'}\n                neuromorphic_metrics['analog_fallback_steps'] += 1\n                self.analog_fallbacks += 1\n                energy_savings_history.append(0.0)\n            \n            current_solution = new_solution\n            current_time += dt\n            \n            # Update metrics periodically\n            if step % max(1, num_time_steps // 10) == 0:\n                self.logger.debug(f\"NPA step {step}/{num_time_steps}: sparsity={sparsity_level:.3f}, neuromorphic={use_neuromorphic}\")\n        \n        solve_time = self.perf_logger.end_timer('neuromorphic_solve')\n        \n        # Compute final metrics\n        neuromorphic_metrics.update({\n            'total_solve_time': solve_time,\n            'average_sparsity': np.mean(sparsity_history),\n            'total_energy_savings': np.sum(energy_savings_history),\n            'neuromorphic_fraction': neuromorphic_metrics['neuromorphic_steps'] / num_time_steps,\n            'spike_statistics': self.event_buffer.get_sparsity_statistics()\n        })\n        \n        self.logger.info(f\"NPA solve completed: {neuromorphic_metrics['neuromorphic_fraction']:.1%} neuromorphic, {neuromorphic_metrics['total_energy_savings']:.2f} energy savings\")\n        \n        return current_solution, neuromorphic_metrics\n    \n    def _compute_sparsity(self, solution: np.ndarray) -> float:\n        \"\"\"Compute sparsity level of solution.\"\"\"\n        total_elements = solution.size\n        if total_elements == 0:\n            return 1.0\n        \n        # Count near-zero elements\n        threshold = np.max(np.abs(solution)) * 0.01 if np.max(np.abs(solution)) > 0 else 1e-10\n        near_zero_elements = np.sum(np.abs(solution) < threshold)\n        \n        sparsity = near_zero_elements / total_elements\n        return sparsity\n    \n    def _solve_neuromorphic_step(\n        self,\n        pde,\n        current_solution: np.ndarray,\n        dt: float,\n        current_time: float\n    ) -> Tuple[np.ndarray, Dict[str, Any]]:\n        \"\"\"Solve single time step using neuromorphic acceleration.\"\"\"\n        step_metrics = {'method': 'neuromorphic', 'spike_events': 0, 'active_neurons': 0}\n        \n        # Encode current solution to spike events\n        spike_events = self.spike_encoder.encode_data(current_solution, current_time)\n        step_metrics['spike_events'] = len(spike_events)\n        \n        # Add events to buffer\n        for event in spike_events:\n            self.event_buffer.add_event(event)\n        \n        # Process spikes through neuromorphic dynamics\n        processed_events = self._process_spike_dynamics(spike_events, dt)\n        step_metrics['processed_events'] = len(processed_events)\n        \n        # Decode spike events back to solution\n        new_solution = self.spike_decoder.decode_events(processed_events, current_time + dt)\n        \n        # Ensure solution maintains correct dimensions\n        if len(new_solution) != len(current_solution):\n            if len(new_solution) > len(current_solution):\n                new_solution = new_solution[:len(current_solution)]\n            else:\n                padded_solution = np.zeros_like(current_solution)\n                padded_solution[:len(new_solution)] = new_solution\n                new_solution = padded_solution\n        \n        step_metrics['active_neurons'] = len(self.event_buffer.active_neurons)\n        \n        return new_solution, step_metrics\n    \n    def _solve_analog_step(\n        self,\n        pde,\n        current_solution: np.ndarray,\n        dt: float,\n        current_time: float\n    ) -> np.ndarray:\n        \"\"\"Solve single time step using analog fallback.\"\"\"\n        # Simple forward Euler step (could be enhanced)\n        try:\n            # Use base analog solver for one iteration\n            solution = self.base_solver.solve(pde, iterations=1, convergence_threshold=1e-6)\n            \n            # If base solver returns wrong dimensions, interpolate\n            if len(solution) != len(current_solution):\n                if len(solution) > len(current_solution):\n                    solution = solution[:len(current_solution)]\n                else:\n                    padded_solution = np.zeros_like(current_solution)\n                    padded_solution[:len(solution)] = solution\n                    solution = padded_solution\n            \n            return solution\n            \n        except Exception as e:\n            self.logger.warning(f\"Analog fallback failed: {e}\")\n            return current_solution  # Return unchanged solution\n    \n    def _process_spike_dynamics(\n        self,\n        input_events: List[SpikeEvent],\n        dt: float\n    ) -> List[SpikeEvent]:\n        \"\"\"Process spike events through neuromorphic dynamics.\"\"\"\n        output_events = []\n        \n        # Update neuron states based on input spikes\n        for event in input_events:\n            if event.neuron_id in self.neuron_states:\n                neuron = self.neuron_states[event.neuron_id]\n                \n                # Check refractory period\n                time_since_last_spike = event.timestamp - neuron.last_spike_time\n                if time_since_last_spike < neuron.refractory_period:\n                    continue\n                \n                # Update membrane potential\n                neuron.accumulated_input += event.spike_value\n                neuron.membrane_potential += neuron.accumulated_input\n                \n                # Apply leak\n                neuron.membrane_potential *= (1.0 - neuron.leak_rate * dt)\n                \n                # Check for output spike\n                if neuron.membrane_potential > neuron.threshold:\n                    # Generate output spike\n                    output_event = SpikeEvent(\n                        timestamp=event.timestamp + dt * 0.1,  # Small delay\n                        neuron_id=event.neuron_id,\n                        spike_value=neuron.membrane_potential - neuron.threshold,\n                        metadata={\n                            'processed': True,\n                            'membrane_potential': neuron.membrane_potential\n                        }\n                    )\n                    output_events.append(output_event)\n                    \n                    # Reset neuron\n                    neuron.membrane_potential = 0.0\n                    neuron.last_spike_time = event.timestamp\n                    neuron.spike_count += 1\n                \n                # Reset accumulated input after processing\n                neuron.accumulated_input = 0.0\n        \n        return output_events\n    \n    def _estimate_energy_savings(self, sparsity_level: float) -> float:\n        \"\"\"Estimate energy savings from neuromorphic processing.\"\"\"\n        # Energy model: neuromorphic energy scales with activity, not problem size\n        base_analog_energy = 1.0  # Normalized base energy for analog computation\n        \n        # Neuromorphic energy depends on spike activity\n        activity_level = 1.0 - sparsity_level\n        neuromorphic_energy = activity_level * 0.01  # Very low energy for sparse activity\n        \n        # Additional savings from event-driven computation\n        event_efficiency = 0.001 if sparsity_level > 0.95 else 0.01\n        neuromorphic_energy *= event_efficiency\n        \n        energy_saving = base_analog_energy - neuromorphic_energy\n        return max(0.0, energy_saving)  # Ensure non-negative savings\n    \n    def get_neuromorphic_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get comprehensive neuromorphic processing statistics.\"\"\"\n        stats = {\n            'system_configuration': {\n                'max_neurons': self.max_neurons,\n                'sparsity_threshold': self.sparsity_threshold,\n                'encoding_scheme': self.spike_encoder.encoding_scheme.value,\n                'decoding_scheme': self.spike_decoder.decoding_scheme.value\n            },\n            'activation_statistics': {\n                'neuromorphic_activations': self.neuromorphic_activations,\n                'analog_fallbacks': self.analog_fallbacks,\n                'neuromorphic_fraction': self.neuromorphic_activations / max(1, self.neuromorphic_activations + self.analog_fallbacks)\n            },\n            'event_buffer_statistics': self.event_buffer.get_sparsity_statistics(),\n            'encoding_statistics': dict(self.spike_encoder.encoding_statistics),\n            'decoding_statistics': dict(self.spike_decoder.decoding_statistics),\n            'neuron_utilization': {\n                'total_neurons': self.max_neurons,\n                'active_neurons': len(self.event_buffer.active_neurons),\n                'utilization_fraction': len(self.event_buffer.active_neurons) / self.max_neurons\n            }\n        }\n        \n        # Energy analysis\n        if self.energy_savings:\n            stats['energy_analysis'] = {\n                'total_energy_savings': np.sum(self.energy_savings),\n                'average_energy_savings': np.mean(self.energy_savings),\n                'peak_energy_savings': np.max(self.energy_savings),\n                'energy_efficiency_ratio': np.sum(self.energy_savings) / max(1, len(self.energy_savings))\n            }\n        \n        # Sparsity analysis\n        if self.sparsity_levels:\n            stats['sparsity_analysis'] = {\n                'average_sparsity': np.mean(self.sparsity_levels),\n                'peak_sparsity': np.max(self.sparsity_levels),\n                'sparsity_std': np.std(self.sparsity_levels),\n                'high_sparsity_fraction': np.mean(np.array(self.sparsity_levels) > self.sparsity_threshold)\n            }\n        \n        return stats\n    \n    def optimize_neuromorphic_parameters(\n        self,\n        sample_data: List[np.ndarray],\n        target_energy_reduction: float = 0.9\n    ) -> Dict[str, Any]:\n        \"\"\"Optimize neuromorphic parameters for target performance.\"\"\"\n        optimization_results = {\n            'optimal_sparsity_threshold': self.sparsity_threshold,\n            'optimal_encoding_scheme': self.spike_encoder.encoding_scheme,\n            'optimization_metrics': {}\n        }\n        \n        # Test different sparsity thresholds\n        sparsity_thresholds = np.linspace(0.5, 0.99, 10)\n        encoding_schemes = list(SpikeEncoding)\n        \n        best_energy_reduction = 0.0\n        best_config = None\n        \n        for threshold in sparsity_thresholds:\n            for encoding_scheme in encoding_schemes:\n                # Create test configuration\n                test_encoder = NeuromorphicSpikeEncoder(encoding_scheme=encoding_scheme)\n                test_decoder = NeuromorphicSpikeDecoder(decoding_scheme=encoding_scheme)\n                \n                # Simulate processing with sample data\n                total_energy_saving = 0.0\n                total_samples = len(sample_data)\n                \n                for sample in sample_data:\n                    sample_sparsity = self._compute_sparsity(sample)\n                    \n                    if sample_sparsity >= threshold:\n                        # Would use neuromorphic processing\n                        energy_saving = self._estimate_energy_savings(sample_sparsity)\n                        total_energy_saving += energy_saving\n                \n                avg_energy_reduction = total_energy_saving / max(1, total_samples)\n                \n                # Check if this configuration meets target\n                if avg_energy_reduction > best_energy_reduction:\n                    best_energy_reduction = avg_energy_reduction\n                    best_config = {\n                        'sparsity_threshold': threshold,\n                        'encoding_scheme': encoding_scheme,\n                        'energy_reduction': avg_energy_reduction\n                    }\n        \n        if best_config and best_config['energy_reduction'] >= target_energy_reduction:\n            # Update system with optimal parameters\n            self.sparsity_threshold = best_config['sparsity_threshold']\n            self.spike_encoder.encoding_scheme = best_config['encoding_scheme']\n            self.spike_decoder.decoding_scheme = best_config['encoding_scheme']\n            \n            optimization_results.update({\n                'optimal_sparsity_threshold': best_config['sparsity_threshold'],\n                'optimal_encoding_scheme': best_config['encoding_scheme'],\n                'achieved_energy_reduction': best_config['energy_reduction'],\n                'optimization_successful': True\n            })\n            \n            self.logger.info(f\"Neuromorphic optimization successful: {best_config['energy_reduction']:.1%} energy reduction\")\n        else:\n            optimization_results['optimization_successful'] = False\n            self.logger.warning(f\"Failed to achieve target energy reduction of {target_energy_reduction:.1%}\")\n        \n        return optimization_results"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/research/validation_benchmark_suite.py",
      "line_number": 1,
      "details": "\"\"\"Validation and Benchmark Suite for Advanced Analog Algorithms.\n\nThis module implements comprehensive validation and benchmarking for all\nadvanced algorithms with statistical analysis and publication-ready results.\n\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple, Callable, Union\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\nimport json\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\n\nfrom ..core.solver import AnalogPDESolver\nfrom ..core.equations import PoissonEquation, HeatEquation, WaveEquation\nfrom ..utils.logger import get_logger, PerformanceLogger\n\nfrom .integrated_solver_framework import AdvancedSolverFramework, AlgorithmType, ProblemCharacteristics\nfrom .advanced_analog_algorithms import PrecisionLevel\nfrom .neuromorphic_acceleration import SpikeEncoding\n\n\nclass BenchmarkType(Enum):\n    \"\"\"Types of benchmarks to run.\"\"\"\n    ACCURACY = \"accuracy\"\n    PERFORMANCE = \"performance\" \n    ENERGY_EFFICIENCY = \"energy_efficiency\"\n    SCALABILITY = \"scalability\"\n    ROBUSTNESS = \"robustness\"\n    COMPARATIVE = \"comparative\"\n\n\n@dataclass\nclass BenchmarkProblem:\n    \"\"\"Standard benchmark problem definition.\"\"\"\n    name: str\n    pde_constructor: Callable\n    pde_kwargs: Dict[str, Any]\n    reference_solution: Optional[np.ndarray]\n    analytical_solution: Optional[Callable]\n    problem_size: Tuple[int, ...]\n    expected_sparsity: float\n    time_dependent: bool\n    multi_physics: bool\n    difficulty_level: str  # 'easy', 'medium', 'hard', 'extreme'\n\n\n@dataclass\nclass BenchmarkResult:\n    \"\"\"Results from a single benchmark run.\"\"\"\n    algorithm: AlgorithmType\n    problem_name: str\n    solve_time: float\n    accuracy: float\n    energy_estimate: float\n    memory_usage: float\n    convergence_achieved: bool\n    error_metrics: Dict[str, float]\n    additional_metrics: Dict[str, Any]\n\n\nclass ValidationBenchmarkSuite:\n    \"\"\"Comprehensive validation and benchmark suite for advanced algorithms.\"\"\"\n    \n    def __init__(\n        self,\n        framework: AdvancedSolverFramework,\n        output_directory: str = \"benchmark_results\",\n        statistical_significance: float = 0.05,\n        num_statistical_runs: int = 10\n    ):\n        \"\"\"Initialize benchmark suite.\n        \n        Args:\n            framework: Advanced solver framework to benchmark\n            output_directory: Directory for benchmark results\n            statistical_significance: P-value threshold for statistical tests\n            num_statistical_runs: Number of runs for statistical analysis\n        \"\"\"\n        self.logger = get_logger('benchmark_suite')\n        self.perf_logger = PerformanceLogger(self.logger)\n        \n        self.framework = framework\n        self.output_dir = Path(output_directory)\n        self.output_dir.mkdir(parents=True, exist_ok=True)\n        \n        self.statistical_significance = statistical_significance\n        self.num_statistical_runs = num_statistical_runs\n        \n        # Initialize benchmark problems\n        self.benchmark_problems = self._create_benchmark_problems()\n        \n        # Results storage\n        self.results = {}\n        self.statistical_results = {}\n        \n        self.logger.info(f\"Initialized benchmark suite with {len(self.benchmark_problems)} problems\")\n    \n    def _create_benchmark_problems(self) -> List[BenchmarkProblem]:\n        \"\"\"Create standard benchmark problems.\"\"\"\n        problems = []\n        \n        # 1. Simple Poisson problems\n        problems.extend([\n            BenchmarkProblem(\n                name=\"poisson_2d_small\",\n                pde_constructor=PoissonEquation,\n                pde_kwargs={\n                    'domain_size': (64, 64),\n                    'boundary_conditions': 'dirichlet',\n                    'source_function': lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n                },\n                reference_solution=None,\n                analytical_solution=lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y) / (2 * np.pi**2),\n                problem_size=(64, 64),\n                expected_sparsity=0.3,\n                time_dependent=False,\n                multi_physics=False,\n                difficulty_level='easy'\n            ),\n            \n            BenchmarkProblem(\n                name=\"poisson_2d_large\",\n                pde_constructor=PoissonEquation,\n                pde_kwargs={\n                    'domain_size': (256, 256),\n                    'boundary_conditions': 'dirichlet',\n                    'source_function': lambda x, y: np.exp(-(x**2 + y**2))\n                },\n                reference_solution=None,\n                analytical_solution=None,\n                problem_size=(256, 256),\n                expected_sparsity=0.1,\n                time_dependent=False,\n                multi_physics=False,\n                difficulty_level='medium'\n            ),\n            \n            BenchmarkProblem(\n                name=\"poisson_sparse\",\n                pde_constructor=PoissonEquation,\n                pde_kwargs={\n                    'domain_size': (128, 128),\n                    'boundary_conditions': 'dirichlet',\n                    'source_function': lambda x, y: (np.abs(x - 0.5) < 0.1) * (np.abs(y - 0.5) < 0.1)\n                },\n                reference_solution=None,\n                analytical_solution=None,\n                problem_size=(128, 128),\n                expected_sparsity=0.95,\n                time_dependent=False,\n                multi_physics=False,\n                difficulty_level='medium'\n            )\n        ])\n        \n        # 2. Heat equation problems\n        problems.extend([\n            BenchmarkProblem(\n                name=\"heat_1d_transient\",\n                pde_constructor=HeatEquation,\n                pde_kwargs={\n                    'domain_size': (128,),\n                    'boundary_conditions': 'dirichlet',\n                    'initial_condition': lambda x: np.sin(np.pi * x),\n                    'diffusivity': 0.1\n                },\n                reference_solution=None,\n                analytical_solution=lambda x, t: np.exp(-np.pi**2 * 0.1 * t) * np.sin(np.pi * x),\n                problem_size=(128,),\n                expected_sparsity=0.2,\n                time_dependent=True,\n                multi_physics=False,\n                difficulty_level='medium'\n            ),\n            \n            BenchmarkProblem(\n                name=\"heat_2d_gaussian\",\n                pde_constructor=HeatEquation,\n                pde_kwargs={\n                    'domain_size': (64, 64),\n                    'boundary_conditions': 'neumann',\n                    'initial_condition': lambda x, y: np.exp(-10 * ((x - 0.5)**2 + (y - 0.5)**2)),\n                    'diffusivity': 0.05\n                },\n                reference_solution=None,\n                analytical_solution=None,\n                problem_size=(64, 64),\n                expected_sparsity=0.4,\n                time_dependent=True,\n                multi_physics=False,\n                difficulty_level='medium'\n            )\n        ])\n        \n        # 3. Wave equation problems\n        problems.extend([\n            BenchmarkProblem(\n                name=\"wave_1d_oscillation\",\n                pde_constructor=WaveEquation,\n                pde_kwargs={\n                    'domain_size': (128,),\n                    'boundary_conditions': 'dirichlet',\n                    'initial_condition': lambda x: np.sin(2 * np.pi * x),\n                    'initial_velocity': lambda x: np.zeros_like(x),\n                    'wave_speed': 1.0\n                },\n                reference_solution=None,\n                analytical_solution=lambda x, t: np.sin(2 * np.pi * (x - t)),\n                problem_size=(128,),\n                expected_sparsity=0.1,\n                time_dependent=True,\n                multi_physics=False,\n                difficulty_level='medium'\n            )\n        ])\n        \n        # 4. Multi-scale problems\n        problems.extend([\n            BenchmarkProblem(\n                name=\"multiscale_poisson\",\n                pde_constructor=PoissonEquation,\n                pde_kwargs={\n                    'domain_size': (256, 256),\n                    'boundary_conditions': 'mixed',\n                    'source_function': lambda x, y: (np.sin(10 * np.pi * x) * np.sin(10 * np.pi * y) +\n                                                   0.1 * np.sin(100 * np.pi * x) * np.sin(100 * np.pi * y))\n                },\n                reference_solution=None,\n                analytical_solution=None,\n                problem_size=(256, 256),\n                expected_sparsity=0.05,\n                time_dependent=False,\n                multi_physics=False,\n                difficulty_level='hard'\n            )\n        ])\n        \n        # 5. Extreme challenge problems\n        problems.extend([\n            BenchmarkProblem(\n                name=\"extreme_sparse_poisson\",\n                pde_constructor=PoissonEquation,\n                pde_kwargs={\n                    'domain_size': (512, 512),\n                    'boundary_conditions': 'dirichlet',\n                    'source_function': lambda x, y: ((x - 0.25)**2 + (y - 0.25)**2 < 0.01) * 1000.0\n                },\n                reference_solution=None,\n                analytical_solution=None,\n                problem_size=(512, 512),\n                expected_sparsity=0.99,\n                time_dependent=False,\n                multi_physics=False,\n                difficulty_level='extreme'\n            ),\n            \n            BenchmarkProblem(\n                name=\"extreme_multiscale_heat\",\n                pde_constructor=HeatEquation,\n                pde_kwargs={\n                    'domain_size': (256, 256),\n                    'boundary_conditions': 'periodic',\n                    'initial_condition': lambda x, y: (np.sin(np.pi * x) * np.sin(np.pi * y) +\n                                                      0.01 * np.sin(50 * np.pi * x) * np.sin(50 * np.pi * y)),\n                    'diffusivity': 0.001\n                },\n                reference_solution=None,\n                analytical_solution=None,\n                problem_size=(256, 256),\n                expected_sparsity=0.02,\n                time_dependent=True,\n                multi_physics=False,\n                difficulty_level='extreme'\n            )\n        ])\n        \n        return problems\n    \n    def run_comprehensive_benchmark(\n        self,\n        benchmark_types: List[BenchmarkType] = None,\n        algorithms_to_test: List[AlgorithmType] = None,\n        parallel_execution: bool = True\n    ) -> Dict[str, Any]:\n        \"\"\"Run comprehensive benchmark suite.\n        \n        Args:\n            benchmark_types: Types of benchmarks to run\n            algorithms_to_test: Specific algorithms to test\n            parallel_execution: Whether to run benchmarks in parallel\n            \n        Returns:\n            Comprehensive benchmark results\n        \"\"\"\n        if benchmark_types is None:\n            benchmark_types = list(BenchmarkType)\n        \n        if algorithms_to_test is None:\n            algorithms_to_test = list(self.framework.algorithms.keys())\n        \n        self.logger.info(f\"Starting comprehensive benchmark: {len(benchmark_types)} types, {len(algorithms_to_test)} algorithms\")\n        \n        comprehensive_results = {\n            'benchmark_config': {\n                'timestamp': time.time(),\n                'benchmark_types': [bt.value for bt in benchmark_types],\n                'algorithms_tested': [alg.value for alg in algorithms_to_test],\n                'num_problems': len(self.benchmark_problems),\n                'statistical_runs': self.num_statistical_runs\n            },\n            'results_by_type': {},\n            'statistical_analysis': {},\n            'summary_metrics': {},\n            'recommendations': {}\n        }\n        \n        # Run each benchmark type\n        for benchmark_type in benchmark_types:\n            self.logger.info(f\"Running {benchmark_type.value} benchmark\")\n            \n            if benchmark_type == BenchmarkType.ACCURACY:\n                results = self.run_accuracy_benchmark(algorithms_to_test)\n            elif benchmark_type == BenchmarkType.PERFORMANCE:\n                results = self.run_performance_benchmark(algorithms_to_test)\n            elif benchmark_type == BenchmarkType.ENERGY_EFFICIENCY:\n                results = self.run_energy_benchmark(algorithms_to_test)\n            elif benchmark_type == BenchmarkType.SCALABILITY:\n                results = self.run_scalability_benchmark(algorithms_to_test)\n            elif benchmark_type == BenchmarkType.ROBUSTNESS:\n                results = self.run_robustness_benchmark(algorithms_to_test)\n            elif benchmark_type == BenchmarkType.COMPARATIVE:\n                results = self.run_comparative_benchmark(algorithms_to_test)\n            else:\n                results = {}\n            \n            comprehensive_results['results_by_type'][benchmark_type.value] = results\n        \n        # Statistical analysis\n        comprehensive_results['statistical_analysis'] = self._perform_statistical_analysis()\n        \n        # Summary metrics\n        comprehensive_results['summary_metrics'] = self._compute_summary_metrics()\n        \n        # Algorithm recommendations\n        comprehensive_results['recommendations'] = self._generate_recommendations()\n        \n        # Save results\n        self._save_benchmark_results(comprehensive_results)\n        \n        self.logger.info(\"Comprehensive benchmark completed\")\n        return comprehensive_results\n    \n    def run_accuracy_benchmark(self, algorithms: List[AlgorithmType]) -> Dict[str, Any]:\n        \"\"\"Run accuracy-focused benchmark.\"\"\"\n        self.logger.info(\"Running accuracy benchmark\")\n        \n        accuracy_results = {\n            'algorithm_errors': {},\n            'convergence_rates': {},\n            'analytical_comparisons': {},\n            'conservation_errors': {}\n        }\n        \n        for algorithm in algorithms:\n            if algorithm not in self.framework.algorithms:\n                continue\n                \n            algorithm_errors = []\n            convergence_data = []\n            analytical_errors = []\n            conservation_errors = []\n            \n            for problem in self.benchmark_problems:\n                try:\n                    # Run multiple times for statistical significance\n                    problem_errors = []\n                    \n                    for run in range(self.num_statistical_runs):\n                        # Create PDE instance\n                        pde = problem.pde_constructor(**problem.pde_kwargs)\n                        \n                        # Solve with selected algorithm\n                        characteristics = ProblemCharacteristics(\n                            problem_size=problem.problem_size,\n                            sparsity_level=problem.expected_sparsity,\n                            time_dependent=problem.time_dependent,\n                            multi_physics=problem.multi_physics,\n                            conservation_required='navier' in problem.name.lower(),\n                            accuracy_requirement=1e-6,\n                            energy_budget=None,\n                            real_time_requirement=False,\n                            physics_constraints=[],\n                            boundary_complexity='simple'\n                        )\n                        \n                        solution, solve_info = self.framework._execute_solve(\n                            algorithm, pde, characteristics, {}\n                        )\n                        \n                        # Compute error metrics\n                        if problem.analytical_solution is not None:\n                            # Compare with analytical solution\n                            x = np.linspace(0, 1, problem.problem_size[0])\n                            if len(problem.problem_size) == 1:\n                                analytical = problem.analytical_solution(x, 0 if not problem.time_dependent else 1.0)\n                            else:\n                                y = np.linspace(0, 1, problem.problem_size[1])\n                                X, Y = np.meshgrid(x, y)\n                                analytical = problem.analytical_solution(X, Y).flatten()\n                            \n                            # Resize if necessary\n                            if len(solution) != len(analytical):\n                                if len(solution) > len(analytical):\n                                    solution = solution[:len(analytical)]\n                                else:\n                                    analytical = analytical[:len(solution)]\n                            \n                            error = np.linalg.norm(solution - analytical)\n                            analytical_errors.append(error)\n                        \n                        elif problem.reference_solution is not None:\n                            # Compare with reference solution\n                            reference = problem.reference_solution\n                            if len(solution) != len(reference):\n                                if len(solution) > len(reference):\n                                    solution = solution[:len(reference)]\n                                else:\n                                    reference = reference[:len(solution)]\n                            \n                            error = np.linalg.norm(solution - reference)\n                            problem_errors.append(error)\n                        \n                        # Check conservation if required\n                        if 'navier' in problem.name.lower() or 'conservation' in problem.name.lower():\n                            # Simple conservation check\n                            initial_mass = np.sum(np.ones_like(solution))\n                            final_mass = np.sum(solution)\n                            conservation_error = abs(initial_mass - final_mass) / initial_mass\n                            conservation_errors.append(conservation_error)\n                        \n                    if problem_errors:\n                        algorithm_errors.extend(problem_errors)\n                        \n                except Exception as e:\n                    self.logger.warning(f\"Accuracy benchmark failed for {algorithm.value} on {problem.name}: {e}\")\n            \n            # Store results\n            if algorithm_errors:\n                accuracy_results['algorithm_errors'][algorithm.value] = {\n                    'mean_error': np.mean(algorithm_errors),\n                    'std_error': np.std(algorithm_errors),\n                    'max_error': np.max(algorithm_errors),\n                    'min_error': np.min(algorithm_errors),\n                    'errors': algorithm_errors\n                }\n            \n            if analytical_errors:\n                accuracy_results['analytical_comparisons'][algorithm.value] = {\n                    'mean_error': np.mean(analytical_errors),\n                    'std_error': np.std(analytical_errors),\n                    'errors': analytical_errors\n                }\n            \n            if conservation_errors:\n                accuracy_results['conservation_errors'][algorithm.value] = {\n                    'mean_error': np.mean(conservation_errors),\n                    'max_error': np.max(conservation_errors),\n                    'errors': conservation_errors\n                }\n        \n        return accuracy_results\n    \n    def run_performance_benchmark(self, algorithms: List[AlgorithmType]) -> Dict[str, Any]:\n        \"\"\"Run performance-focused benchmark.\"\"\"\n        self.logger.info(\"Running performance benchmark\")\n        \n        performance_results = {\n            'solve_times': {},\n            'throughput': {},\n            'memory_usage': {},\n            'scalability_metrics': {}\n        }\n        \n        for algorithm in algorithms:\n            if algorithm not in self.framework.algorithms:\n                continue\n                \n            solve_times = []\n            memory_usage = []\n            throughput_data = []\n            \n            for problem in self.benchmark_problems:\n                try:\n                    problem_times = []\n                    \n                    for run in range(self.num_statistical_runs):\n                        # Create PDE instance\n                        pde = problem.pde_constructor(**problem.pde_kwargs)\n                        \n                        # Measure solve time\n                        self.perf_logger.start_timer(f'perf_bench_{algorithm.value}_{problem.name}_{run}')\n                        \n                        characteristics = ProblemCharacteristics(\n                            problem_size=problem.problem_size,\n                            sparsity_level=problem.expected_sparsity,\n                            time_dependent=problem.time_dependent,\n                            multi_physics=problem.multi_physics,\n                            conservation_required=False,\n                            accuracy_requirement=1e-6,\n                            energy_budget=None,\n                            real_time_requirement=False,\n                            physics_constraints=[],\n                            boundary_complexity='simple'\n                        )\n                        \n                        solution, solve_info = self.framework._execute_solve(\n                            algorithm, pde, characteristics, {}\n                        )\n                        \n                        solve_time = self.perf_logger.end_timer(f'perf_bench_{algorithm.value}_{problem.name}_{run}')\n                        problem_times.append(solve_time)\n                        \n                        # Estimate memory usage (simplified)\n                        problem_size = np.prod(problem.problem_size)\n                        memory_estimate = problem_size * 8 * 4  # 4 arrays of double precision\n                        memory_usage.append(memory_estimate)\n                        \n                        # Compute throughput (elements per second)\n                        if solve_time > 0:\n                            throughput = problem_size / solve_time\n                            throughput_data.append(throughput)\n                    \n                    solve_times.extend(problem_times)\n                    \n                except Exception as e:\n                    self.logger.warning(f\"Performance benchmark failed for {algorithm.value} on {problem.name}: {e}\")\n            \n            # Store results\n            if solve_times:\n                performance_results['solve_times'][algorithm.value] = {\n                    'mean_time': np.mean(solve_times),\n                    'std_time': np.std(solve_times),\n                    'min_time': np.min(solve_times),\n                    'max_time': np.max(solve_times),\n                    'times': solve_times\n                }\n            \n            if memory_usage:\n                performance_results['memory_usage'][algorithm.value] = {\n                    'mean_memory': np.mean(memory_usage),\n                    'max_memory': np.max(memory_usage)\n                }\n            \n            if throughput_data:\n                performance_results['throughput'][algorithm.value] = {\n                    'mean_throughput': np.mean(throughput_data),\n                    'peak_throughput': np.max(throughput_data)\n                }\n        \n        return performance_results\n    \n    def run_energy_benchmark(self, algorithms: List[AlgorithmType]) -> Dict[str, Any]:\n        \"\"\"Run energy efficiency benchmark.\"\"\"\n        self.logger.info(\"Running energy efficiency benchmark\")\n        \n        energy_results = {\n            'energy_estimates': {},\n            'efficiency_ratios': {},\n            'sparsity_benefits': {}\n        }\n        \n        for algorithm in algorithms:\n            if algorithm not in self.framework.algorithms:\n                continue\n                \n            energy_estimates = []\n            efficiency_data = []\n            sparsity_energy_data = []\n            \n            for problem in self.benchmark_problems:\n                try:\n                    for run in range(max(3, self.num_statistical_runs // 3)):  # Fewer runs for energy\n                        pde = problem.pde_constructor(**problem.pde_kwargs)\n                        \n                        characteristics = ProblemCharacteristics(\n                            problem_size=problem.problem_size,\n                            sparsity_level=problem.expected_sparsity,\n                            time_dependent=problem.time_dependent,\n                            multi_physics=problem.multi_physics,\n                            conservation_required=False,\n                            accuracy_requirement=1e-6,\n                            energy_budget=None,\n                            real_time_requirement=False,\n                            physics_constraints=[],\n                            boundary_complexity='simple'\n                        )\n                        \n                        # Estimate energy consumption\n                        problem_size = np.prod(problem.problem_size)\n                        \n                        # Energy model based on algorithm type\n                        if algorithm == AlgorithmType.NEUROMORPHIC:\n                            # Neuromorphic energy scales with sparsity\n                            activity = 1.0 - problem.expected_sparsity\n                            base_energy = problem_size * 1e-12  # 1pJ per operation base\n                            energy_estimate = base_energy * activity * 0.001  # 1000\u00d7 reduction for sparse\n                        elif algorithm == AlgorithmType.HETEROGENEOUS_PRECISION:\n                            # Energy scales with precision usage\n                            base_energy = problem_size * 1e-9  # 1nJ per operation\n                            precision_factor = 0.5  # Assume 50% energy reduction from mixed precision\n                            energy_estimate = base_energy * precision_factor\n                        elif algorithm == AlgorithmType.TEMPORAL_CASCADE:\n                            # Energy for pipelined computation\n                            if problem.time_dependent:\n                                base_energy = problem_size * 1e-9\n                                pipeline_efficiency = 0.8  # 20% overhead for pipeline\n                                energy_estimate = base_energy * pipeline_efficiency\n                            else:\n                                energy_estimate = problem_size * 1e-9\n                        else:\n                            # Default analog energy\n                            energy_estimate = problem_size * 1e-9  # 1nJ per operation\n                        \n                        energy_estimates.append(energy_estimate)\n                        \n                        # Efficiency relative to base analog\n                        base_energy = problem_size * 1e-9\n                        efficiency_ratio = base_energy / max(energy_estimate, 1e-15)\n                        efficiency_data.append(efficiency_ratio)\n                        \n                        # Sparsity benefit analysis\n                        if problem.expected_sparsity > 0.5:\n                            sparsity_benefit = problem.expected_sparsity * efficiency_ratio\n                            sparsity_energy_data.append(sparsity_benefit)\n                \n                except Exception as e:\n                    self.logger.warning(f\"Energy benchmark failed for {algorithm.value} on {problem.name}: {e}\")\n            \n            # Store results\n            if energy_estimates:\n                energy_results['energy_estimates'][algorithm.value] = {\n                    'mean_energy': np.mean(energy_estimates),\n                    'total_energy': np.sum(energy_estimates),\n                    'energy_range': [np.min(energy_estimates), np.max(energy_estimates)]\n                }\n            \n            if efficiency_data:\n                energy_results['efficiency_ratios'][algorithm.value] = {\n                    'mean_efficiency': np.mean(efficiency_data),\n                    'peak_efficiency': np.max(efficiency_data)\n                }\n            \n            if sparsity_energy_data:\n                energy_results['sparsity_benefits'][algorithm.value] = {\n                    'sparsity_efficiency': np.mean(sparsity_energy_data)\n                }\n        \n        return energy_results\n    \n    def run_scalability_benchmark(self, algorithms: List[AlgorithmType]) -> Dict[str, Any]:\n        \"\"\"Run scalability benchmark.\"\"\"\n        self.logger.info(\"Running scalability benchmark\")\n        \n        scalability_results = {\n            'scaling_curves': {},\n            'complexity_analysis': {},\n            'parallel_efficiency': {}\n        }\n        \n        # Test different problem sizes\n        test_sizes = [64, 128, 256, 512]\n        \n        for algorithm in algorithms:\n            if algorithm not in self.framework.algorithms:\n                continue\n                \n            scaling_data = []\n            complexity_data = []\n            \n            for size in test_sizes:\n                try:\n                    # Create scalability test problem\n                    pde = PoissonEquation(\n                        domain_size=(size, size),\n                        boundary_conditions='dirichlet',\n                        source_function=lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n                    )\n                    \n                    characteristics = ProblemCharacteristics(\n                        problem_size=(size, size),\n                        sparsity_level=0.1,\n                        time_dependent=False,\n                        multi_physics=False,\n                        conservation_required=False,\n                        accuracy_requirement=1e-6,\n                        energy_budget=None,\n                        real_time_requirement=False,\n                        physics_constraints=[],\n                        boundary_complexity='simple'\n                    )\n                    \n                    # Measure solve time for this size\n                    solve_times = []\n                    for run in range(3):  # Fewer runs for scalability\n                        self.perf_logger.start_timer(f'scale_bench_{algorithm.value}_{size}_{run}')\n                        \n                        solution, solve_info = self.framework._execute_solve(\n                            algorithm, pde, characteristics, {}\n                        )\n                        \n                        solve_time = self.perf_logger.end_timer(f'scale_bench_{algorithm.value}_{size}_{run}')\n                        solve_times.append(solve_time)\n                    \n                    avg_solve_time = np.mean(solve_times)\n                    scaling_data.append((size**2, avg_solve_time))  # (problem_size, time)\n                    \n                    # Complexity analysis\n                    problem_size = size**2\n                    operations_estimate = problem_size * np.log(problem_size)  # Estimated operations\n                    if avg_solve_time > 0:\n                        ops_per_second = operations_estimate / avg_solve_time\n                        complexity_data.append((problem_size, ops_per_second))\n                \n                except Exception as e:\n                    self.logger.warning(f\"Scalability benchmark failed for {algorithm.value} at size {size}: {e}\")\n            \n            # Store results\n            if scaling_data:\n                sizes, times = zip(*scaling_data)\n                \n                # Fit scaling curve (linear in log-log space indicates power law)\n                if len(sizes) > 2:\n                    log_sizes = np.log(sizes)\n                    log_times = np.log(times)\n                    slope, intercept, r_value, p_value, std_err = stats.linregress(log_sizes, log_times)\n                    \n                    scalability_results['scaling_curves'][algorithm.value] = {\n                        'sizes': list(sizes),\n                        'times': list(times),\n                        'scaling_exponent': slope,\n                        'r_squared': r_value**2,\n                        'complexity_class': 'O(n^{:.2f})'.format(slope)\n                    }\n            \n            if complexity_data:\n                scalability_results['complexity_analysis'][algorithm.value] = {\n                    'operations_per_second': [ops for _, ops in complexity_data],\n                    'problem_sizes': [size for size, _ in complexity_data]\n                }\n        \n        return scalability_results\n    \n    def run_robustness_benchmark(self, algorithms: List[AlgorithmType]) -> Dict[str, Any]:\n        \"\"\"Run robustness benchmark.\"\"\"\n        self.logger.info(\"Running robustness benchmark\")\n        \n        robustness_results = {\n            'noise_tolerance': {},\n            'parameter_sensitivity': {},\n            'failure_rates': {}\n        }\n        \n        # Add noise to test problems\n        noise_levels = [0.0, 0.01, 0.1, 0.5]\n        \n        for algorithm in algorithms:\n            if algorithm not in self.framework.algorithms:\n                continue\n                \n            noise_performance = {}\n            failure_count = 0\n            total_attempts = 0\n            \n            for noise_level in noise_levels:\n                noise_results = []\n                \n                for problem in self.benchmark_problems[:3]:  # Test on subset for robustness\n                    try:\n                        pde = problem.pde_constructor(**problem.pde_kwargs)\n                        \n                        characteristics = ProblemCharacteristics(\n                            problem_size=problem.problem_size,\n                            sparsity_level=problem.expected_sparsity,\n                            time_dependent=problem.time_dependent,\n                            multi_physics=problem.multi_physics,\n                            conservation_required=False,\n                            accuracy_requirement=1e-6,\n                            energy_budget=None,\n                            real_time_requirement=False,\n                            physics_constraints=[],\n                            boundary_complexity='simple'\n                        )\n                        \n                        # Add noise to initial conditions or parameters\n                        noisy_kwargs = problem.pde_kwargs.copy()\n                        if 'initial_condition' in noisy_kwargs and callable(noisy_kwargs['initial_condition']):\n                            original_ic = noisy_kwargs['initial_condition']\n                            noisy_kwargs['initial_condition'] = lambda x: original_ic(x) + noise_level * np.random.random(x.shape)\n                        \n                        noisy_pde = problem.pde_constructor(**noisy_kwargs)\n                        \n                        solution, solve_info = self.framework._execute_solve(\n                            algorithm, noisy_pde, characteristics, {}\n                        )\n                        \n                        # Check if solution is reasonable\n                        if np.isfinite(solution).all() and np.max(np.abs(solution)) < 1e6:\n                            noise_results.append(np.linalg.norm(solution))\n                        else:\n                            failure_count += 1\n                        \n                        total_attempts += 1\n                    \n                    except Exception as e:\n                        failure_count += 1\n                        total_attempts += 1\n                        self.logger.debug(f\"Robustness test failed: {e}\")\n                \n                if noise_results:\n                    noise_performance[noise_level] = {\n                        'mean_norm': np.mean(noise_results),\n                        'std_norm': np.std(noise_results)\n                    }\n            \n            robustness_results['noise_tolerance'][algorithm.value] = noise_performance\n            \n            if total_attempts > 0:\n                robustness_results['failure_rates'][algorithm.value] = failure_count / total_attempts\n        \n        return robustness_results\n    \n    def run_comparative_benchmark(self, algorithms: List[AlgorithmType]) -> Dict[str, Any]:\n        \"\"\"Run comparative benchmark between algorithms.\"\"\"\n        self.logger.info(\"Running comparative benchmark\")\n        \n        comparative_results = {\n            'pairwise_comparisons': {},\n            'performance_rankings': {},\n            'use_case_recommendations': {}\n        }\n        \n        # Compare each pair of algorithms\n        for i, alg1 in enumerate(algorithms):\n            for j, alg2 in enumerate(algorithms):\n                if i >= j or alg1 not in self.framework.algorithms or alg2 not in self.framework.algorithms:\n                    continue\n                \n                comparison_key = f\"{alg1.value}_vs_{alg2.value}\"\n                wins_alg1 = 0\n                wins_alg2 = 0\n                ties = 0\n                \n                for problem in self.benchmark_problems:\n                    try:\n                        pde = problem.pde_constructor(**problem.pde_kwargs)\n                        \n                        characteristics = ProblemCharacteristics(\n                            problem_size=problem.problem_size,\n                            sparsity_level=problem.expected_sparsity,\n                            time_dependent=problem.time_dependent,\n                            multi_physics=problem.multi_physics,\n                            conservation_required=False,\n                            accuracy_requirement=1e-6,\n                            energy_budget=None,\n                            real_time_requirement=False,\n                            physics_constraints=[],\n                            boundary_complexity='simple'\n                        )\n                        \n                        # Time both algorithms\n                        self.perf_logger.start_timer(f'comp_alg1_{i}')\n                        solution1, _ = self.framework._execute_solve(alg1, pde, characteristics, {})\n                        time1 = self.perf_logger.end_timer(f'comp_alg1_{i}')\n                        \n                        self.perf_logger.start_timer(f'comp_alg2_{j}')\n                        solution2, _ = self.framework._execute_solve(alg2, pde, characteristics, {})\n                        time2 = self.perf_logger.end_timer(f'comp_alg2_{j}')\n                        \n                        # Compare performance (speed-based)\n                        if time1 < time2 * 0.9:  # 10% margin for ties\n                            wins_alg1 += 1\n                        elif time2 < time1 * 0.9:\n                            wins_alg2 += 1\n                        else:\n                            ties += 1\n                    \n                    except Exception as e:\n                        self.logger.warning(f\"Comparative benchmark failed for {comparison_key}: {e}\")\n                \n                comparative_results['pairwise_comparisons'][comparison_key] = {\n                    'alg1_wins': wins_alg1,\n                    'alg2_wins': wins_alg2,\n                    'ties': ties,\n                    'total_comparisons': wins_alg1 + wins_alg2 + ties\n                }\n        \n        # Create performance rankings\n        algorithm_scores = {}\n        for algorithm in algorithms:\n            if algorithm not in self.framework.algorithms:\n                continue\n                \n            score = 0\n            for comparison_key, results in comparative_results['pairwise_comparisons'].items():\n                if algorithm.value in comparison_key:\n                    if comparison_key.startswith(algorithm.value):\n                        score += results['alg1_wins']\n                    else:\n                        score += results['alg2_wins']\n            \n            algorithm_scores[algorithm.value] = score\n        \n        # Sort by score\n        ranked_algorithms = sorted(algorithm_scores.items(), key=lambda x: x[1], reverse=True)\n        comparative_results['performance_rankings'] = {\n            'rankings': ranked_algorithms,\n            'scores': algorithm_scores\n        }\n        \n        return comparative_results\n    \n    def _perform_statistical_analysis(self) -> Dict[str, Any]:\n        \"\"\"Perform statistical analysis of benchmark results.\"\"\"\n        statistical_analysis = {\n            'significance_tests': {},\n            'confidence_intervals': {},\n            'effect_sizes': {},\n            'hypothesis_tests': {}\n        }\n        \n        # Extract performance data for statistical tests\n        if hasattr(self, 'results') and 'results_by_type' in self.results:\n            performance_data = self.results['results_by_type'].get('performance', {})\n            \n            if 'solve_times' in performance_data:\n                algorithms = list(performance_data['solve_times'].keys())\n                \n                # Pairwise statistical tests\n                for i, alg1 in enumerate(algorithms):\n                    for j, alg2 in enumerate(algorithms):\n                        if i >= j:\n                            continue\n                        \n                        times1 = performance_data['solve_times'][alg1]['times']\n                        times2 = performance_data['solve_times'][alg2]['times']\n                        \n                        # Mann-Whitney U test (non-parametric)\n                        try:\n                            statistic, p_value = stats.mannwhitneyu(times1, times2, alternative='two-sided')\n                            \n                            test_key = f\"{alg1}_vs_{alg2}\"\n                            statistical_analysis['significance_tests'][test_key] = {\n                                'test': 'mann_whitney_u',\n                                'statistic': statistic,\n                                'p_value': p_value,\n                                'significant': p_value < self.statistical_significance,\n                                'effect_size': self._compute_effect_size(times1, times2)\n                            }\n                        except Exception as e:\n                            self.logger.warning(f\"Statistical test failed for {alg1} vs {alg2}: {e}\")\n        \n        return statistical_analysis\n    \n    def _compute_effect_size(self, group1: List[float], group2: List[float]) -> float:\n        \"\"\"Compute Cohen's d effect size.\"\"\"\n        try:\n            mean1, mean2 = np.mean(group1), np.mean(group2)\n            std1, std2 = np.std(group1, ddof=1), np.std(group2, ddof=1)\n            \n            # Pooled standard deviation\n            n1, n2 = len(group1), len(group2)\n            pooled_std = np.sqrt(((n1 - 1) * std1**2 + (n2 - 1) * std2**2) / (n1 + n2 - 2))\n            \n            # Cohen's d\n            if pooled_std > 0:\n                cohens_d = (mean1 - mean2) / pooled_std\n                return abs(cohens_d)\n            else:\n                return 0.0\n        except:\n            return 0.0\n    \n    def _compute_summary_metrics(self) -> Dict[str, Any]:\n        \"\"\"Compute summary metrics across all benchmarks.\"\"\"\n        summary = {\n            'overall_performance': {},\n            'algorithm_strengths': {},\n            'problem_difficulty': {},\n            'resource_efficiency': {}\n        }\n        \n        # Placeholder for summary computation\n        # In a real implementation, this would aggregate results across all benchmark types\n        \n        return summary\n    \n    def _generate_recommendations(self) -> Dict[str, Any]:\n        \"\"\"Generate algorithm recommendations based on benchmark results.\"\"\"\n        recommendations = {\n            'best_overall': None,\n            'best_for_sparse': None,\n            'best_for_accuracy': None,\n            'best_for_speed': None,\n            'best_for_energy': None,\n            'use_case_guidance': {}\n        }\n        \n        # Analyze results and generate recommendations\n        # This would be based on the comprehensive benchmark results\n        \n        return recommendations\n    \n    def _save_benchmark_results(self, results: Dict[str, Any]) -> None:\n        \"\"\"Save benchmark results to file.\"\"\"\n        timestamp = time.strftime(\"%Y%m%d_%H%M%S\")\n        results_file = self.output_dir / f\"benchmark_results_{timestamp}.json\"\n        \n        # Convert numpy arrays to lists for JSON serialization\n        def convert_numpy(obj):\n            if isinstance(obj, np.ndarray):\n                return obj.tolist()\n            elif isinstance(obj, np.integer):\n                return int(obj)\n            elif isinstance(obj, np.floating):\n                return float(obj)\n            elif isinstance(obj, dict):\n                return {key: convert_numpy(value) for key, value in obj.items()}\n            elif isinstance(obj, list):\n                return [convert_numpy(item) for item in obj]\n            return obj\n        \n        serializable_results = convert_numpy(results)\n        \n        try:\n            with open(results_file, 'w') as f:\n                json.dump(serializable_results, f, indent=2)\n            \n            self.logger.info(f\"Benchmark results saved to {results_file}\")\n        except Exception as e:\n            self.logger.error(f\"Failed to save benchmark results: {e}\")\n    \n    def generate_performance_report(self, results: Dict[str, Any]) -> str:\n        \"\"\"Generate a comprehensive performance report.\"\"\"\n        report_lines = [\n            \"# Analog PDE Solver Advanced Algorithms Benchmark Report\",\n            f\"Generated: {time.strftime('%Y-%m-%d %H:%M:%S')}\",\n            \"\",\n            \"## Executive Summary\",\n            \"\"\n        ]\n        \n        # Add benchmark configuration\n        if 'benchmark_config' in results:\n            config = results['benchmark_config']\n            report_lines.extend([\n                \"### Benchmark Configuration\",\n                f\"- Algorithms tested: {', '.join(config.get('algorithms_tested', []))}\",\n                f\"- Benchmark types: {', '.join(config.get('benchmark_types', []))}\",\n                f\"- Number of problems: {config.get('num_problems', 0)}\",\n                f\"- Statistical runs per problem: {config.get('statistical_runs', 0)}\",\n                \"\"\n            ])\n        \n        # Add performance summary\n        report_lines.extend([\n            \"### Key Findings\",\n            \"- [Key finding 1]\",\n            \"- [Key finding 2]\", \n            \"- [Key finding 3]\",\n            \"\",\n            \"## Detailed Results\",\n            \"\"\n        ])\n        \n        # Add algorithm-specific results\n        if 'results_by_type' in results:\n            for benchmark_type, benchmark_results in results['results_by_type'].items():\n                report_lines.extend([\n                    f\"### {benchmark_type.title()} Benchmark\",\n                    \"\"\n                ])\n                \n                # Add specific results for each benchmark type\n                if benchmark_type == 'performance' and 'solve_times' in benchmark_results:\n                    for algorithm, perf_data in benchmark_results['solve_times'].items():\n                        report_lines.extend([\n                            f\"**{algorithm}:**\",\n                            f\"- Mean solve time: {perf_data.get('mean_time', 0):.4f}s\",\n                            f\"- Standard deviation: {perf_data.get('std_time', 0):.4f}s\",\n                            f\"- Range: {perf_data.get('min_time', 0):.4f}s - {perf_data.get('max_time', 0):.4f}s\",\n                            \"\"\n                        ])\n        \n        # Add recommendations\n        if 'recommendations' in results:\n            report_lines.extend([\n                \"## Recommendations\",\n                \"\"\n            ])\n            \n            recommendations = results['recommendations']\n            for rec_type, recommendation in recommendations.items():\n                if recommendation:\n                    report_lines.append(f\"- **{rec_type.replace('_', ' ').title()}:** {recommendation}\")\n            \n            report_lines.append(\"\")\n        \n        return '"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "analog_pde_solver/validation/hardware_validator.py",
      "line_number": 1,
      "details": "\"\"\"Hardware validation and verification tools for analog PDE solver.\"\"\"\n\nimport numpy as np\nimport logging\nfrom typing import Dict, Any, List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport time\n\nlogger = logging.getLogger(__name__)\n\n\nclass HardwareTestLevel(Enum):\n    \"\"\"Hardware validation test levels.\"\"\"\n    BASIC = \"basic\"\n    STANDARD = \"standard\"\n    COMPREHENSIVE = \"comprehensive\"\n    PRODUCTION = \"production\"\n\n\n@dataclass\nclass HardwareValidationResult:\n    \"\"\"Result of hardware validation.\"\"\"\n    is_valid: bool\n    reliability_score: float  # 0.0 to 1.0\n    performance_metrics: Dict[str, float]\n    compliance_results: Dict[str, bool]\n    warnings: List[str]\n    errors: List[str]\n    test_level: HardwareTestLevel\n    tests_passed: int\n    tests_total: int\n    validation_time: float\n    \n    def summary(self) -> str:\n        \"\"\"Generate hardware validation summary.\"\"\"\n        status = \"\u2705 VALID\" if self.is_valid else \"\u274c INVALID\"\n        return f\"\"\"\nHardware Validation Summary\n{status} (Reliability: {self.reliability_score:.2%})\nTests: {self.tests_passed}/{self.tests_total}\nLevel: {self.test_level.value.upper()}\nTime: {self.validation_time:.2f}s\nErrors: {len(self.errors)} | Warnings: {len(self.warnings)}\n\"\"\"\n\n\nclass HardwareValidator:\n    \"\"\"Hardware validation and compliance checker for analog crossbar arrays.\"\"\"\n    \n    def __init__(self, test_level: HardwareTestLevel = HardwareTestLevel.STANDARD):\n        \"\"\"Initialize hardware validator.\n        \n        Args:\n            test_level: Level of hardware testing to perform\n        \"\"\"\n        self.test_level = test_level\n        self.logger = logging.getLogger(__name__)\n        \n        # Hardware limits and thresholds\n        self.limits = self._get_hardware_limits(test_level)\n        \n    def _get_hardware_limits(self, level: HardwareTestLevel) -> Dict[str, Any]:\n        \"\"\"Get hardware validation limits for test level.\"\"\"\n        base_limits = {\n            \"max_power_mw\": 1000.0,\n            \"max_temperature_c\": 85.0,\n            \"min_efficiency\": 0.1,\n            \"max_crossbar_size\": 1024,\n            \"min_conductance_s\": 1e-9,\n            \"max_conductance_s\": 1e-3,\n            \"max_programming_voltage_v\": 5.0,\n            \"min_snr_db\": 20.0,\n            \"max_latency_ms\": 100.0,\n            \"min_accuracy_bits\": 8\n        }\n        \n        if level == HardwareTestLevel.PRODUCTION:\n            # Stricter limits for production\n            base_limits.update({\n                \"max_power_mw\": 500.0,\n                \"max_temperature_c\": 70.0,\n                \"min_efficiency\": 0.3,\n                \"min_snr_db\": 30.0,\n                \"max_latency_ms\": 50.0,\n                \"min_accuracy_bits\": 10\n            })\n        elif level == HardwareTestLevel.BASIC:\n            # Relaxed limits for basic testing\n            base_limits.update({\n                \"max_power_mw\": 2000.0,\n                \"max_temperature_c\": 95.0,\n                \"min_efficiency\": 0.05,\n                \"min_snr_db\": 15.0,\n                \"max_latency_ms\": 200.0,\n                \"min_accuracy_bits\": 6\n            })\n        \n        return base_limits\n    \n    def validate_hardware(\n        self,\n        crossbar_array,\n        power_consumption_mw: Optional[float] = None,\n        temperature_c: Optional[float] = None,\n        performance_metrics: Optional[Dict[str, float]] = None,\n        compliance_data: Optional[Dict[str, Any]] = None\n    ) -> HardwareValidationResult:\n        \"\"\"Validate hardware implementation comprehensively.\n        \n        Args:\n            crossbar_array: Crossbar array implementation to validate\n            power_consumption_mw: Power consumption in milliwatts\n            temperature_c: Operating temperature in Celsius\n            performance_metrics: Performance metrics dictionary\n            compliance_data: Regulatory compliance data\n            \n        Returns:\n            Hardware validation result\n        \"\"\"\n        start_time = time.time()\n        self.logger.info(f\"Starting hardware validation at {self.test_level.value} level\")\n        \n        errors = []\n        warnings = []\n        perf_metrics = {}\n        compliance_results = {}\n        tests_passed = 0\n        tests_total = 0\n        \n        # Test 1: Crossbar Array Validation\n        tests_total += 1\n        if self._validate_crossbar_array(crossbar_array):\n            tests_passed += 1\n        else:\n            errors.append(\"Crossbar array validation failed\")\n        \n        # Test 2: Power Consumption Analysis\n        if power_consumption_mw is not None:\n            tests_total += 1\n            power_valid, power_metrics = self._validate_power_consumption(power_consumption_mw)\n            perf_metrics.update(power_metrics)\n            \n            if power_valid:\n                tests_passed += 1\n            else:\n                errors.append(f\"Power consumption exceeds limit: {power_consumption_mw:.1f}mW\")\n        \n        # Test 3: Thermal Analysis\n        if temperature_c is not None:\n            tests_total += 1\n            thermal_valid = self._validate_thermal_performance(temperature_c)\n            perf_metrics[\"operating_temperature_c\"] = temperature_c\n            \n            if thermal_valid:\n                tests_passed += 1\n            else:\n                errors.append(f\"Operating temperature too high: {temperature_c:.1f}\u00b0C\")\n        \n        # Test 4: Conductance Range Validation\n        tests_total += 1\n        conductance_valid = self._validate_conductance_range(crossbar_array)\n        if conductance_valid:\n            tests_passed += 1\n        else:\n            warnings.append(\"Conductance range may be suboptimal\")\n        \n        # Test 5: Signal Integrity\n        tests_total += 1\n        signal_metrics = self._validate_signal_integrity(crossbar_array)\n        perf_metrics.update(signal_metrics)\n        \n        snr_db = signal_metrics.get(\"snr_db\", 0)\n        if snr_db >= self.limits[\"min_snr_db\"]:\n            tests_passed += 1\n        else:\n            errors.append(f\"Poor signal-to-noise ratio: {snr_db:.1f}dB\")\n        \n        # Test 6: Latency Performance\n        tests_total += 1\n        latency_ms = self._measure_latency(crossbar_array)\n        perf_metrics[\"latency_ms\"] = latency_ms\n        \n        if latency_ms <= self.limits[\"max_latency_ms\"]:\n            tests_passed += 1\n        else:\n            warnings.append(f\"High latency detected: {latency_ms:.1f}ms\")\n        \n        # Test 7: Accuracy Assessment\n        tests_total += 1\n        accuracy_bits = self._assess_computation_accuracy(crossbar_array)\n        perf_metrics[\"effective_bits\"] = accuracy_bits\n        \n        if accuracy_bits >= self.limits[\"min_accuracy_bits\"]:\n            tests_passed += 1\n        else:\n            errors.append(f\"Insufficient computational accuracy: {accuracy_bits:.1f} bits\")\n        \n        # Test 8: Device Variability\n        if self.test_level in [HardwareTestLevel.COMPREHENSIVE, HardwareTestLevel.PRODUCTION]:\n            tests_total += 1\n            variability_score = self._assess_device_variability(crossbar_array)\n            perf_metrics[\"variability_score\"] = variability_score\n            \n            if variability_score >= 0.7:\n                tests_passed += 1\n            elif variability_score >= 0.5:\n                warnings.append(f\"Moderate device variability: {variability_score:.2f}\")\n            else:\n                errors.append(f\"High device variability: {variability_score:.2f}\")\n        \n        # Test 9: Endurance Testing\n        if self.test_level == HardwareTestLevel.PRODUCTION:\n            tests_total += 1\n            endurance_cycles = self._test_endurance(crossbar_array)\n            perf_metrics[\"endurance_cycles\"] = endurance_cycles\n            \n            if endurance_cycles >= 1e6:\n                tests_passed += 1\n            elif endurance_cycles >= 1e4:\n                warnings.append(f\"Limited endurance: {endurance_cycles:.0e} cycles\")\n            else:\n                errors.append(f\"Poor endurance: {endurance_cycles:.0e} cycles\")\n        \n        # Test 10: Compliance Checks\n        if compliance_data:\n            compliance_results = self._validate_regulatory_compliance(compliance_data)\n            tests_total += len(compliance_results)\n            tests_passed += sum(compliance_results.values())\n            \n            failed_compliance = [k for k, v in compliance_results.items() if not v]\n            if failed_compliance:\n                errors.extend([f\"Compliance failure: {std}\" for std in failed_compliance])\n        \n        # Calculate overall validation result\n        reliability_score = tests_passed / tests_total if tests_total > 0 else 0.0\n        is_valid = len(errors) == 0 and reliability_score >= 0.8\n        \n        # Apply stricter criteria for production\n        if self.test_level == HardwareTestLevel.PRODUCTION and reliability_score < 0.95:\n            is_valid = False\n        \n        validation_time = time.time() - start_time\n        \n        result = HardwareValidationResult(\n            is_valid=is_valid,\n            reliability_score=reliability_score,\n            performance_metrics=perf_metrics,\n            compliance_results=compliance_results,\n            warnings=warnings,\n            errors=errors,\n            test_level=self.test_level,\n            tests_passed=tests_passed,\n            tests_total=tests_total,\n            validation_time=validation_time\n        )\n        \n        self.logger.info(f\"Hardware validation completed: {'PASSED' if is_valid else 'FAILED'} \"\n                        f\"({reliability_score:.1%} reliability)\")\n        \n        return result\n    \n    def _validate_crossbar_array(self, crossbar_array) -> bool:\n        \"\"\"Validate crossbar array structure and parameters.\"\"\"\n        try:\n            if not hasattr(crossbar_array, 'rows') or not hasattr(crossbar_array, 'cols'):\n                return False\n            \n            # Check size limits\n            if crossbar_array.rows * crossbar_array.cols > self.limits[\"max_crossbar_size\"]:\n                return False\n            \n            # Check conductance arrays exist\n            if not hasattr(crossbar_array, 'g_positive') or not hasattr(crossbar_array, 'g_negative'):\n                return False\n            \n            # Validate conductance values\n            g_pos = crossbar_array.g_positive\n            g_neg = crossbar_array.g_negative\n            \n            if not isinstance(g_pos, np.ndarray) or not isinstance(g_neg, np.ndarray):\n                return False\n            \n            if g_pos.shape != (crossbar_array.rows, crossbar_array.cols):\n                return False\n            \n            if g_neg.shape != (crossbar_array.rows, crossbar_array.cols):\n                return False\n            \n            return True\n            \n        except Exception as e:\n            self.logger.warning(f\"Crossbar validation failed: {e}\")\n            return False\n    \n    def _validate_power_consumption(self, power_mw: float) -> Tuple[bool, Dict[str, float]]:\n        \"\"\"Validate power consumption metrics.\"\"\"\n        metrics = {\n            \"power_consumption_mw\": power_mw,\n            \"power_efficiency\": min(1.0, 1000.0 / max(1.0, power_mw))  # Normalized efficiency\n        }\n        \n        is_valid = power_mw <= self.limits[\"max_power_mw\"]\n        \n        if power_mw > self.limits[\"max_power_mw\"] * 0.8:\n            metrics[\"power_warning\"] = True\n        \n        return is_valid, metrics\n    \n    def _validate_thermal_performance(self, temperature_c: float) -> bool:\n        \"\"\"Validate thermal performance.\"\"\"\n        return temperature_c <= self.limits[\"max_temperature_c\"]\n    \n    def _validate_conductance_range(self, crossbar_array) -> bool:\n        \"\"\"Validate conductance range is within specifications.\"\"\"\n        try:\n            g_pos = crossbar_array.g_positive\n            g_neg = crossbar_array.g_negative\n            \n            min_g = min(g_pos.min(), g_neg.min())\n            max_g = max(g_pos.max(), g_neg.max())\n            \n            return (min_g >= self.limits[\"min_conductance_s\"] and \n                   max_g <= self.limits[\"max_conductance_s\"])\n        except:\n            return False\n    \n    def _validate_signal_integrity(self, crossbar_array) -> Dict[str, float]:\n        \"\"\"Validate signal integrity and compute SNR.\"\"\"\n        try:\n            # Simulate a test pattern\n            test_input = np.ones(crossbar_array.rows) * 0.5\n            output = crossbar_array.compute_vmm(test_input)\n            \n            # Estimate SNR (simplified)\n            signal_power = np.mean(output**2)\n            noise_power = np.var(output) * 0.1  # Assume 10% of variance is noise\n            \n            if noise_power > 0:\n                snr_ratio = signal_power / noise_power\n                snr_db = 10 * np.log10(snr_ratio)\n            else:\n                snr_db = 60.0  # High SNR if no noise detected\n            \n            return {\n                \"snr_db\": snr_db,\n                \"signal_power\": signal_power,\n                \"noise_power\": noise_power\n            }\n            \n        except Exception as e:\n            self.logger.warning(f\"Signal integrity validation failed: {e}\")\n            return {\"snr_db\": 0.0, \"signal_power\": 0.0, \"noise_power\": float('inf')}\n    \n    def _measure_latency(self, crossbar_array) -> float:\n        \"\"\"Measure computational latency.\"\"\"\n        try:\n            test_input = np.random.random(crossbar_array.rows)\n            \n            # Measure computation time\n            start_time = time.time()\n            \n            # Run multiple iterations for accurate measurement\n            for _ in range(100):\n                _ = crossbar_array.compute_vmm(test_input)\n            \n            end_time = time.time()\n            \n            # Convert to milliseconds per operation\n            latency_ms = (end_time - start_time) * 1000 / 100\n            \n            return latency_ms\n            \n        except Exception:\n            return float('inf')\n    \n    def _assess_computation_accuracy(self, crossbar_array) -> float:\n        \"\"\"Assess effective computation accuracy in bits.\"\"\"\n        try:\n            # Test with known patterns\n            test_patterns = [\n                np.ones(crossbar_array.rows),\n                np.zeros(crossbar_array.rows),\n                np.random.random(crossbar_array.rows),\n                np.sin(np.linspace(0, 2*np.pi, crossbar_array.rows))\n            ]\n            \n            errors = []\n            \n            for pattern in test_patterns:\n                # Compute expected result (simplified)\n                expected = np.dot(crossbar_array.g_positive.T, pattern) - \\\n                          np.dot(crossbar_array.g_negative.T, pattern)\n                \n                # Compute actual result\n                actual = crossbar_array.compute_vmm(pattern)\n                \n                # Compute relative error\n                rel_error = np.mean(np.abs(actual - expected) / (np.abs(expected) + 1e-12))\n                errors.append(rel_error)\n            \n            # Estimate effective bits from average error\n            avg_error = np.mean(errors)\n            if avg_error > 0:\n                effective_bits = -np.log2(avg_error)\n            else:\n                effective_bits = 16.0  # High precision\n            \n            return min(16.0, max(1.0, effective_bits))\n            \n        except Exception:\n            return 1.0\n    \n    def _assess_device_variability(self, crossbar_array) -> float:\n        \"\"\"Assess device-to-device variability.\"\"\"\n        try:\n            g_pos = crossbar_array.g_positive\n            g_neg = crossbar_array.g_negative\n            \n            # Compute coefficient of variation for conductances\n            cv_pos = np.std(g_pos) / (np.mean(g_pos) + 1e-12)\n            cv_neg = np.std(g_neg) / (np.mean(g_neg) + 1e-12)\n            \n            avg_cv = (cv_pos + cv_neg) / 2\n            \n            # Convert to variability score (0 = high variability, 1 = low variability)\n            variability_score = np.exp(-avg_cv * 5)  # Exponential decay\n            \n            return min(1.0, variability_score)\n            \n        except Exception:\n            return 0.0\n    \n    def _test_endurance(self, crossbar_array) -> float:\n        \"\"\"Test endurance (simplified simulation).\"\"\"\n        try:\n            # Simulate endurance testing\n            # In practice, this would involve repeated programming cycles\n            \n            # Estimate based on conductance stability\n            g_pos = crossbar_array.g_positive\n            g_stability = 1.0 - np.std(g_pos) / (np.mean(g_pos) + 1e-12)\n            \n            # Convert stability to estimated endurance cycles\n            if g_stability > 0.9:\n                endurance = 1e7  # High endurance\n            elif g_stability > 0.7:\n                endurance = 1e5  # Medium endurance\n            else:\n                endurance = 1e3   # Low endurance\n            \n            return endurance\n            \n        except Exception:\n            return 1e2\n    \n    def _validate_regulatory_compliance(self, compliance_data: Dict[str, Any]) -> Dict[str, bool]:\n        \"\"\"Validate regulatory compliance.\"\"\"\n        results = {}\n        \n        # EMC compliance\n        if \"emc_test_report\" in compliance_data:\n            results[\"EMC\"] = compliance_data[\"emc_test_report\"].get(\"passed\", False)\n        \n        # Safety compliance\n        if \"safety_certification\" in compliance_data:\n            results[\"Safety\"] = compliance_data[\"safety_certification\"].get(\"certified\", False)\n        \n        # Environmental compliance (RoHS, REACH)\n        if \"environmental_data\" in compliance_data:\n            env_data = compliance_data[\"environmental_data\"]\n            results[\"RoHS\"] = env_data.get(\"rohs_compliant\", True)\n            results[\"REACH\"] = env_data.get(\"reach_compliant\", True)\n        \n        # Export control\n        if \"export_control\" in compliance_data:\n            results[\"Export_Control\"] = compliance_data[\"export_control\"].get(\"approved\", True)\n        \n        return results\n    \n    def generate_hardware_report(self, result: HardwareValidationResult) -> str:\n        \"\"\"Generate comprehensive hardware validation report.\"\"\"\n        report_lines = [\n            \"=\" * 60,\n            \"ANALOG PDE SOLVER - HARDWARE VALIDATION REPORT\",\n            \"=\" * 60,\n            f\"Test Level: {result.test_level.value.upper()}\",\n            f\"Overall Result: {'\u2705 PASSED' if result.is_valid else '\u274c FAILED'}\",\n            f\"Reliability Score: {result.reliability_score:.1%}\",\n            f\"Tests Passed: {result.tests_passed}/{result.tests_total}\",\n            f\"Validation Time: {result.validation_time:.2f}s\",\n            \"\"\n        ]\n        \n        # Performance metrics\n        if result.performance_metrics:\n            report_lines.append(\"\ud83d\udcca Performance Metrics:\")\n            for metric, value in result.performance_metrics.items():\n                if isinstance(value, float):\n                    report_lines.append(f\"  {metric}: {value:.3f}\")\n                else:\n                    report_lines.append(f\"  {metric}: {value}\")\n            report_lines.append(\"\")\n        \n        # Compliance results\n        if result.compliance_results:\n            report_lines.append(\"\ud83d\udccb Regulatory Compliance:\")\n            for standard, passed in result.compliance_results.items():\n                status = \"\u2705 PASS\" if passed else \"\u274c FAIL\"\n                report_lines.append(f\"  {standard}: {status}\")\n            report_lines.append(\"\")\n        \n        # Errors\n        if result.errors:\n            report_lines.append(\"\u274c Critical Issues:\")\n            for error in result.errors:\n                report_lines.append(f\"  \u2022 {error}\")\n            report_lines.append(\"\")\n        \n        # Warnings\n        if result.warnings:\n            report_lines.append(\"\u26a0\ufe0f  Warnings:\")\n            for warning in result.warnings:\n                report_lines.append(f\"  \u2022 {warning}\")\n            report_lines.append(\"\")\n        \n        # Recommendations\n        report_lines.append(\"\ud83d\udd27 Recommendations:\")\n        if result.reliability_score < 0.8:\n            report_lines.append(\"  \u2022 Hardware requires significant improvements for production use\")\n        if \"power_consumption_mw\" in result.performance_metrics:\n            power = result.performance_metrics[\"power_consumption_mw\"]\n            if power > 500:\n                report_lines.append(f\"  \u2022 Consider power optimization (current: {power:.1f}mW)\")\n        if \"snr_db\" in result.performance_metrics:\n            snr = result.performance_metrics[\"snr_db\"]\n            if snr < 25:\n                report_lines.append(f\"  \u2022 Improve signal integrity (current SNR: {snr:.1f}dB)\")\n        \n        report_lines.extend([\n            \"\",\n            \"=\" * 60,\n            \"Report generated by Terragon Labs Hardware Validation Suite\",\n            \"=\" * 60\n        ])\n        \n        return \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/e2e/test_full_pipeline.py",
      "line_number": 1,
      "details": "\"\"\"\nEnd-to-end tests for the complete analog PDE solver pipeline.\n\"\"\"\nimport pytest\nimport numpy as np\nimport tempfile\nfrom pathlib import Path\nfrom unittest.mock import patch, MagicMock\n\n# Import test fixtures\nfrom tests.fixtures.pde_fixtures import poisson_2d_simple, hardware_configurations\n\n\n@pytest.mark.integration\n@pytest.mark.slow\nclass TestFullPipeline:\n    \"\"\"End-to-end pipeline tests.\"\"\"\n    \n    def test_complete_poisson_solver_pipeline(self, poisson_2d_simple, temp_dir):\n        \"\"\"Test complete pipeline from PDE specification to analog solution.\"\"\"\n        \n        # Mock the analog PDE solver components for testing\n        with patch('analog_pde_solver.core.solver.AnalogPDESolver') as MockSolver:\n            # Setup mock solver\n            mock_solver = MagicMock()\n            MockSolver.return_value = mock_solver\n            \n            # Mock the solve method to return a reasonable solution\n            grid_size = poisson_2d_simple.grid_size\n            x = np.linspace(0, 1, grid_size[0])\n            y = np.linspace(0, 1, grid_size[1])\n            X, Y = np.meshgrid(x, y)\n            mock_solution = poisson_2d_simple.analytical_solution(X, Y)\n            mock_solver.solve.return_value = mock_solution\n            \n            # Test the pipeline\n            try:\n                # 1. Create PDE specification\n                pde_spec = {\n                    \"type\": \"poisson\",\n                    \"grid_size\": grid_size,\n                    \"boundary_conditions\": poisson_2d_simple.boundary_conditions\n                }\n                \n                # 2. Initialize analog solver\n                solver = MockSolver(\n                    crossbar_size=max(grid_size),\n                    conductance_range=(1e-9, 1e-6),\n                    noise_model=\"realistic\"\n                )\n                \n                # 3. Map PDE to analog hardware\n                hardware_config = {\"mapped\": True}\n                mock_solver.map_pde_to_crossbar.return_value = hardware_config\n                \n                # 4. Solve the PDE\n                solution = mock_solver.solve(\n                    iterations=100,\n                    convergence_threshold=poisson_2d_simple.tolerance\n                )\n                \n                # 5. Verify solution quality\n                assert solution is not None\n                assert solution.shape == grid_size\n                \n                # Check that solution is reasonable (not all zeros or NaN)\n                assert not np.all(solution == 0), \"Solution is all zeros\"\n                assert not np.any(np.isnan(solution)), \"Solution contains NaN\"\n                assert not np.any(np.isinf(solution)), \"Solution contains infinity\"\n                \n                # 6. Generate RTL (mocked)\n                rtl_output = temp_dir / \"poisson_solver.v\"\n                mock_solver.to_rtl.return_value.save.return_value = str(rtl_output)\n                \n                # Verify RTL generation was called\n                mock_solver.to_rtl.assert_called_once()\n                \n            except Exception as e:\n                pytest.fail(f\"Pipeline test failed with exception: {e}\")\n    \n    def test_navier_stokes_pipeline(self, temp_dir):\n        \"\"\"Test Navier-Stokes solver pipeline.\"\"\"\n        \n        with patch('analog_pde_solver.NavierStokesAnalog') as MockNS:\n            mock_ns = MagicMock()\n            MockNS.return_value = mock_ns\n            \n            # Mock fluid simulation results\n            grid_size = (128, 128)\n            mock_u = np.random.random(grid_size) * 0.1\n            mock_v = np.random.random(grid_size) * 0.1\n            mock_pressure = np.random.random(grid_size)\n            \n            mock_ns.update_velocity.return_value = (mock_u, mock_v)\n            mock_ns.solve_pressure_poisson.return_value = mock_pressure\n            mock_ns.apply_pressure_correction.return_value = (mock_u, mock_v)\n            \n            try:\n                # Initialize Navier-Stokes solver\n                ns_solver = MockNS(\n                    resolution=(128, 128),\n                    reynolds_number=1000,\n                    time_step=0.001\n                )\n                \n                # Configure hardware\n                ns_solver.configure_hardware(\n                    num_crossbars=4,\n                    precision_bits=8,\n                    update_scheme=\"semi-implicit\"\n                )\n                \n                # Run simulation for a few timesteps\n                for timestep in range(10):\n                    u, v = ns_solver.update_velocity()\n                    pressure = ns_solver.solve_pressure_poisson()\n                    u, v = ns_solver.apply_pressure_correction(u, v, pressure)\n                    \n                    # Verify results are reasonable\n                    assert u is not None and v is not None\n                    assert pressure is not None\n                    assert u.shape == grid_size\n                    assert v.shape == grid_size\n                    assert pressure.shape == grid_size\n                \n                # Analyze power consumption\n                mock_power_analysis = MagicMock()\n                mock_power_analysis.avg_power_mw = 5.2\n                mock_power_analysis.energy_per_iter_nj = 0.8\n                mock_ns.analyze_power.return_value = mock_power_analysis\n                \n                power_analysis = ns_solver.analyze_power()\n                assert power_analysis.avg_power_mw > 0\n                assert power_analysis.energy_per_iter_nj > 0\n                \n            except Exception as e:\n                pytest.fail(f\"Navier-Stokes pipeline test failed: {e}\")\n    \n    @pytest.mark.hardware\n    def test_spice_integration_pipeline(self, hardware_configurations, temp_dir):\n        \"\"\"Test SPICE integration pipeline.\"\"\"\n        \n        config = hardware_configurations[0]  # Use small crossbar config\n        \n        with patch('analog_pde_solver.spice.SPICESimulator') as MockSpice:\n            mock_spice = MagicMock()\n            MockSpice.return_value = mock_spice\n            \n            # Mock SPICE simulation results\n            mock_results = MagicMock()\n            mock_results.get_node_voltages.return_value = np.random.random((64, 64))\n            mock_spice.transient.return_value = mock_results\n            \n            try:\n                # Create SPICE simulator\n                spice_sim = MockSpice()\n                \n                # Add crossbar components\n                rows, cols = config[\"size\"]\n                for i in range(min(rows, 8)):  # Limit for testing\n                    for j in range(min(cols, 8)):\n                        spice_sim.add_component(\n                            f\"R_{i}_{j}\",\n                            \"memristor\",\n                            nodes=(f\"row_{i}\", f\"col_{j}\"),\n                            params={\n                                \"ron\": 1e3,\n                                \"roff\": 1e6,\n                                \"rinit\": 1e4,\n                                \"model\": \"hp_memristor\"\n                            }\n                        )\n                \n                # Add peripheral circuits\n                spice_sim.add_dac_array(\"input_dac\", resolution=config[\"dac_bits\"], voltage_range=1.0)\n                spice_sim.add_adc_array(\"output_adc\", resolution=config[\"adc_bits\"], sampling_rate=1e6)\n                \n                # Run simulation\n                results = spice_sim.transient(\n                    stop_time=1e-3,\n                    time_step=1e-6,\n                    initial_conditions={\"initial\": \"state\"}\n                )\n                \n                # Extract and verify solution\n                analog_solution = results.get_node_voltages(\"output_nodes\")\n                assert analog_solution is not None\n                assert analog_solution.shape == (64, 64)\n                assert not np.any(np.isnan(analog_solution))\n                \n            except Exception as e:\n                pytest.fail(f\"SPICE integration pipeline test failed: {e}\")\n    \n    def test_rtl_generation_pipeline(self, temp_dir):\n        \"\"\"Test RTL generation pipeline.\"\"\"\n        \n        with patch('analog_pde_solver.compiler.TorchToAnalog') as MockCompiler:\n            mock_compiler = MagicMock()\n            MockCompiler.return_value = mock_compiler\n            \n            # Mock compiled model\n            mock_analog_model = MagicMock()\n            mock_compiler.compile.return_value = mock_analog_model\n            \n            try:\n                # Create compiler\n                compiler = MockCompiler()\n                \n                # Mock PyTorch model compilation\n                mock_model = MagicMock()\n                analog_model = compiler.compile(\n                    model=mock_model,\n                    target_hardware=\"crossbar_array\",\n                    optimization_level=3\n                )\n                \n                # Generate RTL files\n                rtl_file = temp_dir / \"pde_accelerator.v\"\n                constraints_file = temp_dir / \"constraints.xdc\"\n                \n                analog_model.export_rtl.return_value = str(rtl_file)\n                analog_model.export_constraints.return_value = str(constraints_file)\n                \n                # Verify RTL generation\n                rtl_output = analog_model.export_rtl(\"pde_accelerator.v\")\n                constraints_output = analog_model.export_constraints(\"constraints.xdc\")\n                \n                assert rtl_output is not None\n                assert constraints_output is not None\n                \n                # Verify methods were called\n                analog_model.export_rtl.assert_called_once()\n                analog_model.export_constraints.assert_called_once()\n                \n            except Exception as e:\n                pytest.fail(f\"RTL generation pipeline test failed: {e}\")\n    \n    def test_error_handling_pipeline(self):\n        \"\"\"Test error handling throughout the pipeline.\"\"\"\n        \n        with patch('analog_pde_solver.core.solver.AnalogPDESolver') as MockSolver:\n            # Test convergence failure\n            mock_solver = MagicMock()\n            MockSolver.return_value = mock_solver\n            mock_solver.solve.side_effect = RuntimeError(\"Convergence failed\")\n            \n            with pytest.raises(RuntimeError, match=\"Convergence failed\"):\n                solver = MockSolver()\n                solver.solve()\n            \n            # Test invalid hardware configuration\n            mock_solver.solve.side_effect = ValueError(\"Invalid crossbar size\")\n            \n            with pytest.raises(ValueError, match=\"Invalid crossbar size\"):\n                solver = MockSolver()\n                solver.solve()\n\n\nif __name__ == \"__main__\":\n    pytest.main([__file__, \"-v\"])"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "tests/performance/test_performance_benchmarks.py",
      "line_number": 1,
      "details": "\"\"\"\nPerformance benchmarking tests for analog PDE solver.\n\"\"\"\nimport pytest\nimport time\nimport psutil\nimport numpy as np\nfrom contextlib import contextmanager\nfrom typing import Dict, Any\n\n# Import the performance fixtures\nfrom tests.fixtures.pde_fixtures import performance_benchmarks\n\n\nclass PerformanceMonitor:\n    \"\"\"Monitor performance metrics during test execution.\"\"\"\n    \n    def __init__(self):\n        self.start_time = None\n        self.end_time = None\n        self.start_memory = None\n        self.end_memory = None\n        self.peak_memory = None\n        \n    @contextmanager\n    def monitor(self):\n        \"\"\"Context manager for performance monitoring.\"\"\"\n        # Record start metrics\n        self.start_time = time.perf_counter()\n        self.start_memory = psutil.Process().memory_info().rss\n        self.peak_memory = self.start_memory\n        \n        try:\n            yield self\n        finally:\n            # Record end metrics\n            self.end_time = time.perf_counter()\n            self.end_memory = psutil.Process().memory_info().rss\n            self.peak_memory = max(self.peak_memory, self.end_memory)\n    \n    @property\n    def elapsed_time(self) -> float:\n        \"\"\"Get elapsed time in seconds.\"\"\"\n        if self.start_time and self.end_time:\n            return self.end_time - self.start_time\n        return 0.0\n    \n    @property\n    def memory_usage(self) -> int:\n        \"\"\"Get peak memory usage in bytes.\"\"\"\n        return self.peak_memory - self.start_memory if self.peak_memory and self.start_memory else 0\n\n\n@pytest.mark.slow\n@pytest.mark.performance\nclass TestPerformanceBenchmarks:\n    \"\"\"Performance benchmark test suite.\"\"\"\n    \n    def test_small_problem_performance(self, performance_benchmarks):\n        \"\"\"Test performance on small problems.\"\"\"\n        benchmark = performance_benchmarks[\"small\"]\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            # Simulate analog PDE solving\n            grid_size = benchmark[\"grid_size\"]\n            matrix = np.random.random(grid_size)\n            \n            # Simulate iterative solving\n            for _ in range(100):\n                matrix = 0.25 * (\n                    np.roll(matrix, 1, axis=0) +\n                    np.roll(matrix, -1, axis=0) +\n                    np.roll(matrix, 1, axis=1) +\n                    np.roll(matrix, -1, axis=1)\n                )\n        \n        # Verify performance constraints\n        assert monitor.elapsed_time < benchmark[\"max_time\"], \\\n            f\"Small problem took {monitor.elapsed_time:.2f}s, expected < {benchmark['max_time']}s\"\n        \n        assert monitor.memory_usage < benchmark[\"max_memory\"], \\\n            f\"Small problem used {monitor.memory_usage} bytes, expected < {benchmark['max_memory']} bytes\"\n    \n    @pytest.mark.slow\n    def test_medium_problem_performance(self, performance_benchmarks):\n        \"\"\"Test performance on medium problems.\"\"\"\n        benchmark = performance_benchmarks[\"medium\"]\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            grid_size = benchmark[\"grid_size\"]\n            matrix = np.random.random(grid_size)\n            \n            # Simulate more intensive computation\n            for _ in range(50):\n                matrix = 0.25 * (\n                    np.roll(matrix, 1, axis=0) +\n                    np.roll(matrix, -1, axis=0) +\n                    np.roll(matrix, 1, axis=1) +\n                    np.roll(matrix, -1, axis=1)\n                )\n        \n        assert monitor.elapsed_time < benchmark[\"max_time\"]\n        assert monitor.memory_usage < benchmark[\"max_memory\"]\n    \n    @pytest.mark.slow\n    @pytest.mark.hardware\n    def test_large_problem_performance(self, performance_benchmarks):\n        \"\"\"Test performance on large problems (requires hardware acceleration).\"\"\"\n        benchmark = performance_benchmarks[\"large\"]\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            grid_size = benchmark[\"grid_size\"]\n            # For large problems, we'd typically use hardware acceleration\n            matrix = np.random.random(grid_size)\n            \n            # Simulate hardware-accelerated computation\n            for _ in range(10):  # Fewer iterations due to hardware speedup\n                matrix = 0.25 * (\n                    np.roll(matrix, 1, axis=0) +\n                    np.roll(matrix, -1, axis=0) +\n                    np.roll(matrix, 1, axis=1) +\n                    np.roll(matrix, -1, axis=1)\n                )\n        \n        assert monitor.elapsed_time < benchmark[\"max_time\"]\n        assert monitor.memory_usage < benchmark[\"max_memory\"]\n    \n    def test_convergence_performance(self):\n        \"\"\"Test convergence rate performance.\"\"\"\n        grid_size = (128, 128)\n        tolerance = 1e-6\n        max_iterations = 1000\n        \n        # Create test problem\n        solution = np.random.random(grid_size)\n        target = np.zeros_like(solution)\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            for iteration in range(max_iterations):\n                old_solution = solution.copy()\n                \n                # Gauss-Seidel iteration\n                solution[1:-1, 1:-1] = 0.25 * (\n                    solution[0:-2, 1:-1] +  # up\n                    solution[2:, 1:-1] +    # down\n                    solution[1:-1, 0:-2] +  # left\n                    solution[1:-1, 2:]      # right\n                )\n                \n                # Check convergence\n                residual = np.max(np.abs(solution - old_solution))\n                if residual < tolerance:\n                    break\n        \n        # Verify reasonable convergence\n        assert iteration < max_iterations * 0.8, \\\n            f\"Convergence took {iteration} iterations, expected < {max_iterations * 0.8}\"\n        \n        # Verify performance\n        time_per_iteration = monitor.elapsed_time / (iteration + 1)\n        assert time_per_iteration < 0.1, \\\n            f\"Average time per iteration: {time_per_iteration:.3f}s, expected < 0.1s\"\n\n\n@pytest.mark.performance\ndef test_memory_scaling():\n    \"\"\"Test memory usage scaling with problem size.\"\"\"\n    sizes = [(32, 32), (64, 64), (128, 128)]\n    memory_usage = []\n    \n    for size in sizes:\n        monitor = PerformanceMonitor()\n        \n        with monitor.monitor():\n            # Allocate arrays similar to analog solver\n            matrix = np.random.random(size)\n            conductance_pos = np.random.random(size)\n            conductance_neg = np.random.random(size)\n            \n            # Simulate some computation\n            result = matrix @ conductance_pos.T - matrix @ conductance_neg.T\n        \n        memory_usage.append(monitor.memory_usage)\n    \n    # Verify roughly linear scaling (within 2x tolerance)\n    for i in range(1, len(memory_usage)):\n        size_ratio = (sizes[i][0] * sizes[i][1]) / (sizes[i-1][0] * sizes[i-1][1])\n        memory_ratio = memory_usage[i] / memory_usage[i-1]\n        \n        assert memory_ratio < size_ratio * 2, \\\n            f\"Memory scaling worse than expected: {memory_ratio:.2f} vs {size_ratio:.2f}\"\n\n\nif __name__ == \"__main__\":\n    # Run performance tests\n    pytest.main([__file__, \"-v\", \"--tb=short\"])"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_internal/cli/cmdoptions.py",
      "line_number": 1,
      "details": "\"\"\"\nshared options and groups\n\nThe principle here is to define options once, but *not* instantiate them\nglobally. One reason being that options with action='append' can carry state\nbetween parses. pip parses general options twice internally, and shouldn't\npass on state. To be consistent, all options will follow this design.\n\"\"\"\n\n# The following comment should be removed at some point in the future.\n# mypy: strict-optional=False\n\nimport importlib.util\nimport logging\nimport os\nimport textwrap\nfrom functools import partial\nfrom optparse import SUPPRESS_HELP, Option, OptionGroup, OptionParser, Values\nfrom textwrap import dedent\nfrom typing import Any, Callable, Dict, Optional, Tuple\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.cli.parser import ConfigOptionParser\nfrom pip._internal.exceptions import CommandError\nfrom pip._internal.locations import USER_CACHE_DIR, get_src_prefix\nfrom pip._internal.models.format_control import FormatControl\nfrom pip._internal.models.index import PyPI\nfrom pip._internal.models.target_python import TargetPython\nfrom pip._internal.utils.hashes import STRONG_HASHES\nfrom pip._internal.utils.misc import strtobool\n\nlogger = logging.getLogger(__name__)\n\n\ndef raise_option_error(parser: OptionParser, option: Option, msg: str) -> None:\n    \"\"\"\n    Raise an option parsing error using parser.error().\n\n    Args:\n      parser: an OptionParser instance.\n      option: an Option instance.\n      msg: the error text.\n    \"\"\"\n    msg = f\"{option} error: {msg}\"\n    msg = textwrap.fill(\" \".join(msg.split()))\n    parser.error(msg)\n\n\ndef make_option_group(group: Dict[str, Any], parser: ConfigOptionParser) -> OptionGroup:\n    \"\"\"\n    Return an OptionGroup object\n    group  -- assumed to be dict with 'name' and 'options' keys\n    parser -- an optparse Parser\n    \"\"\"\n    option_group = OptionGroup(parser, group[\"name\"])\n    for option in group[\"options\"]:\n        option_group.add_option(option())\n    return option_group\n\n\ndef check_dist_restriction(options: Values, check_target: bool = False) -> None:\n    \"\"\"Function for determining if custom platform options are allowed.\n\n    :param options: The OptionParser options.\n    :param check_target: Whether or not to check if --target is being used.\n    \"\"\"\n    dist_restriction_set = any(\n        [\n            options.python_version,\n            options.platforms,\n            options.abis,\n            options.implementation,\n        ]\n    )\n\n    binary_only = FormatControl(set(), {\":all:\"})\n    sdist_dependencies_allowed = (\n        options.format_control != binary_only and not options.ignore_dependencies\n    )\n\n    # Installations or downloads using dist restrictions must not combine\n    # source distributions and dist-specific wheels, as they are not\n    # guaranteed to be locally compatible.\n    if dist_restriction_set and sdist_dependencies_allowed:\n        raise CommandError(\n            \"When restricting platform and interpreter constraints using \"\n            \"--python-version, --platform, --abi, or --implementation, \"\n            \"either --no-deps must be set, or --only-binary=:all: must be \"\n            \"set and --no-binary must not be set (or must be set to \"\n            \":none:).\"\n        )\n\n    if check_target:\n        if not options.dry_run and dist_restriction_set and not options.target_dir:\n            raise CommandError(\n                \"Can not use any platform or abi specific options unless \"\n                \"installing via '--target' or using '--dry-run'\"\n            )\n\n\ndef _path_option_check(option: Option, opt: str, value: str) -> str:\n    return os.path.expanduser(value)\n\n\ndef _package_name_option_check(option: Option, opt: str, value: str) -> str:\n    return canonicalize_name(value)\n\n\nclass PipOption(Option):\n    TYPES = Option.TYPES + (\"path\", \"package_name\")\n    TYPE_CHECKER = Option.TYPE_CHECKER.copy()\n    TYPE_CHECKER[\"package_name\"] = _package_name_option_check\n    TYPE_CHECKER[\"path\"] = _path_option_check\n\n\n###########\n# options #\n###########\n\nhelp_: Callable[..., Option] = partial(\n    Option,\n    \"-h\",\n    \"--help\",\n    dest=\"help\",\n    action=\"help\",\n    help=\"Show help.\",\n)\n\ndebug_mode: Callable[..., Option] = partial(\n    Option,\n    \"--debug\",\n    dest=\"debug_mode\",\n    action=\"store_true\",\n    default=False,\n    help=(\n        \"Let unhandled exceptions propagate outside the main subroutine, \"\n        \"instead of logging them to stderr.\"\n    ),\n)\n\nisolated_mode: Callable[..., Option] = partial(\n    Option,\n    \"--isolated\",\n    dest=\"isolated_mode\",\n    action=\"store_true\",\n    default=False,\n    help=(\n        \"Run pip in an isolated mode, ignoring environment variables and user \"\n        \"configuration.\"\n    ),\n)\n\nrequire_virtualenv: Callable[..., Option] = partial(\n    Option,\n    \"--require-virtualenv\",\n    \"--require-venv\",\n    dest=\"require_venv\",\n    action=\"store_true\",\n    default=False,\n    help=(\n        \"Allow pip to only run in a virtual environment; \"\n        \"exit with an error otherwise.\"\n    ),\n)\n\noverride_externally_managed: Callable[..., Option] = partial(\n    Option,\n    \"--break-system-packages\",\n    dest=\"override_externally_managed\",\n    action=\"store_true\",\n    help=\"Allow pip to modify an EXTERNALLY-MANAGED Python installation\",\n)\n\npython: Callable[..., Option] = partial(\n    Option,\n    \"--python\",\n    dest=\"python\",\n    help=\"Run pip with the specified Python interpreter.\",\n)\n\nverbose: Callable[..., Option] = partial(\n    Option,\n    \"-v\",\n    \"--verbose\",\n    dest=\"verbose\",\n    action=\"count\",\n    default=0,\n    help=\"Give more output. Option is additive, and can be used up to 3 times.\",\n)\n\nno_color: Callable[..., Option] = partial(\n    Option,\n    \"--no-color\",\n    dest=\"no_color\",\n    action=\"store_true\",\n    default=False,\n    help=\"Suppress colored output.\",\n)\n\nversion: Callable[..., Option] = partial(\n    Option,\n    \"-V\",\n    \"--version\",\n    dest=\"version\",\n    action=\"store_true\",\n    help=\"Show version and exit.\",\n)\n\nquiet: Callable[..., Option] = partial(\n    Option,\n    \"-q\",\n    \"--quiet\",\n    dest=\"quiet\",\n    action=\"count\",\n    default=0,\n    help=(\n        \"Give less output. Option is additive, and can be used up to 3\"\n        \" times (corresponding to WARNING, ERROR, and CRITICAL logging\"\n        \" levels).\"\n    ),\n)\n\nprogress_bar: Callable[..., Option] = partial(\n    Option,\n    \"--progress-bar\",\n    dest=\"progress_bar\",\n    type=\"choice\",\n    choices=[\"on\", \"off\"],\n    default=\"on\",\n    help=\"Specify whether the progress bar should be used [on, off] (default: on)\",\n)\n\nlog: Callable[..., Option] = partial(\n    PipOption,\n    \"--log\",\n    \"--log-file\",\n    \"--local-log\",\n    dest=\"log\",\n    metavar=\"path\",\n    type=\"path\",\n    help=\"Path to a verbose appending log.\",\n)\n\nno_input: Callable[..., Option] = partial(\n    Option,\n    # Don't ask for input\n    \"--no-input\",\n    dest=\"no_input\",\n    action=\"store_true\",\n    default=False,\n    help=\"Disable prompting for input.\",\n)\n\nkeyring_provider: Callable[..., Option] = partial(\n    Option,\n    \"--keyring-provider\",\n    dest=\"keyring_provider\",\n    choices=[\"auto\", \"disabled\", \"import\", \"subprocess\"],\n    default=\"auto\",\n    help=(\n        \"Enable the credential lookup via the keyring library if user input is allowed.\"\n        \" Specify which mechanism to use [disabled, import, subprocess].\"\n        \" (default: disabled)\"\n    ),\n)\n\nproxy: Callable[..., Option] = partial(\n    Option,\n    \"--proxy\",\n    dest=\"proxy\",\n    type=\"str\",\n    default=\"\",\n    help=\"Specify a proxy in the form scheme://[user:passwd@]proxy.server:port.\",\n)\n\nretries: Callable[..., Option] = partial(\n    Option,\n    \"--retries\",\n    dest=\"retries\",\n    type=\"int\",\n    default=5,\n    help=\"Maximum number of retries each connection should attempt \"\n    \"(default %default times).\",\n)\n\ntimeout: Callable[..., Option] = partial(\n    Option,\n    \"--timeout\",\n    \"--default-timeout\",\n    metavar=\"sec\",\n    dest=\"timeout\",\n    type=\"float\",\n    default=15,\n    help=\"Set the socket timeout (default %default seconds).\",\n)\n\n\ndef exists_action() -> Option:\n    return Option(\n        # Option when path already exist\n        \"--exists-action\",\n        dest=\"exists_action\",\n        type=\"choice\",\n        choices=[\"s\", \"i\", \"w\", \"b\", \"a\"],\n        default=[],\n        action=\"append\",\n        metavar=\"action\",\n        help=\"Default action when a path already exists: \"\n        \"(s)witch, (i)gnore, (w)ipe, (b)ackup, (a)bort.\",\n    )\n\n\ncert: Callable[..., Option] = partial(\n    PipOption,\n    \"--cert\",\n    dest=\"cert\",\n    type=\"path\",\n    metavar=\"path\",\n    help=(\n        \"Path to PEM-encoded CA certificate bundle. \"\n        \"If provided, overrides the default. \"\n        \"See 'SSL Certificate Verification' in pip documentation \"\n        \"for more information.\"\n    ),\n)\n\nclient_cert: Callable[..., Option] = partial(\n    PipOption,\n    \"--client-cert\",\n    dest=\"client_cert\",\n    type=\"path\",\n    default=None,\n    metavar=\"path\",\n    help=\"Path to SSL client certificate, a single file containing the \"\n    \"private key and the certificate in PEM format.\",\n)\n\nindex_url: Callable[..., Option] = partial(\n    Option,\n    \"-i\",\n    \"--index-url\",\n    \"--pypi-url\",\n    dest=\"index_url\",\n    metavar=\"URL\",\n    default=PyPI.simple_url,\n    help=\"Base URL of the Python Package Index (default %default). \"\n    \"This should point to a repository compliant with PEP 503 \"\n    \"(the simple repository API) or a local directory laid out \"\n    \"in the same format.\",\n)\n\n\ndef extra_index_url() -> Option:\n    return Option(\n        \"--extra-index-url\",\n        dest=\"extra_index_urls\",\n        metavar=\"URL\",\n        action=\"append\",\n        default=[],\n        help=\"Extra URLs of package indexes to use in addition to \"\n        \"--index-url. Should follow the same rules as \"\n        \"--index-url.\",\n    )\n\n\nno_index: Callable[..., Option] = partial(\n    Option,\n    \"--no-index\",\n    dest=\"no_index\",\n    action=\"store_true\",\n    default=False,\n    help=\"Ignore package index (only looking at --find-links URLs instead).\",\n)\n\n\ndef find_links() -> Option:\n    return Option(\n        \"-f\",\n        \"--find-links\",\n        dest=\"find_links\",\n        action=\"append\",\n        default=[],\n        metavar=\"url\",\n        help=\"If a URL or path to an html file, then parse for links to \"\n        \"archives such as sdist (.tar.gz) or wheel (.whl) files. \"\n        \"If a local path or file:// URL that's a directory, \"\n        \"then look for archives in the directory listing. \"\n        \"Links to VCS project URLs are not supported.\",\n    )\n\n\ndef trusted_host() -> Option:\n    return Option(\n        \"--trusted-host\",\n        dest=\"trusted_hosts\",\n        action=\"append\",\n        metavar=\"HOSTNAME\",\n        default=[],\n        help=\"Mark this host or host:port pair as trusted, even though it \"\n        \"does not have valid or any HTTPS.\",\n    )\n\n\ndef constraints() -> Option:\n    return Option(\n        \"-c\",\n        \"--constraint\",\n        dest=\"constraints\",\n        action=\"append\",\n        default=[],\n        metavar=\"file\",\n        help=\"Constrain versions using the given constraints file. \"\n        \"This option can be used multiple times.\",\n    )\n\n\ndef requirements() -> Option:\n    return Option(\n        \"-r\",\n        \"--requirement\",\n        dest=\"requirements\",\n        action=\"append\",\n        default=[],\n        metavar=\"file\",\n        help=\"Install from the given requirements file. \"\n        \"This option can be used multiple times.\",\n    )\n\n\ndef editable() -> Option:\n    return Option(\n        \"-e\",\n        \"--editable\",\n        dest=\"editables\",\n        action=\"append\",\n        default=[],\n        metavar=\"path/url\",\n        help=(\n            \"Install a project in editable mode (i.e. setuptools \"\n            '\"develop mode\") from a local project path or a VCS url.'\n        ),\n    )\n\n\ndef _handle_src(option: Option, opt_str: str, value: str, parser: OptionParser) -> None:\n    value = os.path.abspath(value)\n    setattr(parser.values, option.dest, value)\n\n\nsrc: Callable[..., Option] = partial(\n    PipOption,\n    \"--src\",\n    \"--source\",\n    \"--source-dir\",\n    \"--source-directory\",\n    dest=\"src_dir\",\n    type=\"path\",\n    metavar=\"dir\",\n    default=get_src_prefix(),\n    action=\"callback\",\n    callback=_handle_src,\n    help=\"Directory to check out editable projects into. \"\n    'The default in a virtualenv is \"<venv path>/src\". '\n    'The default for global installs is \"<current dir>/src\".',\n)\n\n\ndef _get_format_control(values: Values, option: Option) -> Any:\n    \"\"\"Get a format_control object.\"\"\"\n    return getattr(values, option.dest)\n\n\ndef _handle_no_binary(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(\n        value,\n        existing.no_binary,\n        existing.only_binary,\n    )\n\n\ndef _handle_only_binary(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    existing = _get_format_control(parser.values, option)\n    FormatControl.handle_mutual_excludes(\n        value,\n        existing.only_binary,\n        existing.no_binary,\n    )\n\n\ndef no_binary() -> Option:\n    format_control = FormatControl(set(), set())\n    return Option(\n        \"--no-binary\",\n        dest=\"format_control\",\n        action=\"callback\",\n        callback=_handle_no_binary,\n        type=\"str\",\n        default=format_control,\n        help=\"Do not use binary packages. Can be supplied multiple times, and \"\n        'each time adds to the existing value. Accepts either \":all:\" to '\n        'disable all binary packages, \":none:\" to empty the set (notice '\n        \"the colons), or one or more package names with commas between \"\n        \"them (no colons). Note that some packages are tricky to compile \"\n        \"and may fail to install when this option is used on them.\",\n    )\n\n\ndef only_binary() -> Option:\n    format_control = FormatControl(set(), set())\n    return Option(\n        \"--only-binary\",\n        dest=\"format_control\",\n        action=\"callback\",\n        callback=_handle_only_binary,\n        type=\"str\",\n        default=format_control,\n        help=\"Do not use source packages. Can be supplied multiple times, and \"\n        'each time adds to the existing value. Accepts either \":all:\" to '\n        'disable all source packages, \":none:\" to empty the set, or one '\n        \"or more package names with commas between them. Packages \"\n        \"without binary distributions will fail to install when this \"\n        \"option is used on them.\",\n    )\n\n\nplatforms: Callable[..., Option] = partial(\n    Option,\n    \"--platform\",\n    dest=\"platforms\",\n    metavar=\"platform\",\n    action=\"append\",\n    default=None,\n    help=(\n        \"Only use wheels compatible with <platform>. Defaults to the \"\n        \"platform of the running system. Use this option multiple times to \"\n        \"specify multiple platforms supported by the target interpreter.\"\n    ),\n)\n\n\n# This was made a separate function for unit-testing purposes.\ndef _convert_python_version(value: str) -> Tuple[Tuple[int, ...], Optional[str]]:\n    \"\"\"\n    Convert a version string like \"3\", \"37\", or \"3.7.3\" into a tuple of ints.\n\n    :return: A 2-tuple (version_info, error_msg), where `error_msg` is\n        non-None if and only if there was a parsing error.\n    \"\"\"\n    if not value:\n        # The empty string is the same as not providing a value.\n        return (None, None)\n\n    parts = value.split(\".\")\n    if len(parts) > 3:\n        return ((), \"at most three version parts are allowed\")\n\n    if len(parts) == 1:\n        # Then we are in the case of \"3\" or \"37\".\n        value = parts[0]\n        if len(value) > 1:\n            parts = [value[0], value[1:]]\n\n    try:\n        version_info = tuple(int(part) for part in parts)\n    except ValueError:\n        return ((), \"each version part must be an integer\")\n\n    return (version_info, None)\n\n\ndef _handle_python_version(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"\n    Handle a provided --python-version value.\n    \"\"\"\n    version_info, error_msg = _convert_python_version(value)\n    if error_msg is not None:\n        msg = f\"invalid --python-version value: {value!r}: {error_msg}\"\n        raise_option_error(parser, option=option, msg=msg)\n\n    parser.values.python_version = version_info\n\n\npython_version: Callable[..., Option] = partial(\n    Option,\n    \"--python-version\",\n    dest=\"python_version\",\n    metavar=\"python_version\",\n    action=\"callback\",\n    callback=_handle_python_version,\n    type=\"str\",\n    default=None,\n    help=dedent(\n        \"\"\"\\\n    The Python interpreter version to use for wheel and \"Requires-Python\"\n    compatibility checks. Defaults to a version derived from the running\n    interpreter. The version can be specified using up to three dot-separated\n    integers (e.g. \"3\" for 3.0.0, \"3.7\" for 3.7.0, or \"3.7.3\"). A major-minor\n    version can also be given as a string without dots (e.g. \"37\" for 3.7.0).\n    \"\"\"\n    ),\n)\n\n\nimplementation: Callable[..., Option] = partial(\n    Option,\n    \"--implementation\",\n    dest=\"implementation\",\n    metavar=\"implementation\",\n    default=None,\n    help=(\n        \"Only use wheels compatible with Python \"\n        \"implementation <implementation>, e.g. 'pp', 'jy', 'cp', \"\n        \" or 'ip'. If not specified, then the current \"\n        \"interpreter implementation is used.  Use 'py' to force \"\n        \"implementation-agnostic wheels.\"\n    ),\n)\n\n\nabis: Callable[..., Option] = partial(\n    Option,\n    \"--abi\",\n    dest=\"abis\",\n    metavar=\"abi\",\n    action=\"append\",\n    default=None,\n    help=(\n        \"Only use wheels compatible with Python abi <abi>, e.g. 'pypy_41'. \"\n        \"If not specified, then the current interpreter abi tag is used. \"\n        \"Use this option multiple times to specify multiple abis supported \"\n        \"by the target interpreter. Generally you will need to specify \"\n        \"--implementation, --platform, and --python-version when using this \"\n        \"option.\"\n    ),\n)\n\n\ndef add_target_python_options(cmd_opts: OptionGroup) -> None:\n    cmd_opts.add_option(platforms())\n    cmd_opts.add_option(python_version())\n    cmd_opts.add_option(implementation())\n    cmd_opts.add_option(abis())\n\n\ndef make_target_python(options: Values) -> TargetPython:\n    target_python = TargetPython(\n        platforms=options.platforms,\n        py_version_info=options.python_version,\n        abis=options.abis,\n        implementation=options.implementation,\n    )\n\n    return target_python\n\n\ndef prefer_binary() -> Option:\n    return Option(\n        \"--prefer-binary\",\n        dest=\"prefer_binary\",\n        action=\"store_true\",\n        default=False,\n        help=(\n            \"Prefer binary packages over source packages, even if the \"\n            \"source packages are newer.\"\n        ),\n    )\n\n\ncache_dir: Callable[..., Option] = partial(\n    PipOption,\n    \"--cache-dir\",\n    dest=\"cache_dir\",\n    default=USER_CACHE_DIR,\n    metavar=\"dir\",\n    type=\"path\",\n    help=\"Store the cache data in <dir>.\",\n)\n\n\ndef _handle_no_cache_dir(\n    option: Option, opt: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"\n    Process a value provided for the --no-cache-dir option.\n\n    This is an optparse.Option callback for the --no-cache-dir option.\n    \"\"\"\n    # The value argument will be None if --no-cache-dir is passed via the\n    # command-line, since the option doesn't accept arguments.  However,\n    # the value can be non-None if the option is triggered e.g. by an\n    # environment variable, like PIP_NO_CACHE_DIR=true.\n    if value is not None:\n        # Then parse the string value to get argument error-checking.\n        try:\n            strtobool(value)\n        except ValueError as exc:\n            raise_option_error(parser, option=option, msg=str(exc))\n\n    # Originally, setting PIP_NO_CACHE_DIR to a value that strtobool()\n    # converted to 0 (like \"false\" or \"no\") caused cache_dir to be disabled\n    # rather than enabled (logic would say the latter).  Thus, we disable\n    # the cache directory not just on values that parse to True, but (for\n    # backwards compatibility reasons) also on values that parse to False.\n    # In other words, always set it to False if the option is provided in\n    # some (valid) form.\n    parser.values.cache_dir = False\n\n\nno_cache: Callable[..., Option] = partial(\n    Option,\n    \"--no-cache-dir\",\n    dest=\"cache_dir\",\n    action=\"callback\",\n    callback=_handle_no_cache_dir,\n    help=\"Disable the cache.\",\n)\n\nno_deps: Callable[..., Option] = partial(\n    Option,\n    \"--no-deps\",\n    \"--no-dependencies\",\n    dest=\"ignore_dependencies\",\n    action=\"store_true\",\n    default=False,\n    help=\"Don't install package dependencies.\",\n)\n\nignore_requires_python: Callable[..., Option] = partial(\n    Option,\n    \"--ignore-requires-python\",\n    dest=\"ignore_requires_python\",\n    action=\"store_true\",\n    help=\"Ignore the Requires-Python information.\",\n)\n\nno_build_isolation: Callable[..., Option] = partial(\n    Option,\n    \"--no-build-isolation\",\n    dest=\"build_isolation\",\n    action=\"store_false\",\n    default=True,\n    help=\"Disable isolation when building a modern source distribution. \"\n    \"Build dependencies specified by PEP 518 must be already installed \"\n    \"if this option is used.\",\n)\n\ncheck_build_deps: Callable[..., Option] = partial(\n    Option,\n    \"--check-build-dependencies\",\n    dest=\"check_build_deps\",\n    action=\"store_true\",\n    default=False,\n    help=\"Check the build dependencies when PEP517 is used.\",\n)\n\n\ndef _handle_no_use_pep517(\n    option: Option, opt: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"\n    Process a value provided for the --no-use-pep517 option.\n\n    This is an optparse.Option callback for the no_use_pep517 option.\n    \"\"\"\n    # Since --no-use-pep517 doesn't accept arguments, the value argument\n    # will be None if --no-use-pep517 is passed via the command-line.\n    # However, the value can be non-None if the option is triggered e.g.\n    # by an environment variable, for example \"PIP_NO_USE_PEP517=true\".\n    if value is not None:\n        msg = \"\"\"A value was passed for --no-use-pep517,\n        probably using either the PIP_NO_USE_PEP517 environment variable\n        or the \"no-use-pep517\" config file option. Use an appropriate value\n        of the PIP_USE_PEP517 environment variable or the \"use-pep517\"\n        config file option instead.\n        \"\"\"\n        raise_option_error(parser, option=option, msg=msg)\n\n    # If user doesn't wish to use pep517, we check if setuptools and wheel are installed\n    # and raise error if it is not.\n    packages = (\"setuptools\", \"wheel\")\n    if not all(importlib.util.find_spec(package) for package in packages):\n        msg = (\n            f\"It is not possible to use --no-use-pep517 \"\n            f\"without {' and '.join(packages)} installed.\"\n        )\n        raise_option_error(parser, option=option, msg=msg)\n\n    # Otherwise, --no-use-pep517 was passed via the command-line.\n    parser.values.use_pep517 = False\n\n\nuse_pep517: Any = partial(\n    Option,\n    \"--use-pep517\",\n    dest=\"use_pep517\",\n    action=\"store_true\",\n    default=None,\n    help=\"Use PEP 517 for building source distributions \"\n    \"(use --no-use-pep517 to force legacy behaviour).\",\n)\n\nno_use_pep517: Any = partial(\n    Option,\n    \"--no-use-pep517\",\n    dest=\"use_pep517\",\n    action=\"callback\",\n    callback=_handle_no_use_pep517,\n    default=None,\n    help=SUPPRESS_HELP,\n)\n\n\ndef _handle_config_settings(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    key, sep, val = value.partition(\"=\")\n    if sep != \"=\":\n        parser.error(f\"Arguments to {opt_str} must be of the form KEY=VAL\")\n    dest = getattr(parser.values, option.dest)\n    if dest is None:\n        dest = {}\n        setattr(parser.values, option.dest, dest)\n    if key in dest:\n        if isinstance(dest[key], list):\n            dest[key].append(val)\n        else:\n            dest[key] = [dest[key], val]\n    else:\n        dest[key] = val\n\n\nconfig_settings: Callable[..., Option] = partial(\n    Option,\n    \"-C\",\n    \"--config-settings\",\n    dest=\"config_settings\",\n    type=str,\n    action=\"callback\",\n    callback=_handle_config_settings,\n    metavar=\"settings\",\n    help=\"Configuration settings to be passed to the PEP 517 build backend. \"\n    \"Settings take the form KEY=VALUE. Use multiple --config-settings options \"\n    \"to pass multiple keys to the backend.\",\n)\n\nbuild_options: Callable[..., Option] = partial(\n    Option,\n    \"--build-option\",\n    dest=\"build_options\",\n    metavar=\"options\",\n    action=\"append\",\n    help=\"Extra arguments to be supplied to 'setup.py bdist_wheel'.\",\n)\n\nglobal_options: Callable[..., Option] = partial(\n    Option,\n    \"--global-option\",\n    dest=\"global_options\",\n    action=\"append\",\n    metavar=\"options\",\n    help=\"Extra global options to be supplied to the setup.py \"\n    \"call before the install or bdist_wheel command.\",\n)\n\nno_clean: Callable[..., Option] = partial(\n    Option,\n    \"--no-clean\",\n    action=\"store_true\",\n    default=False,\n    help=\"Don't clean up build directories.\",\n)\n\npre: Callable[..., Option] = partial(\n    Option,\n    \"--pre\",\n    action=\"store_true\",\n    default=False,\n    help=\"Include pre-release and development versions. By default, \"\n    \"pip only finds stable versions.\",\n)\n\ndisable_pip_version_check: Callable[..., Option] = partial(\n    Option,\n    \"--disable-pip-version-check\",\n    dest=\"disable_pip_version_check\",\n    action=\"store_true\",\n    default=True,\n    help=\"Don't periodically check PyPI to determine whether a new version \"\n    \"of pip is available for download. Implied with --no-index.\",\n)\n\nroot_user_action: Callable[..., Option] = partial(\n    Option,\n    \"--root-user-action\",\n    dest=\"root_user_action\",\n    default=\"warn\",\n    choices=[\"warn\", \"ignore\"],\n    help=\"Action if pip is run as a root user. By default, a warning message is shown.\",\n)\n\n\ndef _handle_merge_hash(\n    option: Option, opt_str: str, value: str, parser: OptionParser\n) -> None:\n    \"\"\"Given a value spelled \"algo:digest\", append the digest to a list\n    pointed to in a dict by the algo name.\"\"\"\n    if not parser.values.hashes:\n        parser.values.hashes = {}\n    try:\n        algo, digest = value.split(\":\", 1)\n    except ValueError:\n        parser.error(\n            f\"Arguments to {opt_str} must be a hash name \"\n            \"followed by a value, like --hash=sha256:\"\n            \"abcde...\"\n        )\n    if algo not in STRONG_HASHES:\n        parser.error(\n            \"Allowed hash algorithms for {} are {}.\".format(\n                opt_str, \", \".join(STRONG_HASHES)\n            )\n        )\n    parser.values.hashes.setdefault(algo, []).append(digest)\n\n\nhash: Callable[..., Option] = partial(\n    Option,\n    \"--hash\",\n    # Hash values eventually end up in InstallRequirement.hashes due to\n    # __dict__ copying in process_line().\n    dest=\"hashes\",\n    action=\"callback\",\n    callback=_handle_merge_hash,\n    type=\"string\",\n    help=\"Verify that the package's archive matches this \"\n    \"hash before installing. Example: --hash=sha256:abcdef...\",\n)\n\n\nrequire_hashes: Callable[..., Option] = partial(\n    Option,\n    \"--require-hashes\",\n    dest=\"require_hashes\",\n    action=\"store_true\",\n    default=False,\n    help=\"Require a hash to check each requirement against, for \"\n    \"repeatable installs. This option is implied when any package in a \"\n    \"requirements file has a --hash option.\",\n)\n\n\nlist_path: Callable[..., Option] = partial(\n    PipOption,\n    \"--path\",\n    dest=\"path\",\n    type=\"path\",\n    action=\"append\",\n    help=\"Restrict to the specified installation path for listing \"\n    \"packages (can be used multiple times).\",\n)\n\n\ndef check_list_path_option(options: Values) -> None:\n    if options.path and (options.user or options.local):\n        raise CommandError(\"Cannot combine '--path' with '--user' or '--local'\")\n\n\nlist_exclude: Callable[..., Option] = partial(\n    PipOption,\n    \"--exclude\",\n    dest=\"excludes\",\n    action=\"append\",\n    metavar=\"package\",\n    type=\"package_name\",\n    help=\"Exclude specified package from the output\",\n)\n\n\nno_python_version_warning: Callable[..., Option] = partial(\n    Option,\n    \"--no-python-version-warning\",\n    dest=\"no_python_version_warning\",\n    action=\"store_true\",\n    default=False,\n    help=\"Silence deprecation warnings for upcoming unsupported Pythons.\",\n)\n\n\n# Features that are now always on. A warning is printed if they are used.\nALWAYS_ENABLED_FEATURES = [\n    \"no-binary-enable-wheel-cache\",  # always on since 23.1\n]\n\nuse_new_feature: Callable[..., Option] = partial(\n    Option,\n    \"--use-feature\",\n    dest=\"features_enabled\",\n    metavar=\"feature\",\n    action=\"append\",\n    default=[],\n    choices=[\n        \"fast-deps\",\n        \"truststore\",\n    ]\n    + ALWAYS_ENABLED_FEATURES,\n    help=\"Enable new functionality, that may be backward incompatible.\",\n)\n\nuse_deprecated_feature: Callable[..., Option] = partial(\n    Option,\n    \"--use-deprecated\",\n    dest=\"deprecated_features_enabled\",\n    metavar=\"feature\",\n    action=\"append\",\n    default=[],\n    choices=[\n        \"legacy-resolver\",\n    ],\n    help=(\"Enable deprecated functionality, that will be removed in the future.\"),\n)\n\n\n##########\n# groups #\n##########\n\ngeneral_group: Dict[str, Any] = {\n    \"name\": \"General Options\",\n    \"options\": [\n        help_,\n        debug_mode,\n        isolated_mode,\n        require_virtualenv,\n        python,\n        verbose,\n        version,\n        quiet,\n        log,\n        no_input,\n        keyring_provider,\n        proxy,\n        retries,\n        timeout,\n        exists_action,\n        trusted_host,\n        cert,\n        client_cert,\n        cache_dir,\n        no_cache,\n        disable_pip_version_check,\n        no_color,\n        no_python_version_warning,\n        use_new_feature,\n        use_deprecated_feature,\n    ],\n}\n\nindex_group: Dict[str, Any] = {\n    \"name\": \"Package Index Options\",\n    \"options\": [\n        index_url,\n        extra_index_url,\n        no_index,\n        find_links,\n    ],\n}"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_internal/models/format_control.py",
      "line_number": 1,
      "details": "from typing import FrozenSet, Optional, Set\n\nfrom pip._vendor.packaging.utils import canonicalize_name\n\nfrom pip._internal.exceptions import CommandError\n\n\nclass FormatControl:\n    \"\"\"Helper for managing formats from which a package can be installed.\"\"\"\n\n    __slots__ = [\"no_binary\", \"only_binary\"]\n\n    def __init__(\n        self,\n        no_binary: Optional[Set[str]] = None,\n        only_binary: Optional[Set[str]] = None,\n    ) -> None:\n        if no_binary is None:\n            no_binary = set()\n        if only_binary is None:\n            only_binary = set()\n\n        self.no_binary = no_binary\n        self.only_binary = only_binary\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        if self.__slots__ != other.__slots__:\n            return False\n\n        return all(getattr(self, k) == getattr(other, k) for k in self.__slots__)\n\n    def __repr__(self) -> str:\n        return f\"{self.__class__.__name__}({self.no_binary}, {self.only_binary})\"\n\n    @staticmethod\n    def handle_mutual_excludes(value: str, target: Set[str], other: Set[str]) -> None:\n        if value.startswith(\"-\"):\n            raise CommandError(\n                \"--no-binary / --only-binary option requires 1 argument.\"\n            )\n        new = value.split(\",\")\n        while \":all:\" in new:\n            other.clear()\n            target.clear()\n            target.add(\":all:\")\n            del new[: new.index(\":all:\") + 1]\n            # Without a none, we want to discard everything as :all: covers it\n            if \":none:\" not in new:\n                return\n        for name in new:\n            if name == \":none:\":\n                target.clear()\n                continue\n            name = canonicalize_name(name)\n            other.discard(name)\n            target.add(name)\n\n    def get_allowed_formats(self, canonical_name: str) -> FrozenSet[str]:\n        result = {\"binary\", \"source\"}\n        if canonical_name in self.only_binary:\n            result.discard(\"source\")\n        elif canonical_name in self.no_binary:\n            result.discard(\"binary\")\n        elif \":all:\" in self.only_binary:\n            result.discard(\"source\")\n        elif \":all:\" in self.no_binary:\n            result.discard(\"binary\")\n        return frozenset(result)\n\n    def disallow_binaries(self) -> None:\n        self.handle_mutual_excludes(\n            \":all:\",\n            self.no_binary,\n            self.only_binary,\n        )"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/colorama/ansi.py",
      "line_number": 1,
      "details": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\n'''\nThis module generates ANSI character codes to printing colors to terminals.\nSee: http://en.wikipedia.org/wiki/ANSI_escape_code\n'''\n\nCSI = '\\033['\nOSC = '\\033]'\nBEL = '\\a'\n\n\ndef code_to_chars(code):\n    return CSI + str(code) + 'm'\n\ndef set_title(title):\n    return OSC + '2;' + title + BEL\n\ndef clear_screen(mode=2):\n    return CSI + str(mode) + 'J'\n\ndef clear_line(mode=2):\n    return CSI + str(mode) + 'K'\n\n\nclass AnsiCodes(object):\n    def __init__(self):\n        # the subclasses declare class attributes which are numbers.\n        # Upon instantiation we define instance attributes, which are the same\n        # as the class attributes but wrapped with the ANSI escape sequence\n        for name in dir(self):\n            if not name.startswith('_'):\n                value = getattr(self, name)\n                setattr(self, name, code_to_chars(value))\n\n\nclass AnsiCursor(object):\n    def UP(self, n=1):\n        return CSI + str(n) + 'A'\n    def DOWN(self, n=1):\n        return CSI + str(n) + 'B'\n    def FORWARD(self, n=1):\n        return CSI + str(n) + 'C'\n    def BACK(self, n=1):\n        return CSI + str(n) + 'D'\n    def POS(self, x=1, y=1):\n        return CSI + str(y) + ';' + str(x) + 'H'\n\n\nclass AnsiFore(AnsiCodes):\n    BLACK           = 30\n    RED             = 31\n    GREEN           = 32\n    YELLOW          = 33\n    BLUE            = 34\n    MAGENTA         = 35\n    CYAN            = 36\n    WHITE           = 37\n    RESET           = 39\n\n    # These are fairly well supported, but not part of the standard.\n    LIGHTBLACK_EX   = 90\n    LIGHTRED_EX     = 91\n    LIGHTGREEN_EX   = 92\n    LIGHTYELLOW_EX  = 93\n    LIGHTBLUE_EX    = 94\n    LIGHTMAGENTA_EX = 95\n    LIGHTCYAN_EX    = 96\n    LIGHTWHITE_EX   = 97\n\n\nclass AnsiBack(AnsiCodes):\n    BLACK           = 40\n    RED             = 41\n    GREEN           = 42\n    YELLOW          = 43\n    BLUE            = 44\n    MAGENTA         = 45\n    CYAN            = 46\n    WHITE           = 47\n    RESET           = 49\n\n    # These are fairly well supported, but not part of the standard.\n    LIGHTBLACK_EX   = 100\n    LIGHTRED_EX     = 101\n    LIGHTGREEN_EX   = 102\n    LIGHTYELLOW_EX  = 103\n    LIGHTBLUE_EX    = 104\n    LIGHTMAGENTA_EX = 105\n    LIGHTCYAN_EX    = 106\n    LIGHTWHITE_EX   = 107\n\n\nclass AnsiStyle(AnsiCodes):\n    BRIGHT    = 1\n    DIM       = 2\n    NORMAL    = 22\n    RESET_ALL = 0\n\nFore   = AnsiFore()\nBack   = AnsiBack()\nStyle  = AnsiStyle()\nCursor = AnsiCursor()"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/distlib/database.py",
      "line_number": 2,
      "details": "'.join(lines))\n        return shared_path\n\n    def get_distinfo_resource(self, path):\n        if path not in DIST_FILES:\n            raise DistlibException('invalid path for a dist-info file: '\n                                   '%r at %r' % (path, self.path))\n        finder = resources.finder_for_path(self.path)\n        if finder is None:\n            raise DistlibException('Unable to get a finder for %s' % self.path)\n        return finder.find(path)\n\n    def get_distinfo_file(self, path):\n        \"\"\"\n        Returns a path located under the ``.dist-info`` directory. Returns a\n        string representing the path.\n\n        :parameter path: a ``'/'``-separated path relative to the\n                         ``.dist-info`` directory or an absolute path;\n                         If *path* is an absolute path and doesn't start\n                         with the ``.dist-info`` directory path,\n                         a :class:`DistlibException` is raised\n        :type path: str\n        :rtype: str\n        \"\"\"\n        # Check if it is an absolute path  # XXX use relpath, add tests\n        if path.find(os.sep) >= 0:\n            # it's an absolute path?\n            distinfo_dirname, path = path.split(os.sep)[-2:]\n            if distinfo_dirname != self.path.split(os.sep)[-1]:\n                raise DistlibException(\n                    'dist-info file %r does not belong to the %r %s '\n                    'distribution' % (path, self.name, self.version))\n\n        # The file must be relative\n        if path not in DIST_FILES:\n            raise DistlibException('invalid path for a dist-info file: '\n                                   '%r at %r' % (path, self.path))\n\n        return os.path.join(self.path, path)\n\n    def list_distinfo_files(self):\n        \"\"\"\n        Iterates over the ``RECORD`` entries and returns paths for each line if\n        the path is pointing to a file located in the ``.dist-info`` directory\n        or one of its subdirectories.\n\n        :returns: iterator of paths\n        \"\"\"\n        base = os.path.dirname(self.path)\n        for path, checksum, size in self._get_records():\n            # XXX add separator or use real relpath algo\n            if not os.path.isabs(path):\n                path = os.path.join(base, path)\n            if path.startswith(self.path):\n                yield path\n\n    def __eq__(self, other):\n        return (isinstance(other, InstalledDistribution)\n                and self.path == other.path)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    __hash__ = object.__hash__\n\n\nclass EggInfoDistribution(BaseInstalledDistribution):\n    \"\"\"Created with the *path* of the ``.egg-info`` directory or file provided\n    to the constructor. It reads the metadata contained in the file itself, or\n    if the given path happens to be a directory, the metadata is read from the\n    file ``PKG-INFO`` under that directory.\"\"\"\n\n    requested = True  # as we have no way of knowing, assume it was\n    shared_locations = {}\n\n    def __init__(self, path, env=None):\n\n        def set_name_and_version(s, n, v):\n            s.name = n\n            s.key = n.lower()  # for case-insensitive comparisons\n            s.version = v\n\n        self.path = path\n        self.dist_path = env\n        if env and env._cache_enabled and path in env._cache_egg.path:\n            metadata = env._cache_egg.path[path].metadata\n            set_name_and_version(self, metadata.name, metadata.version)\n        else:\n            metadata = self._get_metadata(path)\n\n            # Need to be set before caching\n            set_name_and_version(self, metadata.name, metadata.version)\n\n            if env and env._cache_enabled:\n                env._cache_egg.add(self)\n        super(EggInfoDistribution, self).__init__(metadata, path, env)\n\n    def _get_metadata(self, path):\n        requires = None\n\n        def parse_requires_data(data):\n            \"\"\"Create a list of dependencies from a requires.txt file.\n\n            *data*: the contents of a setuptools-produced requires.txt file.\n            \"\"\"\n            reqs = []\n            lines = data.splitlines()\n            for line in lines:\n                line = line.strip()\n                # sectioned files have bare newlines (separating sections)\n                if not line:  # pragma: no cover\n                    continue\n                if line.startswith('['):  # pragma: no cover\n                    logger.warning(\n                        'Unexpected line: quitting requirement scan: %r', line)\n                    break\n                r = parse_requirement(line)\n                if not r:  # pragma: no cover\n                    logger.warning('Not recognised as a requirement: %r', line)\n                    continue\n                if r.extras:  # pragma: no cover\n                    logger.warning('extra requirements in requires.txt are '\n                                   'not supported')\n                if not r.constraints:\n                    reqs.append(r.name)\n                else:\n                    cons = ', '.join('%s%s' % c for c in r.constraints)\n                    reqs.append('%s (%s)' % (r.name, cons))\n            return reqs\n\n        def parse_requires_path(req_path):\n            \"\"\"Create a list of dependencies from a requires.txt file.\n\n            *req_path*: the path to a setuptools-produced requires.txt file.\n            \"\"\"\n\n            reqs = []\n            try:\n                with codecs.open(req_path, 'r', 'utf-8') as fp:\n                    reqs = parse_requires_data(fp.read())\n            except IOError:\n                pass\n            return reqs\n\n        tl_path = tl_data = None\n        if path.endswith('.egg'):\n            if os.path.isdir(path):\n                p = os.path.join(path, 'EGG-INFO')\n                meta_path = os.path.join(p, 'PKG-INFO')\n                metadata = Metadata(path=meta_path, scheme='legacy')\n                req_path = os.path.join(p, 'requires.txt')\n                tl_path = os.path.join(p, 'top_level.txt')\n                requires = parse_requires_path(req_path)\n            else:\n                # FIXME handle the case where zipfile is not available\n                zipf = zipimport.zipimporter(path)\n                fileobj = StringIO(\n                    zipf.get_data('EGG-INFO/PKG-INFO').decode('utf8'))\n                metadata = Metadata(fileobj=fileobj, scheme='legacy')\n                try:\n                    data = zipf.get_data('EGG-INFO/requires.txt')\n                    tl_data = zipf.get_data('EGG-INFO/top_level.txt').decode(\n                        'utf-8')\n                    requires = parse_requires_data(data.decode('utf-8'))\n                except IOError:\n                    requires = None\n        elif path.endswith('.egg-info'):\n            if os.path.isdir(path):\n                req_path = os.path.join(path, 'requires.txt')\n                requires = parse_requires_path(req_path)\n                path = os.path.join(path, 'PKG-INFO')\n                tl_path = os.path.join(path, 'top_level.txt')\n            metadata = Metadata(path=path, scheme='legacy')\n        else:\n            raise DistlibException('path must end with .egg-info or .egg, '\n                                   'got %r' % path)\n\n        if requires:\n            metadata.add_requirements(requires)\n        # look for top-level modules in top_level.txt, if present\n        if tl_data is None:\n            if tl_path is not None and os.path.exists(tl_path):\n                with open(tl_path, 'rb') as f:\n                    tl_data = f.read().decode('utf-8')\n        if not tl_data:\n            tl_data = []\n        else:\n            tl_data = tl_data.splitlines()\n        self.modules = tl_data\n        return metadata\n\n    def __repr__(self):\n        return '<EggInfoDistribution %r %s at %r>' % (self.name, self.version,\n                                                      self.path)\n\n    def __str__(self):\n        return \"%s %s\" % (self.name, self.version)\n\n    def check_installed_files(self):\n        \"\"\"\n        Checks that the hashes and sizes of the files in ``RECORD`` are\n        matched by the files themselves. Returns a (possibly empty) list of\n        mismatches. Each entry in the mismatch list will be a tuple consisting\n        of the path, 'exists', 'size' or 'hash' according to what didn't match\n        (existence is checked first, then size, then hash), the expected\n        value and the actual value.\n        \"\"\"\n        mismatches = []\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        if os.path.exists(record_path):\n            for path, _, _ in self.list_installed_files():\n                if path == record_path:\n                    continue\n                if not os.path.exists(path):\n                    mismatches.append((path, 'exists', True, False))\n        return mismatches\n\n    def list_installed_files(self):\n        \"\"\"\n        Iterates over the ``installed-files.txt`` entries and returns a tuple\n        ``(path, hash, size)`` for each line.\n\n        :returns: a list of (path, hash, size)\n        \"\"\"\n\n        def _md5(path):\n            f = open(path, 'rb')\n            try:\n                content = f.read()\n            finally:\n                f.close()\n            return hashlib.md5(content).hexdigest()\n\n        def _size(path):\n            return os.stat(path).st_size\n\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        result = []\n        if os.path.exists(record_path):\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    p = os.path.normpath(os.path.join(self.path, line))\n                    # \"./\" is present as a marker between installed files\n                    # and installation metadata files\n                    if not os.path.exists(p):\n                        logger.warning('Non-existent file: %s', p)\n                        if p.endswith(('.pyc', '.pyo')):\n                            continue\n                        # otherwise fall through and fail\n                    if not os.path.isdir(p):\n                        result.append((p, _md5(p), _size(p)))\n            result.append((record_path, None, None))\n        return result\n\n    def list_distinfo_files(self, absolute=False):\n        \"\"\"\n        Iterates over the ``installed-files.txt`` entries and returns paths for\n        each line if the path is pointing to a file located in the\n        ``.egg-info`` directory or one of its subdirectories.\n\n        :parameter absolute: If *absolute* is ``True``, each returned path is\n                          transformed into a local absolute path. Otherwise the\n                          raw value from ``installed-files.txt`` is returned.\n        :type absolute: boolean\n        :returns: iterator of paths\n        \"\"\"\n        record_path = os.path.join(self.path, 'installed-files.txt')\n        if os.path.exists(record_path):\n            skip = True\n            with codecs.open(record_path, 'r', encoding='utf-8') as f:\n                for line in f:\n                    line = line.strip()\n                    if line == './':\n                        skip = False\n                        continue\n                    if not skip:\n                        p = os.path.normpath(os.path.join(self.path, line))\n                        if p.startswith(self.path):\n                            if absolute:\n                                yield p\n                            else:\n                                yield line\n\n    def __eq__(self, other):\n        return (isinstance(other, EggInfoDistribution)\n                and self.path == other.path)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    __hash__ = object.__hash__\n\n\nnew_dist_class = InstalledDistribution\nold_dist_class = EggInfoDistribution\n\n\nclass DependencyGraph(object):\n    \"\"\"\n    Represents a dependency graph between distributions.\n\n    The dependency relationships are stored in an ``adjacency_list`` that maps\n    distributions to a list of ``(other, label)`` tuples where  ``other``\n    is a distribution and the edge is labeled with ``label`` (i.e. the version\n    specifier, if such was provided). Also, for more efficient traversal, for\n    every distribution ``x``, a list of predecessors is kept in\n    ``reverse_list[x]``. An edge from distribution ``a`` to\n    distribution ``b`` means that ``a`` depends on ``b``. If any missing\n    dependencies are found, they are stored in ``missing``, which is a\n    dictionary that maps distributions to a list of requirements that were not\n    provided by any other distributions.\n    \"\"\"\n\n    def __init__(self):\n        self.adjacency_list = {}\n        self.reverse_list = {}\n        self.missing = {}\n\n    def add_distribution(self, distribution):\n        \"\"\"Add the *distribution* to the graph.\n\n        :type distribution: :class:`distutils2.database.InstalledDistribution`\n                            or :class:`distutils2.database.EggInfoDistribution`\n        \"\"\"\n        self.adjacency_list[distribution] = []\n        self.reverse_list[distribution] = []\n        # self.missing[distribution] = []\n\n    def add_edge(self, x, y, label=None):\n        \"\"\"Add an edge from distribution *x* to distribution *y* with the given\n        *label*.\n\n        :type x: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type y: :class:`distutils2.database.InstalledDistribution` or\n                 :class:`distutils2.database.EggInfoDistribution`\n        :type label: ``str`` or ``None``\n        \"\"\"\n        self.adjacency_list[x].append((y, label))\n        # multiple edges are allowed, so be careful\n        if x not in self.reverse_list[y]:\n            self.reverse_list[y].append(x)\n\n    def add_missing(self, distribution, requirement):\n        \"\"\"\n        Add a missing *requirement* for the given *distribution*.\n\n        :type distribution: :class:`distutils2.database.InstalledDistribution`\n                            or :class:`distutils2.database.EggInfoDistribution`\n        :type requirement: ``str``\n        \"\"\"\n        logger.debug('%s missing %r', distribution, requirement)\n        self.missing.setdefault(distribution, []).append(requirement)\n\n    def _repr_dist(self, dist):\n        return '%s %s' % (dist.name, dist.version)\n\n    def repr_node(self, dist, level=1):\n        \"\"\"Prints only a subgraph\"\"\"\n        output = [self._repr_dist(dist)]\n        for other, label in self.adjacency_list[dist]:\n            dist = self._repr_dist(other)\n            if label is not None:\n                dist = '%s [%s]' % (dist, label)\n            output.append('    ' * level + str(dist))\n            suboutput = self.repr_node(other, level + 1)\n            subs = suboutput.split('"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/distlib/database.py",
      "line_number": 1,
      "details": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"PEP 376 implementation.\"\"\"\n\nfrom __future__ import unicode_literals\n\nimport base64\nimport codecs\nimport contextlib\nimport hashlib\nimport logging\nimport os\nimport posixpath\nimport sys\nimport zipimport\n\nfrom . import DistlibException, resources\nfrom .compat import StringIO\nfrom .version import get_scheme, UnsupportedVersionError\nfrom .metadata import (Metadata, METADATA_FILENAME, WHEEL_METADATA_FILENAME,\n                       LEGACY_METADATA_FILENAME)\nfrom .util import (parse_requirement, cached_property, parse_name_and_version,\n                   read_exports, write_exports, CSVReader, CSVWriter)\n\n__all__ = [\n    'Distribution', 'BaseInstalledDistribution', 'InstalledDistribution',\n    'EggInfoDistribution', 'DistributionPath'\n]\n\nlogger = logging.getLogger(__name__)\n\nEXPORTS_FILENAME = 'pydist-exports.json'\nCOMMANDS_FILENAME = 'pydist-commands.json'\n\nDIST_FILES = ('INSTALLER', METADATA_FILENAME, 'RECORD', 'REQUESTED',\n              'RESOURCES', EXPORTS_FILENAME, 'SHARED')\n\nDISTINFO_EXT = '.dist-info'\n\n\nclass _Cache(object):\n    \"\"\"\n    A simple cache mapping names and .dist-info paths to distributions\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Initialise an instance. There is normally one for each DistributionPath.\n        \"\"\"\n        self.name = {}\n        self.path = {}\n        self.generated = False\n\n    def clear(self):\n        \"\"\"\n        Clear the cache, setting it to its initial state.\n        \"\"\"\n        self.name.clear()\n        self.path.clear()\n        self.generated = False\n\n    def add(self, dist):\n        \"\"\"\n        Add a distribution to the cache.\n        :param dist: The distribution to add.\n        \"\"\"\n        if dist.path not in self.path:\n            self.path[dist.path] = dist\n            self.name.setdefault(dist.key, []).append(dist)\n\n\nclass DistributionPath(object):\n    \"\"\"\n    Represents a set of distributions installed on a path (typically sys.path).\n    \"\"\"\n\n    def __init__(self, path=None, include_egg=False):\n        \"\"\"\n        Create an instance from a path, optionally including legacy (distutils/\n        setuptools/distribute) distributions.\n        :param path: The path to use, as a list of directories. If not specified,\n                     sys.path is used.\n        :param include_egg: If True, this instance will look for and return legacy\n                            distributions as well as those based on PEP 376.\n        \"\"\"\n        if path is None:\n            path = sys.path\n        self.path = path\n        self._include_dist = True\n        self._include_egg = include_egg\n\n        self._cache = _Cache()\n        self._cache_egg = _Cache()\n        self._cache_enabled = True\n        self._scheme = get_scheme('default')\n\n    def _get_cache_enabled(self):\n        return self._cache_enabled\n\n    def _set_cache_enabled(self, value):\n        self._cache_enabled = value\n\n    cache_enabled = property(_get_cache_enabled, _set_cache_enabled)\n\n    def clear_cache(self):\n        \"\"\"\n        Clears the internal cache.\n        \"\"\"\n        self._cache.clear()\n        self._cache_egg.clear()\n\n    def _yield_distributions(self):\n        \"\"\"\n        Yield .dist-info and/or .egg(-info) distributions.\n        \"\"\"\n        # We need to check if we've seen some resources already, because on\n        # some Linux systems (e.g. some Debian/Ubuntu variants) there are\n        # symlinks which alias other files in the environment.\n        seen = set()\n        for path in self.path:\n            finder = resources.finder_for_path(path)\n            if finder is None:\n                continue\n            r = finder.find('')\n            if not r or not r.is_container:\n                continue\n            rset = sorted(r.resources)\n            for entry in rset:\n                r = finder.find(entry)\n                if not r or r.path in seen:\n                    continue\n                try:\n                    if self._include_dist and entry.endswith(DISTINFO_EXT):\n                        possible_filenames = [\n                            METADATA_FILENAME, WHEEL_METADATA_FILENAME,\n                            LEGACY_METADATA_FILENAME\n                        ]\n                        for metadata_filename in possible_filenames:\n                            metadata_path = posixpath.join(\n                                entry, metadata_filename)\n                            pydist = finder.find(metadata_path)\n                            if pydist:\n                                break\n                        else:\n                            continue\n\n                        with contextlib.closing(pydist.as_stream()) as stream:\n                            metadata = Metadata(fileobj=stream,\n                                                scheme='legacy')\n                        logger.debug('Found %s', r.path)\n                        seen.add(r.path)\n                        yield new_dist_class(r.path,\n                                             metadata=metadata,\n                                             env=self)\n                    elif self._include_egg and entry.endswith(\n                            ('.egg-info', '.egg')):\n                        logger.debug('Found %s', r.path)\n                        seen.add(r.path)\n                        yield old_dist_class(r.path, self)\n                except Exception as e:\n                    msg = 'Unable to read distribution at %s, perhaps due to bad metadata: %s'\n                    logger.warning(msg, r.path, e)\n                    import warnings\n                    warnings.warn(msg % (r.path, e), stacklevel=2)\n\n    def _generate_cache(self):\n        \"\"\"\n        Scan the path for distributions and populate the cache with\n        those that are found.\n        \"\"\"\n        gen_dist = not self._cache.generated\n        gen_egg = self._include_egg and not self._cache_egg.generated\n        if gen_dist or gen_egg:\n            for dist in self._yield_distributions():\n                if isinstance(dist, InstalledDistribution):\n                    self._cache.add(dist)\n                else:\n                    self._cache_egg.add(dist)\n\n            if gen_dist:\n                self._cache.generated = True\n            if gen_egg:\n                self._cache_egg.generated = True\n\n    @classmethod\n    def distinfo_dirname(cls, name, version):\n        \"\"\"\n        The *name* and *version* parameters are converted into their\n        filename-escaped form, i.e. any ``'-'`` characters are replaced\n        with ``'_'`` other than the one in ``'dist-info'`` and the one\n        separating the name from the version number.\n\n        :parameter name: is converted to a standard distribution name by replacing\n                         any runs of non- alphanumeric characters with a single\n                         ``'-'``.\n        :type name: string\n        :parameter version: is converted to a standard version string. Spaces\n                            become dots, and all other non-alphanumeric characters\n                            (except dots) become dashes, with runs of multiple\n                            dashes condensed to a single dash.\n        :type version: string\n        :returns: directory name\n        :rtype: string\"\"\"\n        name = name.replace('-', '_')\n        return '-'.join([name, version]) + DISTINFO_EXT\n\n    def get_distributions(self):\n        \"\"\"\n        Provides an iterator that looks for distributions and returns\n        :class:`InstalledDistribution` or\n        :class:`EggInfoDistribution` instances for each one of them.\n\n        :rtype: iterator of :class:`InstalledDistribution` and\n                :class:`EggInfoDistribution` instances\n        \"\"\"\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                yield dist\n        else:\n            self._generate_cache()\n\n            for dist in self._cache.path.values():\n                yield dist\n\n            if self._include_egg:\n                for dist in self._cache_egg.path.values():\n                    yield dist\n\n    def get_distribution(self, name):\n        \"\"\"\n        Looks for a named distribution on the path.\n\n        This function only returns the first result found, as no more than one\n        value is expected. If nothing is found, ``None`` is returned.\n\n        :rtype: :class:`InstalledDistribution`, :class:`EggInfoDistribution`\n                or ``None``\n        \"\"\"\n        result = None\n        name = name.lower()\n        if not self._cache_enabled:\n            for dist in self._yield_distributions():\n                if dist.key == name:\n                    result = dist\n                    break\n        else:\n            self._generate_cache()\n\n            if name in self._cache.name:\n                result = self._cache.name[name][0]\n            elif self._include_egg and name in self._cache_egg.name:\n                result = self._cache_egg.name[name][0]\n        return result\n\n    def provides_distribution(self, name, version=None):\n        \"\"\"\n        Iterates over all distributions to find which distributions provide *name*.\n        If a *version* is provided, it will be used to filter the results.\n\n        This function only returns the first result found, since no more than\n        one values are expected. If the directory is not found, returns ``None``.\n\n        :parameter version: a version specifier that indicates the version\n                            required, conforming to the format in ``PEP-345``\n\n        :type name: string\n        :type version: string\n        \"\"\"\n        matcher = None\n        if version is not None:\n            try:\n                matcher = self._scheme.matcher('%s (%s)' % (name, version))\n            except ValueError:\n                raise DistlibException('invalid name or version: %r, %r' %\n                                       (name, version))\n\n        for dist in self.get_distributions():\n            # We hit a problem on Travis where enum34 was installed and doesn't\n            # have a provides attribute ...\n            if not hasattr(dist, 'provides'):\n                logger.debug('No \"provides\": %s', dist)\n            else:\n                provided = dist.provides\n\n                for p in provided:\n                    p_name, p_ver = parse_name_and_version(p)\n                    if matcher is None:\n                        if p_name == name:\n                            yield dist\n                            break\n                    else:\n                        if p_name == name and matcher.match(p_ver):\n                            yield dist\n                            break\n\n    def get_file_path(self, name, relative_path):\n        \"\"\"\n        Return the path to a resource file.\n        \"\"\"\n        dist = self.get_distribution(name)\n        if dist is None:\n            raise LookupError('no distribution named %r found' % name)\n        return dist.get_resource_path(relative_path)\n\n    def get_exported_entries(self, category, name=None):\n        \"\"\"\n        Return all of the exported entries in a particular category.\n\n        :param category: The category to search for entries.\n        :param name: If specified, only entries with that name are returned.\n        \"\"\"\n        for dist in self.get_distributions():\n            r = dist.exports\n            if category in r:\n                d = r[category]\n                if name is not None:\n                    if name in d:\n                        yield d[name]\n                else:\n                    for v in d.values():\n                        yield v\n\n\nclass Distribution(object):\n    \"\"\"\n    A base class for distributions, whether installed or from indexes.\n    Either way, it must have some metadata, so that's all that's needed\n    for construction.\n    \"\"\"\n\n    build_time_dependency = False\n    \"\"\"\n    Set to True if it's known to be only a build-time dependency (i.e.\n    not needed after installation).\n    \"\"\"\n\n    requested = False\n    \"\"\"A boolean that indicates whether the ``REQUESTED`` metadata file is\n    present (in other words, whether the package was installed by user\n    request or it was installed as a dependency).\"\"\"\n\n    def __init__(self, metadata):\n        \"\"\"\n        Initialise an instance.\n        :param metadata: The instance of :class:`Metadata` describing this\n        distribution.\n        \"\"\"\n        self.metadata = metadata\n        self.name = metadata.name\n        self.key = self.name.lower()  # for case-insensitive comparisons\n        self.version = metadata.version\n        self.locator = None\n        self.digest = None\n        self.extras = None  # additional features requested\n        self.context = None  # environment marker overrides\n        self.download_urls = set()\n        self.digests = {}\n\n    @property\n    def source_url(self):\n        \"\"\"\n        The source archive download URL for this distribution.\n        \"\"\"\n        return self.metadata.source_url\n\n    download_url = source_url  # Backward compatibility\n\n    @property\n    def name_and_version(self):\n        \"\"\"\n        A utility property which displays the name and version in parentheses.\n        \"\"\"\n        return '%s (%s)' % (self.name, self.version)\n\n    @property\n    def provides(self):\n        \"\"\"\n        A set of distribution names and versions provided by this distribution.\n        :return: A set of \"name (version)\" strings.\n        \"\"\"\n        plist = self.metadata.provides\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in plist:\n            plist.append(s)\n        return plist\n\n    def _get_requirements(self, req_attr):\n        md = self.metadata\n        reqts = getattr(md, req_attr)\n        logger.debug('%s: got requirements %r from metadata: %r', self.name,\n                     req_attr, reqts)\n        return set(\n            md.get_requirements(reqts, extras=self.extras, env=self.context))\n\n    @property\n    def run_requires(self):\n        return self._get_requirements('run_requires')\n\n    @property\n    def meta_requires(self):\n        return self._get_requirements('meta_requires')\n\n    @property\n    def build_requires(self):\n        return self._get_requirements('build_requires')\n\n    @property\n    def test_requires(self):\n        return self._get_requirements('test_requires')\n\n    @property\n    def dev_requires(self):\n        return self._get_requirements('dev_requires')\n\n    def matches_requirement(self, req):\n        \"\"\"\n        Say if this instance matches (fulfills) a requirement.\n        :param req: The requirement to match.\n        :rtype req: str\n        :return: True if it matches, else False.\n        \"\"\"\n        # Requirement may contain extras - parse to lose those\n        # from what's passed to the matcher\n        r = parse_requirement(req)\n        scheme = get_scheme(self.metadata.scheme)\n        try:\n            matcher = scheme.matcher(r.requirement)\n        except UnsupportedVersionError:\n            # XXX compat-mode if cannot read the version\n            logger.warning('could not read version %r - using name only', req)\n            name = req.split()[0]\n            matcher = scheme.matcher(name)\n\n        name = matcher.key  # case-insensitive\n\n        result = False\n        for p in self.provides:\n            p_name, p_ver = parse_name_and_version(p)\n            if p_name != name:\n                continue\n            try:\n                result = matcher.match(p_ver)\n                break\n            except UnsupportedVersionError:\n                pass\n        return result\n\n    def __repr__(self):\n        \"\"\"\n        Return a textual representation of this instance,\n        \"\"\"\n        if self.source_url:\n            suffix = ' [%s]' % self.source_url\n        else:\n            suffix = ''\n        return '<Distribution %s (%s)%s>' % (self.name, self.version, suffix)\n\n    def __eq__(self, other):\n        \"\"\"\n        See if this distribution is the same as another.\n        :param other: The distribution to compare with. To be equal to one\n                      another. distributions must have the same type, name,\n                      version and source_url.\n        :return: True if it is the same, else False.\n        \"\"\"\n        if type(other) is not type(self):\n            result = False\n        else:\n            result = (self.name == other.name and self.version == other.version\n                      and self.source_url == other.source_url)\n        return result\n\n    def __hash__(self):\n        \"\"\"\n        Compute hash in a way which matches the equality test.\n        \"\"\"\n        return hash(self.name) + hash(self.version) + hash(self.source_url)\n\n\nclass BaseInstalledDistribution(Distribution):\n    \"\"\"\n    This is the base class for installed distributions (whether PEP 376 or\n    legacy).\n    \"\"\"\n\n    hasher = None\n\n    def __init__(self, metadata, path, env=None):\n        \"\"\"\n        Initialise an instance.\n        :param metadata: An instance of :class:`Metadata` which describes the\n                         distribution. This will normally have been initialised\n                         from a metadata file in the ``path``.\n        :param path:     The path of the ``.dist-info`` or ``.egg-info``\n                         directory for the distribution.\n        :param env:      This is normally the :class:`DistributionPath`\n                         instance where this distribution was found.\n        \"\"\"\n        super(BaseInstalledDistribution, self).__init__(metadata)\n        self.path = path\n        self.dist_path = env\n\n    def get_hash(self, data, hasher=None):\n        \"\"\"\n        Get the hash of some data, using a particular hash algorithm, if\n        specified.\n\n        :param data: The data to be hashed.\n        :type data: bytes\n        :param hasher: The name of a hash implementation, supported by hashlib,\n                       or ``None``. Examples of valid values are ``'sha1'``,\n                       ``'sha224'``, ``'sha384'``, '``sha256'``, ``'md5'`` and\n                       ``'sha512'``. If no hasher is specified, the ``hasher``\n                       attribute of the :class:`InstalledDistribution` instance\n                       is used. If the hasher is determined to be ``None``, MD5\n                       is used as the hashing algorithm.\n        :returns: The hash of the data. If a hasher was explicitly specified,\n                  the returned hash will be prefixed with the specified hasher\n                  followed by '='.\n        :rtype: str\n        \"\"\"\n        if hasher is None:\n            hasher = self.hasher\n        if hasher is None:\n            hasher = hashlib.md5\n            prefix = ''\n        else:\n            hasher = getattr(hashlib, hasher)\n            prefix = '%s=' % self.hasher\n        digest = hasher(data).digest()\n        digest = base64.urlsafe_b64encode(digest).rstrip(b'=').decode('ascii')\n        return '%s%s' % (prefix, digest)\n\n\nclass InstalledDistribution(BaseInstalledDistribution):\n    \"\"\"\n    Created with the *path* of the ``.dist-info`` directory provided to the\n    constructor. It reads the metadata contained in ``pydist.json`` when it is\n    instantiated., or uses a passed in Metadata instance (useful for when\n    dry-run mode is being used).\n    \"\"\"\n\n    hasher = 'sha256'\n\n    def __init__(self, path, metadata=None, env=None):\n        self.modules = []\n        self.finder = finder = resources.finder_for_path(path)\n        if finder is None:\n            raise ValueError('finder unavailable for %s' % path)\n        if env and env._cache_enabled and path in env._cache.path:\n            metadata = env._cache.path[path].metadata\n        elif metadata is None:\n            r = finder.find(METADATA_FILENAME)\n            # Temporary - for Wheel 0.23 support\n            if r is None:\n                r = finder.find(WHEEL_METADATA_FILENAME)\n            # Temporary - for legacy support\n            if r is None:\n                r = finder.find(LEGACY_METADATA_FILENAME)\n            if r is None:\n                raise ValueError('no %s found in %s' %\n                                 (METADATA_FILENAME, path))\n            with contextlib.closing(r.as_stream()) as stream:\n                metadata = Metadata(fileobj=stream, scheme='legacy')\n\n        super(InstalledDistribution, self).__init__(metadata, path, env)\n\n        if env and env._cache_enabled:\n            env._cache.add(self)\n\n        r = finder.find('REQUESTED')\n        self.requested = r is not None\n        p = os.path.join(path, 'top_level.txt')\n        if os.path.exists(p):\n            with open(p, 'rb') as f:\n                data = f.read().decode('utf-8')\n            self.modules = data.splitlines()\n\n    def __repr__(self):\n        return '<InstalledDistribution %r %s at %r>' % (\n            self.name, self.version, self.path)\n\n    def __str__(self):\n        return \"%s %s\" % (self.name, self.version)\n\n    def _get_records(self):\n        \"\"\"\n        Get the list of installed files for the distribution\n        :return: A list of tuples of path, hash and size. Note that hash and\n                 size might be ``None`` for some entries. The path is exactly\n                 as stored in the file (which is as in PEP 376).\n        \"\"\"\n        results = []\n        r = self.get_distinfo_resource('RECORD')\n        with contextlib.closing(r.as_stream()) as stream:\n            with CSVReader(stream=stream) as record_reader:\n                # Base location is parent dir of .dist-info dir\n                # base_location = os.path.dirname(self.path)\n                # base_location = os.path.abspath(base_location)\n                for row in record_reader:\n                    missing = [None for i in range(len(row), 3)]\n                    path, checksum, size = row + missing\n                    # if not os.path.isabs(path):\n                    #     path = path.replace('/', os.sep)\n                    #     path = os.path.join(base_location, path)\n                    results.append((path, checksum, size))\n        return results\n\n    @cached_property\n    def exports(self):\n        \"\"\"\n        Return the information exported by this distribution.\n        :return: A dictionary of exports, mapping an export category to a dict\n                 of :class:`ExportEntry` instances describing the individual\n                 export entries, and keyed by name.\n        \"\"\"\n        result = {}\n        r = self.get_distinfo_resource(EXPORTS_FILENAME)\n        if r:\n            result = self.read_exports()\n        return result\n\n    def read_exports(self):\n        \"\"\"\n        Read exports data from a file in .ini format.\n\n        :return: A dictionary of exports, mapping an export category to a list\n                 of :class:`ExportEntry` instances describing the individual\n                 export entries.\n        \"\"\"\n        result = {}\n        r = self.get_distinfo_resource(EXPORTS_FILENAME)\n        if r:\n            with contextlib.closing(r.as_stream()) as stream:\n                result = read_exports(stream)\n        return result\n\n    def write_exports(self, exports):\n        \"\"\"\n        Write a dictionary of exports to a file in .ini format.\n        :param exports: A dictionary of exports, mapping an export category to\n                        a list of :class:`ExportEntry` instances describing the\n                        individual export entries.\n        \"\"\"\n        rf = self.get_distinfo_file(EXPORTS_FILENAME)\n        with open(rf, 'w') as f:\n            write_exports(exports, f)\n\n    def get_resource_path(self, relative_path):\n        \"\"\"\n        NOTE: This API may change in the future.\n\n        Return the absolute path to a resource file with the given relative\n        path.\n\n        :param relative_path: The path, relative to .dist-info, of the resource\n                              of interest.\n        :return: The absolute path where the resource is to be found.\n        \"\"\"\n        r = self.get_distinfo_resource('RESOURCES')\n        with contextlib.closing(r.as_stream()) as stream:\n            with CSVReader(stream=stream) as resources_reader:\n                for relative, destination in resources_reader:\n                    if relative == relative_path:\n                        return destination\n        raise KeyError('no resource file with relative path %r '\n                       'is installed' % relative_path)\n\n    def list_installed_files(self):\n        \"\"\"\n        Iterates over the ``RECORD`` entries and returns a tuple\n        ``(path, hash, size)`` for each line.\n\n        :returns: iterator of (path, hash, size)\n        \"\"\"\n        for result in self._get_records():\n            yield result\n\n    def write_installed_files(self, paths, prefix, dry_run=False):\n        \"\"\"\n        Writes the ``RECORD`` file, using the ``paths`` iterable passed in. Any\n        existing ``RECORD`` file is silently overwritten.\n\n        prefix is used to determine when to write absolute paths.\n        \"\"\"\n        prefix = os.path.join(prefix, '')\n        base = os.path.dirname(self.path)\n        base_under_prefix = base.startswith(prefix)\n        base = os.path.join(base, '')\n        record_path = self.get_distinfo_file('RECORD')\n        logger.info('creating %s', record_path)\n        if dry_run:\n            return None\n        with CSVWriter(record_path) as writer:\n            for path in paths:\n                if os.path.isdir(path) or path.endswith(('.pyc', '.pyo')):\n                    # do not put size and hash, as in PEP-376\n                    hash_value = size = ''\n                else:\n                    size = '%d' % os.path.getsize(path)\n                    with open(path, 'rb') as fp:\n                        hash_value = self.get_hash(fp.read())\n                if path.startswith(base) or (base_under_prefix\n                                             and path.startswith(prefix)):\n                    path = os.path.relpath(path, base)\n                writer.writerow((path, hash_value, size))\n\n            # add the RECORD file itself\n            if record_path.startswith(base):\n                record_path = os.path.relpath(record_path, base)\n            writer.writerow((record_path, '', ''))\n        return record_path\n\n    def check_installed_files(self):\n        \"\"\"\n        Checks that the hashes and sizes of the files in ``RECORD`` are\n        matched by the files themselves. Returns a (possibly empty) list of\n        mismatches. Each entry in the mismatch list will be a tuple consisting\n        of the path, 'exists', 'size' or 'hash' according to what didn't match\n        (existence is checked first, then size, then hash), the expected\n        value and the actual value.\n        \"\"\"\n        mismatches = []\n        base = os.path.dirname(self.path)\n        record_path = self.get_distinfo_file('RECORD')\n        for path, hash_value, size in self.list_installed_files():\n            if not os.path.isabs(path):\n                path = os.path.join(base, path)\n            if path == record_path:\n                continue\n            if not os.path.exists(path):\n                mismatches.append((path, 'exists', True, False))\n            elif os.path.isfile(path):\n                actual_size = str(os.path.getsize(path))\n                if size and actual_size != size:\n                    mismatches.append((path, 'size', size, actual_size))\n                elif hash_value:\n                    if '=' in hash_value:\n                        hasher = hash_value.split('=', 1)[0]\n                    else:\n                        hasher = None\n\n                    with open(path, 'rb') as f:\n                        actual_hash = self.get_hash(f.read(), hasher)\n                        if actual_hash != hash_value:\n                            mismatches.append(\n                                (path, 'hash', hash_value, actual_hash))\n        return mismatches\n\n    @cached_property\n    def shared_locations(self):\n        \"\"\"\n        A dictionary of shared locations whose keys are in the set 'prefix',\n        'purelib', 'platlib', 'scripts', 'headers', 'data' and 'namespace'.\n        The corresponding value is the absolute path of that category for\n        this distribution, and takes into account any paths selected by the\n        user at installation time (e.g. via command-line arguments). In the\n        case of the 'namespace' key, this would be a list of absolute paths\n        for the roots of namespace packages in this distribution.\n\n        The first time this property is accessed, the relevant information is\n        read from the SHARED file in the .dist-info directory.\n        \"\"\"\n        result = {}\n        shared_path = os.path.join(self.path, 'SHARED')\n        if os.path.isfile(shared_path):\n            with codecs.open(shared_path, 'r', encoding='utf-8') as f:\n                lines = f.read().splitlines()\n            for line in lines:\n                key, value = line.split('=', 1)\n                if key == 'namespace':\n                    result.setdefault(key, []).append(value)\n                else:\n                    result[key] = value\n        return result\n\n    def write_shared_locations(self, paths, dry_run=False):\n        \"\"\"\n        Write shared location information to the SHARED file in .dist-info.\n        :param paths: A dictionary as described in the documentation for\n        :meth:`shared_locations`.\n        :param dry_run: If True, the action is logged but no file is actually\n                        written.\n        :return: The path of the file written to.\n        \"\"\"\n        shared_path = os.path.join(self.path, 'SHARED')\n        logger.info('creating %s', shared_path)\n        if dry_run:\n            return None\n        lines = []\n        for key in ('prefix', 'lib', 'headers', 'scripts', 'data'):\n            path = paths[key]\n            if os.path.isdir(paths[key]):\n                lines.append('%s=%s' % (key, path))\n        for ns in paths.get('namespace', ()):\n            lines.append('namespace=%s' % ns)\n\n        with codecs.open(shared_path, 'w', encoding='utf-8') as f:\n            f.write('"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/distlib/index.py",
      "line_number": 1,
      "details": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport hashlib\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport tempfile\ntry:\n    from threading import Thread\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import Thread\n\nfrom . import DistlibException\nfrom .compat import (HTTPBasicAuthHandler, Request, HTTPPasswordMgr,\n                     urlparse, build_opener, string_types)\nfrom .util import zip_dir, ServerProxy\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_INDEX = 'https://pypi.org/pypi'\nDEFAULT_REALM = 'pypi'\n\n\nclass PackageIndex(object):\n    \"\"\"\n    This class represents a package index compatible with PyPI, the Python\n    Package Index.\n    \"\"\"\n\n    boundary = b'----------ThIs_Is_tHe_distlib_index_bouNdaRY_$'\n\n    def __init__(self, url=None):\n        \"\"\"\n        Initialise an instance.\n\n        :param url: The URL of the index. If not specified, the URL for PyPI is\n                    used.\n        \"\"\"\n        self.url = url or DEFAULT_INDEX\n        self.read_configuration()\n        scheme, netloc, path, params, query, frag = urlparse(self.url)\n        if params or query or frag or scheme not in ('http', 'https'):\n            raise DistlibException('invalid repository: %s' % self.url)\n        self.password_handler = None\n        self.ssl_verifier = None\n        self.gpg = None\n        self.gpg_home = None\n        with open(os.devnull, 'w') as sink:\n            # Use gpg by default rather than gpg2, as gpg2 insists on\n            # prompting for passwords\n            for s in ('gpg', 'gpg2'):\n                try:\n                    rc = subprocess.check_call([s, '--version'], stdout=sink,\n                                               stderr=sink)\n                    if rc == 0:\n                        self.gpg = s\n                        break\n                except OSError:\n                    pass\n\n    def _get_pypirc_command(self):\n        \"\"\"\n        Get the distutils command for interacting with PyPI configurations.\n        :return: the command.\n        \"\"\"\n        from .util import _get_pypirc_command as cmd\n        return cmd()\n\n    def read_configuration(self):\n        \"\"\"\n        Read the PyPI access configuration as supported by distutils. This populates\n        ``username``, ``password``, ``realm`` and ``url`` attributes from the\n        configuration.\n        \"\"\"\n        from .util import _load_pypirc\n        cfg = _load_pypirc(self)\n        self.username = cfg.get('username')\n        self.password = cfg.get('password')\n        self.realm = cfg.get('realm', 'pypi')\n        self.url = cfg.get('repository', self.url)\n\n    def save_configuration(self):\n        \"\"\"\n        Save the PyPI access configuration. You must have set ``username`` and\n        ``password`` attributes before calling this method.\n        \"\"\"\n        self.check_credentials()\n        from .util import _store_pypirc\n        _store_pypirc(self)\n\n    def check_credentials(self):\n        \"\"\"\n        Check that ``username`` and ``password`` have been set, and raise an\n        exception if not.\n        \"\"\"\n        if self.username is None or self.password is None:\n            raise DistlibException('username and password must be set')\n        pm = HTTPPasswordMgr()\n        _, netloc, _, _, _, _ = urlparse(self.url)\n        pm.add_password(self.realm, netloc, self.username, self.password)\n        self.password_handler = HTTPBasicAuthHandler(pm)\n\n    def register(self, metadata):  # pragma: no cover\n        \"\"\"\n        Register a distribution on PyPI, using the provided metadata.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the distribution to be\n                         registered.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        metadata.validate()\n        d = metadata.todict()\n        d[':action'] = 'verify'\n        request = self.encode_request(d.items(), [])\n        self.send_request(request)\n        d[':action'] = 'submit'\n        request = self.encode_request(d.items(), [])\n        return self.send_request(request)\n\n    def _reader(self, name, stream, outbuf):\n        \"\"\"\n        Thread runner for reading lines of from a subprocess into a buffer.\n\n        :param name: The logical name of the stream (used for logging only).\n        :param stream: The stream to read from. This will typically a pipe\n                       connected to the output stream of a subprocess.\n        :param outbuf: The list to append the read lines to.\n        \"\"\"\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()\n\n    def get_sign_command(self, filename, signer, sign_password, keystore=None):  # pragma: no cover\n        \"\"\"\n        Return a suitable command for signing a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The signing command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        if sign_password is not None:\n            cmd.extend(['--batch', '--passphrase-fd', '0'])\n        td = tempfile.mkdtemp()\n        sf = os.path.join(td, os.path.basename(filename) + '.asc')\n        cmd.extend(['--detach-sign', '--armor', '--local-user',\n                    signer, '--output', sf, filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd, sf\n\n    def run_command(self, cmd, input_data=None):\n        \"\"\"\n        Run a command in a child process , passing it any input data specified.\n\n        :param cmd: The command to run.\n        :param input_data: If specified, this must be a byte string containing\n                           data to be sent to the child process.\n        :return: A tuple consisting of the subprocess' exit code, a list of\n                 lines read from the subprocess' ``stdout``, and a list of\n                 lines read from the subprocess' ``stderr``.\n        \"\"\"\n        kwargs = {\n            'stdout': subprocess.PIPE,\n            'stderr': subprocess.PIPE,\n        }\n        if input_data is not None:\n            kwargs['stdin'] = subprocess.PIPE\n        stdout = []\n        stderr = []\n        p = subprocess.Popen(cmd, **kwargs)\n        # We don't use communicate() here because we may need to\n        # get clever with interacting with the command\n        t1 = Thread(target=self._reader, args=('stdout', p.stdout, stdout))\n        t1.start()\n        t2 = Thread(target=self._reader, args=('stderr', p.stderr, stderr))\n        t2.start()\n        if input_data is not None:\n            p.stdin.write(input_data)\n            p.stdin.close()\n\n        p.wait()\n        t1.join()\n        t2.join()\n        return p.returncode, stdout, stderr\n\n    def sign_file(self, filename, signer, sign_password, keystore=None):  # pragma: no cover\n        \"\"\"\n        Sign a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The absolute pathname of the file where the signature is\n                 stored.\n        \"\"\"\n        cmd, sig_file = self.get_sign_command(filename, signer, sign_password,\n                                              keystore)\n        rc, stdout, stderr = self.run_command(cmd,\n                                              sign_password.encode('utf-8'))\n        if rc != 0:\n            raise DistlibException('sign command failed with error '\n                                   'code %s' % rc)\n        return sig_file\n\n    def upload_file(self, metadata, filename, signer=None, sign_password=None,\n                    filetype='sdist', pyversion='source', keystore=None):\n        \"\"\"\n        Upload a release file to the index.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the file to be uploaded.\n        :param filename: The pathname of the file to be uploaded.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param filetype: The type of the file being uploaded. This is the\n                        distutils command which produced that file, e.g.\n                        ``sdist`` or ``bdist_wheel``.\n        :param pyversion: The version of Python which the release relates\n                          to. For code compatible with any Python, this would\n                          be ``source``, otherwise it would be e.g. ``3.2``.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        if not os.path.exists(filename):\n            raise DistlibException('not found: %s' % filename)\n        metadata.validate()\n        d = metadata.todict()\n        sig_file = None\n        if signer:\n            if not self.gpg:\n                logger.warning('no signing program available - not signed')\n            else:\n                sig_file = self.sign_file(filename, signer, sign_password,\n                                          keystore)\n        with open(filename, 'rb') as f:\n            file_data = f.read()\n        md5_digest = hashlib.md5(file_data).hexdigest()\n        sha256_digest = hashlib.sha256(file_data).hexdigest()\n        d.update({\n            ':action': 'file_upload',\n            'protocol_version': '1',\n            'filetype': filetype,\n            'pyversion': pyversion,\n            'md5_digest': md5_digest,\n            'sha256_digest': sha256_digest,\n        })\n        files = [('content', os.path.basename(filename), file_data)]\n        if sig_file:\n            with open(sig_file, 'rb') as f:\n                sig_data = f.read()\n            files.append(('gpg_signature', os.path.basename(sig_file),\n                         sig_data))\n            shutil.rmtree(os.path.dirname(sig_file))\n        request = self.encode_request(d.items(), files)\n        return self.send_request(request)\n\n    def upload_documentation(self, metadata, doc_dir):  # pragma: no cover\n        \"\"\"\n        Upload documentation to the index.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the documentation to be\n                         uploaded.\n        :param doc_dir: The pathname of the directory which contains the\n                        documentation. This should be the directory that\n                        contains the ``index.html`` for the documentation.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        if not os.path.isdir(doc_dir):\n            raise DistlibException('not a directory: %r' % doc_dir)\n        fn = os.path.join(doc_dir, 'index.html')\n        if not os.path.exists(fn):\n            raise DistlibException('not found: %r' % fn)\n        metadata.validate()\n        name, version = metadata.name, metadata.version\n        zip_data = zip_dir(doc_dir).getvalue()\n        fields = [(':action', 'doc_upload'),\n                  ('name', name), ('version', version)]\n        files = [('content', name, zip_data)]\n        request = self.encode_request(fields, files)\n        return self.send_request(request)\n\n    def get_verify_command(self, signature_filename, data_filename,\n                           keystore=None):\n        \"\"\"\n        Return a suitable command for verifying a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The verifying command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        cmd.extend(['--verify', signature_filename, data_filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd\n\n    def verify_signature(self, signature_filename, data_filename,\n                         keystore=None):\n        \"\"\"\n        Verify a signature for a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: True if the signature was verified, else False.\n        \"\"\"\n        if not self.gpg:\n            raise DistlibException('verification unavailable because gpg '\n                                   'unavailable')\n        cmd = self.get_verify_command(signature_filename, data_filename,\n                                      keystore)\n        rc, stdout, stderr = self.run_command(cmd)\n        if rc not in (0, 1):\n            raise DistlibException('verify command failed with error code %s' % rc)\n        return rc == 0\n\n    def download_file(self, url, destfile, digest=None, reporthook=None):\n        \"\"\"\n        This is a convenience method for downloading a file from an URL.\n        Normally, this will be a file from the index, though currently\n        no check is made for this (i.e. a file can be downloaded from\n        anywhere).\n\n        The method is just like the :func:`urlretrieve` function in the\n        standard library, except that it allows digest computation to be\n        done during download and checking that the downloaded data\n        matched any expected value.\n\n        :param url: The URL of the file to be downloaded (assumed to be\n                    available via an HTTP GET request).\n        :param destfile: The pathname where the downloaded file is to be\n                         saved.\n        :param digest: If specified, this must be a (hasher, value)\n                       tuple, where hasher is the algorithm used (e.g.\n                       ``'md5'``) and ``value`` is the expected value.\n        :param reporthook: The same as for :func:`urlretrieve` in the\n                           standard library.\n        \"\"\"\n        if digest is None:\n            digester = None\n            logger.debug('No digest specified')\n        else:\n            if isinstance(digest, (list, tuple)):\n                hasher, digest = digest\n            else:\n                hasher = 'md5'\n            digester = getattr(hashlib, hasher)()\n            logger.debug('Digest specified: %s' % digest)\n        # The following code is equivalent to urlretrieve.\n        # We need to do it this way so that we can compute the\n        # digest of the file as we go.\n        with open(destfile, 'wb') as dfp:\n            # addinfourl is not a context manager on 2.x\n            # so we have to use try/finally\n            sfp = self.send_request(Request(url))\n            try:\n                headers = sfp.info()\n                blocksize = 8192\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, blocksize, size)\n                while True:\n                    block = sfp.read(blocksize)\n                    if not block:\n                        break\n                    read += len(block)\n                    dfp.write(block)\n                    if digester:\n                        digester.update(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, blocksize, size)\n            finally:\n                sfp.close()\n\n        # check that we got the whole file, if we can\n        if size >= 0 and read < size:\n            raise DistlibException(\n                'retrieval incomplete: got only %d out of %d bytes'\n                % (read, size))\n        # if we have a digest, it must match.\n        if digester:\n            actual = digester.hexdigest()\n            if digest != actual:\n                raise DistlibException('%s digest mismatch for %s: expected '\n                                       '%s, got %s' % (hasher, destfile,\n                                                       digest, actual))\n            logger.debug('Digest verified: %s', digest)\n\n    def send_request(self, req):\n        \"\"\"\n        Send a standard library :class:`Request` to PyPI and return its\n        response.\n\n        :param req: The request to send.\n        :return: The HTTP response from PyPI (a standard library HTTPResponse).\n        \"\"\"\n        handlers = []\n        if self.password_handler:\n            handlers.append(self.password_handler)\n        if self.ssl_verifier:\n            handlers.append(self.ssl_verifier)\n        opener = build_opener(*handlers)\n        return opener.open(req)\n\n    def encode_request(self, fields, files):\n        \"\"\"\n        Encode fields and files for posting to an HTTP server.\n\n        :param fields: The fields to send as a list of (fieldname, value)\n                       tuples.\n        :param files: The files to send as a list of (fieldname, filename,\n                      file_bytes) tuple.\n        \"\"\"\n        # Adapted from packaging, which in turn was adapted from\n        # http://code.activestate.com/recipes/146306\n\n        parts = []\n        boundary = self.boundary\n        for k, values in fields:\n            if not isinstance(values, (list, tuple)):\n                values = [values]\n\n            for v in values:\n                parts.extend((\n                    b'--' + boundary,\n                    ('Content-Disposition: form-data; name=\"%s\"' %\n                     k).encode('utf-8'),\n                    b'',\n                    v.encode('utf-8')))\n        for key, filename, value in files:\n            parts.extend((\n                b'--' + boundary,\n                ('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' %\n                 (key, filename)).encode('utf-8'),\n                b'',\n                value))\n\n        parts.extend((b'--' + boundary + b'--', b''))\n\n        body = b'\\r"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/distlib/metadata.py",
      "line_number": 10,
      "details": "|')\n                values = [values]\n\n            if field in _LISTTUPLEFIELDS:\n                values = [','.join(value) for value in values]\n\n            for value in values:\n                self._write_field(fileobject, field, value)\n\n    def update(self, other=None, **kwargs):\n        \"\"\"Set metadata values from the given iterable `other` and kwargs.\n\n        Behavior is like `dict.update`: If `other` has a ``keys`` method,\n        they are looped over and ``self[key]`` is assigned ``other[key]``.\n        Else, ``other`` is an iterable of ``(key, value)`` iterables.\n\n        Keys that don't match a metadata field or that have an empty value are\n        dropped.\n        \"\"\"\n        def _set(key, value):\n            if key in _ATTR2FIELD and value:\n                self.set(self._convert_name(key), value)\n\n        if not other:\n            # other is None or empty container\n            pass\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                _set(k, other[k])\n        else:\n            for k, v in other:\n                _set(k, v)\n\n        if kwargs:\n            for k, v in kwargs.items():\n                _set(k, v)\n\n    def set(self, name, value):\n        \"\"\"Control then set a metadata field.\"\"\"\n        name = self._convert_name(name)\n\n        if ((name in _ELEMENTSFIELD or name == 'Platform') and\n            not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [v.strip() for v in value.split(',')]\n            else:\n                value = []\n        elif (name in _LISTFIELDS and\n              not isinstance(value, (list, tuple))):\n            if isinstance(value, string_types):\n                value = [value]\n            else:\n                value = []\n\n        if logger.isEnabledFor(logging.WARNING):\n            project_name = self['Name']\n\n            scheme = get_scheme(self.scheme)\n            if name in _PREDICATE_FIELDS and value is not None:\n                for v in value:\n                    # check that the values are valid\n                    if not scheme.is_valid_matcher(v.split(';')[0]):\n                        logger.warning(\n                            \"'%s': '%s' is not valid (field '%s')\",\n                            project_name, v, name)\n            # FIXME this rejects UNKNOWN, is that right?\n            elif name in _VERSIONS_FIELDS and value is not None:\n                if not scheme.is_valid_constraint_list(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\",\n                                   project_name, value, name)\n            elif name in _VERSION_FIELDS and value is not None:\n                if not scheme.is_valid_version(value):\n                    logger.warning(\"'%s': '%s' is not a valid version (field '%s')\",\n                                   project_name, value, name)\n\n        if name in _UNICODEFIELDS:\n            if name == 'Description':\n                value = self._remove_line_prefix(value)\n\n        self._fields[name] = value\n\n    def get(self, name, default=_MISSING):\n        \"\"\"Get a metadata field.\"\"\"\n        name = self._convert_name(name)\n        if name not in self._fields:\n            if default is _MISSING:\n                default = self._default_value(name)\n            return default\n        if name in _UNICODEFIELDS:\n            value = self._fields[name]\n            return value\n        elif name in _LISTFIELDS:\n            value = self._fields[name]\n            if value is None:\n                return []\n            res = []\n            for val in value:\n                if name not in _LISTTUPLEFIELDS:\n                    res.append(val)\n                else:\n                    # That's for Project-URL\n                    res.append((val[0], val[1]))\n            return res\n\n        elif name in _ELEMENTSFIELD:\n            value = self._fields[name]\n            if isinstance(value, string_types):\n                return value.split(',')\n        return self._fields[name]\n\n    def check(self, strict=False):\n        \"\"\"Check if the metadata is compliant. If strict is True then raise if\n        no Name or Version are provided\"\"\"\n        self.set_metadata_version()\n\n        # XXX should check the versions (if the file was loaded)\n        missing, warnings = [], []\n\n        for attr in ('Name', 'Version'):  # required by PEP 345\n            if attr not in self:\n                missing.append(attr)\n\n        if strict and missing != []:\n            msg = 'missing required metadata: %s' % ', '.join(missing)\n            raise MetadataMissingError(msg)\n\n        for attr in ('Home-page', 'Author'):\n            if attr not in self:\n                missing.append(attr)\n\n        # checking metadata 1.2 (XXX needs to check 1.1, 1.0)\n        if self['Metadata-Version'] != '1.2':\n            return missing, warnings\n\n        scheme = get_scheme(self.scheme)\n\n        def are_valid_constraints(value):\n            for v in value:\n                if not scheme.is_valid_matcher(v.split(';')[0]):\n                    return False\n            return True\n\n        for fields, controller in ((_PREDICATE_FIELDS, are_valid_constraints),\n                                   (_VERSIONS_FIELDS,\n                                    scheme.is_valid_constraint_list),\n                                   (_VERSION_FIELDS,\n                                    scheme.is_valid_version)):\n            for field in fields:\n                value = self.get(field, None)\n                if value is not None and not controller(value):\n                    warnings.append(\"Wrong value for '%s': %s\" % (field, value))\n\n        return missing, warnings\n\n    def todict(self, skip_missing=False):\n        \"\"\"Return fields as a dict.\n\n        Field names will be converted to use the underscore-lowercase style\n        instead of hyphen-mixed case (i.e. home_page instead of Home-page).\n        This is as per https://www.python.org/dev/peps/pep-0566/#id17.\n        \"\"\"\n        self.set_metadata_version()\n\n        fields = _version2fieldlist(self['Metadata-Version'])\n\n        data = {}\n\n        for field_name in fields:\n            if not skip_missing or field_name in self._fields:\n                key = _FIELD2ATTR[field_name]\n                if key != 'project_url':\n                    data[key] = self[field_name]\n                else:\n                    data[key] = [','.join(u) for u in self[field_name]]\n\n        return data\n\n    def add_requirements(self, requirements):\n        if self['Metadata-Version'] == '1.1':\n            # we can't have 1.1 metadata *and* Setuptools requires\n            for field in ('Obsoletes', 'Requires', 'Provides'):\n                if field in self:\n                    del self[field]\n        self['Requires-Dist'] += requirements\n\n    # Mapping API\n    # TODO could add iter* variants\n\n    def keys(self):\n        return list(_version2fieldlist(self['Metadata-Version']))\n\n    def __iter__(self):\n        for key in self.keys():\n            yield key\n\n    def values(self):\n        return [self[key] for key in self.keys()]\n\n    def items(self):\n        return [(key, self[key]) for key in self.keys()]\n\n    def __repr__(self):\n        return '<%s %s %s>' % (self.__class__.__name__, self.name,\n                               self.version)\n\n\nMETADATA_FILENAME = 'pydist.json'\nWHEEL_METADATA_FILENAME = 'metadata.json'\nLEGACY_METADATA_FILENAME = 'METADATA'\n\n\nclass Metadata(object):\n    \"\"\"\n    The metadata of a release. This implementation uses 2.1\n    metadata where possible. If not possible, it wraps a LegacyMetadata\n    instance which handles the key-value metadata format.\n    \"\"\"\n\n    METADATA_VERSION_MATCHER = re.compile(r'^\\d+(\\.\\d+)*$')\n\n    NAME_MATCHER = re.compile('^[0-9A-Z]([0-9A-Z_.-]*[0-9A-Z])?$', re.I)\n\n    FIELDNAME_MATCHER = re.compile('^[A-Z]([0-9A-Z-]*[0-9A-Z])?$', re.I)\n\n    VERSION_MATCHER = PEP440_VERSION_RE\n\n    SUMMARY_MATCHER = re.compile('.{1,2047}')\n\n    METADATA_VERSION = '2.0'\n\n    GENERATOR = 'distlib (%s)' % __version__\n\n    MANDATORY_KEYS = {\n        'name': (),\n        'version': (),\n        'summary': ('legacy',),\n    }\n\n    INDEX_KEYS = ('name version license summary description author '\n                  'author_email keywords platform home_page classifiers '\n                  'download_url')\n\n    DEPENDENCY_KEYS = ('extras run_requires test_requires build_requires '\n                       'dev_requires provides meta_requires obsoleted_by '\n                       'supports_environments')\n\n    SYNTAX_VALIDATORS = {\n        'metadata_version': (METADATA_VERSION_MATCHER, ()),\n        'name': (NAME_MATCHER, ('legacy',)),\n        'version': (VERSION_MATCHER, ('legacy',)),\n        'summary': (SUMMARY_MATCHER, ('legacy',)),\n        'dynamic': (FIELDNAME_MATCHER, ('legacy',)),\n    }\n\n    __slots__ = ('_legacy', '_data', 'scheme')\n\n    def __init__(self, path=None, fileobj=None, mapping=None,\n                 scheme='default'):\n        if [path, fileobj, mapping].count(None) < 2:\n            raise TypeError('path, fileobj and mapping are exclusive')\n        self._legacy = None\n        self._data = None\n        self.scheme = scheme\n        #import pdb; pdb.set_trace()\n        if mapping is not None:\n            try:\n                self._validate_mapping(mapping, scheme)\n                self._data = mapping\n            except MetadataUnrecognizedVersionError:\n                self._legacy = LegacyMetadata(mapping=mapping, scheme=scheme)\n                self.validate()\n        else:\n            data = None\n            if path:\n                with open(path, 'rb') as f:\n                    data = f.read()\n            elif fileobj:\n                data = fileobj.read()\n            if data is None:\n                # Initialised with no args - to be added\n                self._data = {\n                    'metadata_version': self.METADATA_VERSION,\n                    'generator': self.GENERATOR,\n                }\n            else:\n                if not isinstance(data, text_type):\n                    data = data.decode('utf-8')\n                try:\n                    self._data = json.loads(data)\n                    self._validate_mapping(self._data, scheme)\n                except ValueError:\n                    # Note: MetadataUnrecognizedVersionError does not\n                    # inherit from ValueError (it's a DistlibException,\n                    # which should not inherit from ValueError).\n                    # The ValueError comes from the json.load - if that\n                    # succeeds and we get a validation error, we want\n                    # that to propagate\n                    self._legacy = LegacyMetadata(fileobj=StringIO(data),\n                                                  scheme=scheme)\n                    self.validate()\n\n    common_keys = set(('name', 'version', 'license', 'keywords', 'summary'))\n\n    none_list = (None, list)\n    none_dict = (None, dict)\n\n    mapped_keys = {\n        'run_requires': ('Requires-Dist', list),\n        'build_requires': ('Setup-Requires-Dist', list),\n        'dev_requires': none_list,\n        'test_requires': none_list,\n        'meta_requires': none_list,\n        'extras': ('Provides-Extra', list),\n        'modules': none_list,\n        'namespaces': none_list,\n        'exports': none_dict,\n        'commands': none_dict,\n        'classifiers': ('Classifier', list),\n        'source_url': ('Download-URL', None),\n        'metadata_version': ('Metadata-Version', None),\n    }\n\n    del none_list, none_dict\n\n    def __getattribute__(self, key):\n        common = object.__getattribute__(self, 'common_keys')\n        mapped = object.__getattribute__(self, 'mapped_keys')\n        if key in mapped:\n            lk, maker = mapped[key]\n            if self._legacy:\n                if lk is None:\n                    result = None if maker is None else maker()\n                else:\n                    result = self._legacy.get(lk)\n            else:\n                value = None if maker is None else maker()\n                if key not in ('commands', 'exports', 'modules', 'namespaces',\n                               'classifiers'):\n                    result = self._data.get(key, value)\n                else:\n                    # special cases for PEP 459\n                    sentinel = object()\n                    result = sentinel\n                    d = self._data.get('extensions')\n                    if d:\n                        if key == 'commands':\n                            result = d.get('python.commands', value)\n                        elif key == 'classifiers':\n                            d = d.get('python.details')\n                            if d:\n                                result = d.get(key, value)\n                        else:\n                            d = d.get('python.exports')\n                            if not d:\n                                d = self._data.get('python.exports')\n                            if d:\n                                result = d.get(key, value)\n                    if result is sentinel:\n                        result = value\n        elif key not in common:\n            result = object.__getattribute__(self, key)\n        elif self._legacy:\n            result = self._legacy.get(key)\n        else:\n            result = self._data.get(key)\n        return result\n\n    def _validate_value(self, key, value, scheme=None):\n        if key in self.SYNTAX_VALIDATORS:\n            pattern, exclusions = self.SYNTAX_VALIDATORS[key]\n            if (scheme or self.scheme) not in exclusions:\n                m = pattern.match(value)\n                if not m:\n                    raise MetadataInvalidError(\"'%s' is an invalid value for \"\n                                               \"the '%s' property\" % (value,\n                                                                    key))\n\n    def __setattr__(self, key, value):\n        self._validate_value(key, value)\n        common = object.__getattribute__(self, 'common_keys')\n        mapped = object.__getattribute__(self, 'mapped_keys')\n        if key in mapped:\n            lk, _ = mapped[key]\n            if self._legacy:\n                if lk is None:\n                    raise NotImplementedError\n                self._legacy[lk] = value\n            elif key not in ('commands', 'exports', 'modules', 'namespaces',\n                             'classifiers'):\n                self._data[key] = value\n            else:\n                # special cases for PEP 459\n                d = self._data.setdefault('extensions', {})\n                if key == 'commands':\n                    d['python.commands'] = value\n                elif key == 'classifiers':\n                    d = d.setdefault('python.details', {})\n                    d[key] = value\n                else:\n                    d = d.setdefault('python.exports', {})\n                    d[key] = value\n        elif key not in common:\n            object.__setattr__(self, key, value)\n        else:\n            if key == 'keywords':\n                if isinstance(value, string_types):\n                    value = value.strip()\n                    if value:\n                        value = value.split()\n                    else:\n                        value = []\n            if self._legacy:\n                self._legacy[key] = value\n            else:\n                self._data[key] = value\n\n    @property\n    def name_and_version(self):\n        return _get_name_and_version(self.name, self.version, True)\n\n    @property\n    def provides(self):\n        if self._legacy:\n            result = self._legacy['Provides-Dist']\n        else:\n            result = self._data.setdefault('provides', [])\n        s = '%s (%s)' % (self.name, self.version)\n        if s not in result:\n            result.append(s)\n        return result\n\n    @provides.setter\n    def provides(self, value):\n        if self._legacy:\n            self._legacy['Provides-Dist'] = value\n        else:\n            self._data['provides'] = value\n\n    def get_requirements(self, reqts, extras=None, env=None):\n        \"\"\"\n        Base method to get dependencies, given a set of extras\n        to satisfy and an optional environment context.\n        :param reqts: A list of sometimes-wanted dependencies,\n                      perhaps dependent on extras and environment.\n        :param extras: A list of optional components being requested.\n        :param env: An optional environment for marker evaluation.\n        \"\"\"\n        if self._legacy:\n            result = reqts\n        else:\n            result = []\n            extras = get_extras(extras or [], self.extras)\n            for d in reqts:\n                if 'extra' not in d and 'environment' not in d:\n                    # unconditional\n                    include = True\n                else:\n                    if 'extra' not in d:\n                        # Not extra-dependent - only environment-dependent\n                        include = True\n                    else:\n                        include = d.get('extra') in extras\n                    if include:\n                        # Not excluded because of extras, check environment\n                        marker = d.get('environment')\n                        if marker:\n                            include = interpret(marker, env)\n                if include:\n                    result.extend(d['requires'])\n            for key in ('build', 'dev', 'test'):\n                e = ':%s:' % key\n                if e in extras:\n                    extras.remove(e)\n                    # A recursive call, but it should terminate since 'test'\n                    # has been removed from the extras\n                    reqts = self._data.get('%s_requires' % key, [])\n                    result.extend(self.get_requirements(reqts, extras=extras,\n                                                        env=env))\n        return result\n\n    @property\n    def dictionary(self):\n        if self._legacy:\n            return self._from_legacy()\n        return self._data\n\n    @property\n    def dependencies(self):\n        if self._legacy:\n            raise NotImplementedError\n        else:\n            return extract_by_key(self._data, self.DEPENDENCY_KEYS)\n\n    @dependencies.setter\n    def dependencies(self, value):\n        if self._legacy:\n            raise NotImplementedError\n        else:\n            self._data.update(value)\n\n    def _validate_mapping(self, mapping, scheme):\n        if mapping.get('metadata_version') != self.METADATA_VERSION:\n            raise MetadataUnrecognizedVersionError()\n        missing = []\n        for key, exclusions in self.MANDATORY_KEYS.items():\n            if key not in mapping:\n                if scheme not in exclusions:\n                    missing.append(key)\n        if missing:\n            msg = 'Missing metadata items: %s' % ', '.join(missing)\n            raise MetadataMissingError(msg)\n        for k, v in mapping.items():\n            self._validate_value(k, v, scheme)\n\n    def validate(self):\n        if self._legacy:\n            missing, warnings = self._legacy.check(True)\n            if missing or warnings:\n                logger.warning('Metadata: missing: %s, warnings: %s',\n                               missing, warnings)\n        else:\n            self._validate_mapping(self._data, self.scheme)\n\n    def todict(self):\n        if self._legacy:\n            return self._legacy.todict(True)\n        else:\n            result = extract_by_key(self._data, self.INDEX_KEYS)\n            return result\n\n    def _from_legacy(self):\n        assert self._legacy and not self._data\n        result = {\n            'metadata_version': self.METADATA_VERSION,\n            'generator': self.GENERATOR,\n        }\n        lmd = self._legacy.todict(True)     # skip missing ones\n        for k in ('name', 'version', 'license', 'summary', 'description',\n                  'classifier'):\n            if k in lmd:\n                if k == 'classifier':\n                    nk = 'classifiers'\n                else:\n                    nk = k\n                result[nk] = lmd[k]\n        kw = lmd.get('Keywords', [])\n        if kw == ['']:\n            kw = []\n        result['keywords'] = kw\n        keys = (('requires_dist', 'run_requires'),\n                ('setup_requires_dist', 'build_requires'))\n        for ok, nk in keys:\n            if ok in lmd and lmd[ok]:\n                result[nk] = [{'requires': lmd[ok]}]\n        result['provides'] = self.provides\n        author = {}\n        maintainer = {}\n        return result\n\n    LEGACY_MAPPING = {\n        'name': 'Name',\n        'version': 'Version',\n        ('extensions', 'python.details', 'license'): 'License',\n        'summary': 'Summary',\n        'description': 'Description',\n        ('extensions', 'python.project', 'project_urls', 'Home'): 'Home-page',\n        ('extensions', 'python.project', 'contacts', 0, 'name'): 'Author',\n        ('extensions', 'python.project', 'contacts', 0, 'email'): 'Author-email',\n        'source_url': 'Download-URL',\n        ('extensions', 'python.details', 'classifiers'): 'Classifier',\n    }\n\n    def _to_legacy(self):\n        def process_entries(entries):\n            reqts = set()\n            for e in entries:\n                extra = e.get('extra')\n                env = e.get('environment')\n                rlist = e['requires']\n                for r in rlist:\n                    if not env and not extra:\n                        reqts.add(r)\n                    else:\n                        marker = ''\n                        if extra:\n                            marker = 'extra == \"%s\"' % extra\n                        if env:\n                            if marker:\n                                marker = '(%s) and %s' % (env, marker)\n                            else:\n                                marker = env\n                        reqts.add(';'.join((r, marker)))\n            return reqts\n\n        assert self._data and not self._legacy\n        result = LegacyMetadata()\n        nmd = self._data\n        # import pdb; pdb.set_trace()\n        for nk, ok in self.LEGACY_MAPPING.items():\n            if not isinstance(nk, tuple):\n                if nk in nmd:\n                    result[ok] = nmd[nk]\n            else:\n                d = nmd\n                found = True\n                for k in nk:\n                    try:\n                        d = d[k]\n                    except (KeyError, IndexError):\n                        found = False\n                        break\n                if found:\n                    result[ok] = d\n        r1 = process_entries(self.run_requires + self.meta_requires)\n        r2 = process_entries(self.build_requires + self.dev_requires)\n        if self.extras:\n            result['Provides-Extra'] = sorted(self.extras)\n        result['Requires-Dist'] = sorted(r1)\n        result['Setup-Requires-Dist'] = sorted(r2)\n        # TODO: any other fields wanted\n        return result\n\n    def write(self, path=None, fileobj=None, legacy=False, skip_unknown=True):\n        if [path, fileobj].count(None) != 1:\n            raise ValueError('Exactly one of path and fileobj is needed')\n        self.validate()\n        if legacy:\n            if self._legacy:\n                legacy_md = self._legacy\n            else:\n                legacy_md = self._to_legacy()\n            if path:\n                legacy_md.write(path, skip_unknown=skip_unknown)\n            else:\n                legacy_md.write_file(fileobj, skip_unknown=skip_unknown)\n        else:\n            if self._legacy:\n                d = self._from_legacy()\n            else:\n                d = self._data\n            if fileobj:\n                json.dump(d, fileobj, ensure_ascii=True, indent=2,\n                          sort_keys=True)\n            else:\n                with codecs.open(path, 'w', 'utf-8') as f:\n                    json.dump(d, f, ensure_ascii=True, indent=2,\n                              sort_keys=True)\n\n    def add_requirements(self, requirements):\n        if self._legacy:\n            self._legacy.add_requirements(requirements)\n        else:\n            run_requires = self._data.setdefault('run_requires', [])\n            always = None\n            for entry in run_requires:\n                if 'environment' not in entry and 'extra' not in entry:\n                    always = entry\n                    break\n            if always is None:\n                always = { 'requires': requirements }\n                run_requires.insert(0, always)\n            else:\n                rset = set(always['requires']) | set(requirements)\n                always['requires'] = sorted(rset)\n\n    def __repr__(self):\n        name = self.name or '(no name)'\n        version = self.version or 'no version'\n        return '<%s %s %s (%s)>' % (self.__class__.__name__,\n                                    self.metadata_version, name, version)"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/distro/distro.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python\n# Copyright 2015,2016,2017 Nir Cohen\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThe ``distro`` package (``distro`` stands for Linux Distribution) provides\ninformation about the Linux distribution it runs on, such as a reliable\nmachine-readable distro ID, or version information.\n\nIt is the recommended replacement for Python's original\n:py:func:`platform.linux_distribution` function, but it provides much more\nfunctionality. An alternative implementation became necessary because Python\n3.5 deprecated this function, and Python 3.8 removed it altogether. Its\npredecessor function :py:func:`platform.dist` was already deprecated since\nPython 2.6 and removed in Python 3.8. Still, there are many cases in which\naccess to OS distribution information is needed. See `Python issue 1322\n<https://bugs.python.org/issue1322>`_ for more information.\n\"\"\"\n\nimport argparse\nimport json\nimport logging\nimport os\nimport re\nimport shlex\nimport subprocess\nimport sys\nimport warnings\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    Optional,\n    Sequence,\n    TextIO,\n    Tuple,\n    Type,\n)\n\ntry:\n    from typing import TypedDict\nexcept ImportError:\n    # Python 3.7\n    TypedDict = dict\n\n__version__ = \"1.8.0\"\n\n\nclass VersionDict(TypedDict):\n    major: str\n    minor: str\n    build_number: str\n\n\nclass InfoDict(TypedDict):\n    id: str\n    version: str\n    version_parts: VersionDict\n    like: str\n    codename: str\n\n\n_UNIXCONFDIR = os.environ.get(\"UNIXCONFDIR\", \"/etc\")\n_UNIXUSRLIBDIR = os.environ.get(\"UNIXUSRLIBDIR\", \"/usr/lib\")\n_OS_RELEASE_BASENAME = \"os-release\"\n\n#: Translation table for normalizing the \"ID\" attribute defined in os-release\n#: files, for use by the :func:`distro.id` method.\n#:\n#: * Key: Value as defined in the os-release file, translated to lower case,\n#:   with blanks translated to underscores.\n#:\n#: * Value: Normalized value.\nNORMALIZED_OS_ID = {\n    \"ol\": \"oracle\",  # Oracle Linux\n    \"opensuse-leap\": \"opensuse\",  # Newer versions of OpenSuSE report as opensuse-leap\n}\n\n#: Translation table for normalizing the \"Distributor ID\" attribute returned by\n#: the lsb_release command, for use by the :func:`distro.id` method.\n#:\n#: * Key: Value as returned by the lsb_release command, translated to lower\n#:   case, with blanks translated to underscores.\n#:\n#: * Value: Normalized value.\nNORMALIZED_LSB_ID = {\n    \"enterpriseenterpriseas\": \"oracle\",  # Oracle Enterprise Linux 4\n    \"enterpriseenterpriseserver\": \"oracle\",  # Oracle Linux 5\n    \"redhatenterpriseworkstation\": \"rhel\",  # RHEL 6, 7 Workstation\n    \"redhatenterpriseserver\": \"rhel\",  # RHEL 6, 7 Server\n    \"redhatenterprisecomputenode\": \"rhel\",  # RHEL 6 ComputeNode\n}\n\n#: Translation table for normalizing the distro ID derived from the file name\n#: of distro release files, for use by the :func:`distro.id` method.\n#:\n#: * Key: Value as derived from the file name of a distro release file,\n#:   translated to lower case, with blanks translated to underscores.\n#:\n#: * Value: Normalized value.\nNORMALIZED_DISTRO_ID = {\n    \"redhat\": \"rhel\",  # RHEL 6.x, 7.x\n}\n\n# Pattern for content of distro release file (reversed)\n_DISTRO_RELEASE_CONTENT_REVERSED_PATTERN = re.compile(\n    r\"(?:[^)]*\\)(.*)\\()? *(?:STL )?([\\d.+\\-a-z]*\\d) *(?:esaeler *)?(.+)\"\n)\n\n# Pattern for base file name of distro release file\n_DISTRO_RELEASE_BASENAME_PATTERN = re.compile(r\"(\\w+)[-_](release|version)$\")\n\n# Base file names to be looked up for if _UNIXCONFDIR is not readable.\n_DISTRO_RELEASE_BASENAMES = [\n    \"SuSE-release\",\n    \"arch-release\",\n    \"base-release\",\n    \"centos-release\",\n    \"fedora-release\",\n    \"gentoo-release\",\n    \"mageia-release\",\n    \"mandrake-release\",\n    \"mandriva-release\",\n    \"mandrivalinux-release\",\n    \"manjaro-release\",\n    \"oracle-release\",\n    \"redhat-release\",\n    \"rocky-release\",\n    \"sl-release\",\n    \"slackware-version\",\n]\n\n# Base file names to be ignored when searching for distro release file\n_DISTRO_RELEASE_IGNORE_BASENAMES = (\n    \"debian_version\",\n    \"lsb-release\",\n    \"oem-release\",\n    _OS_RELEASE_BASENAME,\n    \"system-release\",\n    \"plesk-release\",\n    \"iredmail-release\",\n)\n\n\ndef linux_distribution(full_distribution_name: bool = True) -> Tuple[str, str, str]:\n    \"\"\"\n    .. deprecated:: 1.6.0\n\n        :func:`distro.linux_distribution()` is deprecated. It should only be\n        used as a compatibility shim with Python's\n        :py:func:`platform.linux_distribution()`. Please use :func:`distro.id`,\n        :func:`distro.version` and :func:`distro.name` instead.\n\n    Return information about the current OS distribution as a tuple\n    ``(id_name, version, codename)`` with items as follows:\n\n    * ``id_name``:  If *full_distribution_name* is false, the result of\n      :func:`distro.id`. Otherwise, the result of :func:`distro.name`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``codename``:  The extra item (usually in parentheses) after the\n      os-release version number, or the result of :func:`distro.codename`.\n\n    The interface of this function is compatible with the original\n    :py:func:`platform.linux_distribution` function, supporting a subset of\n    its parameters.\n\n    The data it returns may not exactly be the same, because it uses more data\n    sources than the original function, and that may lead to different data if\n    the OS distribution is not consistent across multiple data sources it\n    provides (there are indeed such distributions ...).\n\n    Another reason for differences is the fact that the :func:`distro.id`\n    method normalizes the distro ID string to a reliable machine-readable value\n    for a number of popular OS distributions.\n    \"\"\"\n    warnings.warn(\n        \"distro.linux_distribution() is deprecated. It should only be used as a \"\n        \"compatibility shim with Python's platform.linux_distribution(). Please use \"\n        \"distro.id(), distro.version() and distro.name() instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return _distro.linux_distribution(full_distribution_name)\n\n\ndef id() -> str:\n    \"\"\"\n    Return the distro ID of the current distribution, as a\n    machine-readable string.\n\n    For a number of OS distributions, the returned distro ID value is\n    *reliable*, in the sense that it is documented and that it does not change\n    across releases of the distribution.\n\n    This package maintains the following reliable distro ID values:\n\n    ==============  =========================================\n    Distro ID       Distribution\n    ==============  =========================================\n    \"ubuntu\"        Ubuntu\n    \"debian\"        Debian\n    \"rhel\"          RedHat Enterprise Linux\n    \"centos\"        CentOS\n    \"fedora\"        Fedora\n    \"sles\"          SUSE Linux Enterprise Server\n    \"opensuse\"      openSUSE\n    \"amzn\"          Amazon Linux\n    \"arch\"          Arch Linux\n    \"buildroot\"     Buildroot\n    \"cloudlinux\"    CloudLinux OS\n    \"exherbo\"       Exherbo Linux\n    \"gentoo\"        GenToo Linux\n    \"ibm_powerkvm\"  IBM PowerKVM\n    \"kvmibm\"        KVM for IBM z Systems\n    \"linuxmint\"     Linux Mint\n    \"mageia\"        Mageia\n    \"mandriva\"      Mandriva Linux\n    \"parallels\"     Parallels\n    \"pidora\"        Pidora\n    \"raspbian\"      Raspbian\n    \"oracle\"        Oracle Linux (and Oracle Enterprise Linux)\n    \"scientific\"    Scientific Linux\n    \"slackware\"     Slackware\n    \"xenserver\"     XenServer\n    \"openbsd\"       OpenBSD\n    \"netbsd\"        NetBSD\n    \"freebsd\"       FreeBSD\n    \"midnightbsd\"   MidnightBSD\n    \"rocky\"         Rocky Linux\n    \"aix\"           AIX\n    \"guix\"          Guix System\n    ==============  =========================================\n\n    If you have a need to get distros for reliable IDs added into this set,\n    or if you find that the :func:`distro.id` function returns a different\n    distro ID for one of the listed distros, please create an issue in the\n    `distro issue tracker`_.\n\n    **Lookup hierarchy and transformations:**\n\n    First, the ID is obtained from the following sources, in the specified\n    order. The first available and non-empty value is used:\n\n    * the value of the \"ID\" attribute of the os-release file,\n\n    * the value of the \"Distributor ID\" attribute returned by the lsb_release\n      command,\n\n    * the first part of the file name of the distro release file,\n\n    The so determined ID value then passes the following transformations,\n    before it is returned by this method:\n\n    * it is translated to lower case,\n\n    * blanks (which should not be there anyway) are translated to underscores,\n\n    * a normalization of the ID is performed, based upon\n      `normalization tables`_. The purpose of this normalization is to ensure\n      that the ID is as reliable as possible, even across incompatible changes\n      in the OS distributions. A common reason for an incompatible change is\n      the addition of an os-release file, or the addition of the lsb_release\n      command, with ID values that differ from what was previously determined\n      from the distro release file name.\n    \"\"\"\n    return _distro.id()\n\n\ndef name(pretty: bool = False) -> str:\n    \"\"\"\n    Return the name of the current OS distribution, as a human-readable\n    string.\n\n    If *pretty* is false, the name is returned without version or codename.\n    (e.g. \"CentOS Linux\")\n\n    If *pretty* is true, the version and codename are appended.\n    (e.g. \"CentOS Linux 7.1.1503 (Core)\")\n\n    **Lookup hierarchy:**\n\n    The name is obtained from the following sources, in the specified order.\n    The first available and non-empty value is used:\n\n    * If *pretty* is false:\n\n      - the value of the \"NAME\" attribute of the os-release file,\n\n      - the value of the \"Distributor ID\" attribute returned by the lsb_release\n        command,\n\n      - the value of the \"<name>\" field of the distro release file.\n\n    * If *pretty* is true:\n\n      - the value of the \"PRETTY_NAME\" attribute of the os-release file,\n\n      - the value of the \"Description\" attribute returned by the lsb_release\n        command,\n\n      - the value of the \"<name>\" field of the distro release file, appended\n        with the value of the pretty version (\"<version_id>\" and \"<codename>\"\n        fields) of the distro release file, if available.\n    \"\"\"\n    return _distro.name(pretty)\n\n\ndef version(pretty: bool = False, best: bool = False) -> str:\n    \"\"\"\n    Return the version of the current OS distribution, as a human-readable\n    string.\n\n    If *pretty* is false, the version is returned without codename (e.g.\n    \"7.0\").\n\n    If *pretty* is true, the codename in parenthesis is appended, if the\n    codename is non-empty (e.g. \"7.0 (Maipo)\").\n\n    Some distributions provide version numbers with different precisions in\n    the different sources of distribution information. Examining the different\n    sources in a fixed priority order does not always yield the most precise\n    version (e.g. for Debian 8.2, or CentOS 7.1).\n\n    Some other distributions may not provide this kind of information. In these\n    cases, an empty string would be returned. This behavior can be observed\n    with rolling releases distributions (e.g. Arch Linux).\n\n    The *best* parameter can be used to control the approach for the returned\n    version:\n\n    If *best* is false, the first non-empty version number in priority order of\n    the examined sources is returned.\n\n    If *best* is true, the most precise version number out of all examined\n    sources is returned.\n\n    **Lookup hierarchy:**\n\n    In all cases, the version number is obtained from the following sources.\n    If *best* is false, this order represents the priority order:\n\n    * the value of the \"VERSION_ID\" attribute of the os-release file,\n    * the value of the \"Release\" attribute returned by the lsb_release\n      command,\n    * the version number parsed from the \"<version_id>\" field of the first line\n      of the distro release file,\n    * the version number parsed from the \"PRETTY_NAME\" attribute of the\n      os-release file, if it follows the format of the distro release files.\n    * the version number parsed from the \"Description\" attribute returned by\n      the lsb_release command, if it follows the format of the distro release\n      files.\n    \"\"\"\n    return _distro.version(pretty, best)\n\n\ndef version_parts(best: bool = False) -> Tuple[str, str, str]:\n    \"\"\"\n    Return the version of the current OS distribution as a tuple\n    ``(major, minor, build_number)`` with items as follows:\n\n    * ``major``:  The result of :func:`distro.major_version`.\n\n    * ``minor``:  The result of :func:`distro.minor_version`.\n\n    * ``build_number``:  The result of :func:`distro.build_number`.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.version_parts(best)\n\n\ndef major_version(best: bool = False) -> str:\n    \"\"\"\n    Return the major version of the current OS distribution, as a string,\n    if provided.\n    Otherwise, the empty string is returned. The major version is the first\n    part of the dot-separated version string.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.major_version(best)\n\n\ndef minor_version(best: bool = False) -> str:\n    \"\"\"\n    Return the minor version of the current OS distribution, as a string,\n    if provided.\n    Otherwise, the empty string is returned. The minor version is the second\n    part of the dot-separated version string.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.minor_version(best)\n\n\ndef build_number(best: bool = False) -> str:\n    \"\"\"\n    Return the build number of the current OS distribution, as a string,\n    if provided.\n    Otherwise, the empty string is returned. The build number is the third part\n    of the dot-separated version string.\n\n    For a description of the *best* parameter, see the :func:`distro.version`\n    method.\n    \"\"\"\n    return _distro.build_number(best)\n\n\ndef like() -> str:\n    \"\"\"\n    Return a space-separated list of distro IDs of distributions that are\n    closely related to the current OS distribution in regards to packaging\n    and programming interfaces, for example distributions the current\n    distribution is a derivative from.\n\n    **Lookup hierarchy:**\n\n    This information item is only provided by the os-release file.\n    For details, see the description of the \"ID_LIKE\" attribute in the\n    `os-release man page\n    <http://www.freedesktop.org/software/systemd/man/os-release.html>`_.\n    \"\"\"\n    return _distro.like()\n\n\ndef codename() -> str:\n    \"\"\"\n    Return the codename for the release of the current OS distribution,\n    as a string.\n\n    If the distribution does not have a codename, an empty string is returned.\n\n    Note that the returned codename is not always really a codename. For\n    example, openSUSE returns \"x86_64\". This function does not handle such\n    cases in any special way and just returns the string it finds, if any.\n\n    **Lookup hierarchy:**\n\n    * the codename within the \"VERSION\" attribute of the os-release file, if\n      provided,\n\n    * the value of the \"Codename\" attribute returned by the lsb_release\n      command,\n\n    * the value of the \"<codename>\" field of the distro release file.\n    \"\"\"\n    return _distro.codename()\n\n\ndef info(pretty: bool = False, best: bool = False) -> InfoDict:\n    \"\"\"\n    Return certain machine-readable information items about the current OS\n    distribution in a dictionary, as shown in the following example:\n\n    .. sourcecode:: python\n\n        {\n            'id': 'rhel',\n            'version': '7.0',\n            'version_parts': {\n                'major': '7',\n                'minor': '0',\n                'build_number': ''\n            },\n            'like': 'fedora',\n            'codename': 'Maipo'\n        }\n\n    The dictionary structure and keys are always the same, regardless of which\n    information items are available in the underlying data sources. The values\n    for the various keys are as follows:\n\n    * ``id``:  The result of :func:`distro.id`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``version_parts -> major``:  The result of :func:`distro.major_version`.\n\n    * ``version_parts -> minor``:  The result of :func:`distro.minor_version`.\n\n    * ``version_parts -> build_number``:  The result of\n      :func:`distro.build_number`.\n\n    * ``like``:  The result of :func:`distro.like`.\n\n    * ``codename``:  The result of :func:`distro.codename`.\n\n    For a description of the *pretty* and *best* parameters, see the\n    :func:`distro.version` method.\n    \"\"\"\n    return _distro.info(pretty, best)\n\n\ndef os_release_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the os-release file data source of the current OS distribution.\n\n    See `os-release file`_ for details about these information items.\n    \"\"\"\n    return _distro.os_release_info()\n\n\ndef lsb_release_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the lsb_release command data source of the current OS distribution.\n\n    See `lsb_release command output`_ for details about these information\n    items.\n    \"\"\"\n    return _distro.lsb_release_info()\n\n\ndef distro_release_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the distro release file data source of the current OS distribution.\n\n    See `distro release file`_ for details about these information items.\n    \"\"\"\n    return _distro.distro_release_info()\n\n\ndef uname_info() -> Dict[str, str]:\n    \"\"\"\n    Return a dictionary containing key-value pairs for the information items\n    from the distro release file data source of the current OS distribution.\n    \"\"\"\n    return _distro.uname_info()\n\n\ndef os_release_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the os-release file data source\n    of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n      The empty string, if the item does not exist.\n\n    See `os-release file`_ for details about these information items.\n    \"\"\"\n    return _distro.os_release_attr(attribute)\n\n\ndef lsb_release_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the lsb_release command output\n    data source of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n      The empty string, if the item does not exist.\n\n    See `lsb_release command output`_ for details about these information\n    items.\n    \"\"\"\n    return _distro.lsb_release_attr(attribute)\n\n\ndef distro_release_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the distro release file\n    data source of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n      The empty string, if the item does not exist.\n\n    See `distro release file`_ for details about these information items.\n    \"\"\"\n    return _distro.distro_release_attr(attribute)\n\n\ndef uname_attr(attribute: str) -> str:\n    \"\"\"\n    Return a single named information item from the distro release file\n    data source of the current OS distribution.\n\n    Parameters:\n\n    * ``attribute`` (string): Key of the information item.\n\n    Returns:\n\n    * (string): Value of the information item, if the item exists.\n                The empty string, if the item does not exist.\n    \"\"\"\n    return _distro.uname_attr(attribute)\n\n\ntry:\n    from functools import cached_property\nexcept ImportError:\n    # Python < 3.8\n    class cached_property:  # type: ignore\n        \"\"\"A version of @property which caches the value.  On access, it calls the\n        underlying function and sets the value in `__dict__` so future accesses\n        will not re-call the property.\n        \"\"\"\n\n        def __init__(self, f: Callable[[Any], Any]) -> None:\n            self._fname = f.__name__\n            self._f = f\n\n        def __get__(self, obj: Any, owner: Type[Any]) -> Any:\n            assert obj is not None, f\"call {self._fname} on an instance\"\n            ret = obj.__dict__[self._fname] = self._f(obj)\n            return ret\n\n\nclass LinuxDistribution:\n    \"\"\"\n    Provides information about a OS distribution.\n\n    This package creates a private module-global instance of this class with\n    default initialization arguments, that is used by the\n    `consolidated accessor functions`_ and `single source accessor functions`_.\n    By using default initialization arguments, that module-global instance\n    returns data about the current OS distribution (i.e. the distro this\n    package runs on).\n\n    Normally, it is not necessary to create additional instances of this class.\n    However, in situations where control is needed over the exact data sources\n    that are used, instances of this class can be created with a specific\n    distro release file, or a specific os-release file, or without invoking the\n    lsb_release command.\n    \"\"\"\n\n    def __init__(\n        self,\n        include_lsb: Optional[bool] = None,\n        os_release_file: str = \"\",\n        distro_release_file: str = \"\",\n        include_uname: Optional[bool] = None,\n        root_dir: Optional[str] = None,\n        include_oslevel: Optional[bool] = None,\n    ) -> None:\n        \"\"\"\n        The initialization method of this class gathers information from the\n        available data sources, and stores that in private instance attributes.\n        Subsequent access to the information items uses these private instance\n        attributes, so that the data sources are read only once.\n\n        Parameters:\n\n        * ``include_lsb`` (bool): Controls whether the\n          `lsb_release command output`_ is included as a data source.\n\n          If the lsb_release command is not available in the program execution\n          path, the data source for the lsb_release command will be empty.\n\n        * ``os_release_file`` (string): The path name of the\n          `os-release file`_ that is to be used as a data source.\n\n          An empty string (the default) will cause the default path name to\n          be used (see `os-release file`_ for details).\n\n          If the specified or defaulted os-release file does not exist, the\n          data source for the os-release file will be empty.\n\n        * ``distro_release_file`` (string): The path name of the\n          `distro release file`_ that is to be used as a data source.\n\n          An empty string (the default) will cause a default search algorithm\n          to be used (see `distro release file`_ for details).\n\n          If the specified distro release file does not exist, or if no default\n          distro release file can be found, the data source for the distro\n          release file will be empty.\n\n        * ``include_uname`` (bool): Controls whether uname command output is\n          included as a data source. If the uname command is not available in\n          the program execution path the data source for the uname command will\n          be empty.\n\n        * ``root_dir`` (string): The absolute path to the root directory to use\n          to find distro-related information files. Note that ``include_*``\n          parameters must not be enabled in combination with ``root_dir``.\n\n        * ``include_oslevel`` (bool): Controls whether (AIX) oslevel command\n          output is included as a data source. If the oslevel command is not\n          available in the program execution path the data source will be\n          empty.\n\n        Public instance attributes:\n\n        * ``os_release_file`` (string): The path name of the\n          `os-release file`_ that is actually used as a data source. The\n          empty string if no distro release file is used as a data source.\n\n        * ``distro_release_file`` (string): The path name of the\n          `distro release file`_ that is actually used as a data source. The\n          empty string if no distro release file is used as a data source.\n\n        * ``include_lsb`` (bool): The result of the ``include_lsb`` parameter.\n          This controls whether the lsb information will be loaded.\n\n        * ``include_uname`` (bool): The result of the ``include_uname``\n          parameter. This controls whether the uname information will\n          be loaded.\n\n        * ``include_oslevel`` (bool): The result of the ``include_oslevel``\n          parameter. This controls whether (AIX) oslevel information will be\n          loaded.\n\n        * ``root_dir`` (string): The result of the ``root_dir`` parameter.\n          The absolute path to the root directory to use to find distro-related\n          information files.\n\n        Raises:\n\n        * :py:exc:`ValueError`: Initialization parameters combination is not\n           supported.\n\n        * :py:exc:`OSError`: Some I/O issue with an os-release file or distro\n          release file.\n\n        * :py:exc:`UnicodeError`: A data source has unexpected characters or\n          uses an unexpected encoding.\n        \"\"\"\n        self.root_dir = root_dir\n        self.etc_dir = os.path.join(root_dir, \"etc\") if root_dir else _UNIXCONFDIR\n        self.usr_lib_dir = (\n            os.path.join(root_dir, \"usr/lib\") if root_dir else _UNIXUSRLIBDIR\n        )\n\n        if os_release_file:\n            self.os_release_file = os_release_file\n        else:\n            etc_dir_os_release_file = os.path.join(self.etc_dir, _OS_RELEASE_BASENAME)\n            usr_lib_os_release_file = os.path.join(\n                self.usr_lib_dir, _OS_RELEASE_BASENAME\n            )\n\n            # NOTE: The idea is to respect order **and** have it set\n            #       at all times for API backwards compatibility.\n            if os.path.isfile(etc_dir_os_release_file) or not os.path.isfile(\n                usr_lib_os_release_file\n            ):\n                self.os_release_file = etc_dir_os_release_file\n            else:\n                self.os_release_file = usr_lib_os_release_file\n\n        self.distro_release_file = distro_release_file or \"\"  # updated later\n\n        is_root_dir_defined = root_dir is not None\n        if is_root_dir_defined and (include_lsb or include_uname or include_oslevel):\n            raise ValueError(\n                \"Including subprocess data sources from specific root_dir is disallowed\"\n                \" to prevent false information\"\n            )\n        self.include_lsb = (\n            include_lsb if include_lsb is not None else not is_root_dir_defined\n        )\n        self.include_uname = (\n            include_uname if include_uname is not None else not is_root_dir_defined\n        )\n        self.include_oslevel = (\n            include_oslevel if include_oslevel is not None else not is_root_dir_defined\n        )\n\n    def __repr__(self) -> str:\n        \"\"\"Return repr of all info\"\"\"\n        return (\n            \"LinuxDistribution(\"\n            \"os_release_file={self.os_release_file!r}, \"\n            \"distro_release_file={self.distro_release_file!r}, \"\n            \"include_lsb={self.include_lsb!r}, \"\n            \"include_uname={self.include_uname!r}, \"\n            \"include_oslevel={self.include_oslevel!r}, \"\n            \"root_dir={self.root_dir!r}, \"\n            \"_os_release_info={self._os_release_info!r}, \"\n            \"_lsb_release_info={self._lsb_release_info!r}, \"\n            \"_distro_release_info={self._distro_release_info!r}, \"\n            \"_uname_info={self._uname_info!r}, \"\n            \"_oslevel_info={self._oslevel_info!r})\".format(self=self)\n        )\n\n    def linux_distribution(\n        self, full_distribution_name: bool = True\n    ) -> Tuple[str, str, str]:\n        \"\"\"\n        Return information about the OS distribution that is compatible\n        with Python's :func:`platform.linux_distribution`, supporting a subset\n        of its parameters.\n\n        For details, see :func:`distro.linux_distribution`.\n        \"\"\"\n        return (\n            self.name() if full_distribution_name else self.id(),\n            self.version(),\n            self._os_release_info.get(\"release_codename\") or self.codename(),\n        )\n\n    def id(self) -> str:\n        \"\"\"Return the distro ID of the OS distribution, as a string.\n\n        For details, see :func:`distro.id`.\n        \"\"\"\n\n        def normalize(distro_id: str, table: Dict[str, str]) -> str:\n            distro_id = distro_id.lower().replace(\" \", \"_\")\n            return table.get(distro_id, distro_id)\n\n        distro_id = self.os_release_attr(\"id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_OS_ID)\n\n        distro_id = self.lsb_release_attr(\"distributor_id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_LSB_ID)\n\n        distro_id = self.distro_release_attr(\"id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        distro_id = self.uname_attr(\"id\")\n        if distro_id:\n            return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n        return \"\"\n\n    def name(self, pretty: bool = False) -> str:\n        \"\"\"\n        Return the name of the OS distribution, as a string.\n\n        For details, see :func:`distro.name`.\n        \"\"\"\n        name = (\n            self.os_release_attr(\"name\")\n            or self.lsb_release_attr(\"distributor_id\")\n            or self.distro_release_attr(\"name\")\n            or self.uname_attr(\"name\")\n        )\n        if pretty:\n            name = self.os_release_attr(\"pretty_name\") or self.lsb_release_attr(\n                \"description\"\n            )\n            if not name:\n                name = self.distro_release_attr(\"name\") or self.uname_attr(\"name\")\n                version = self.version(pretty=True)\n                if version:\n                    name = f\"{name} {version}\"\n        return name or \"\"\n\n    def version(self, pretty: bool = False, best: bool = False) -> str:\n        \"\"\"\n        Return the version of the OS distribution, as a string.\n\n        For details, see :func:`distro.version`.\n        \"\"\"\n        versions = [\n            self.os_release_attr(\"version_id\"),\n            self.lsb_release_attr(\"release\"),\n            self.distro_release_attr(\"version_id\"),\n            self._parse_distro_release_content(self.os_release_attr(\"pretty_name\")).get(\n                \"version_id\", \"\"\n            ),\n            self._parse_distro_release_content(\n                self.lsb_release_attr(\"description\")\n            ).get(\"version_id\", \"\"),\n            self.uname_attr(\"release\"),\n        ]\n        if self.uname_attr(\"id\").startswith(\"aix\"):\n            # On AIX platforms, prefer oslevel command output.\n            versions.insert(0, self.oslevel_info())\n        elif self.id() == \"debian\" or \"debian\" in self.like().split():\n            # On Debian-like, add debian_version file content to candidates list.\n            versions.append(self._debian_version)\n        version = \"\"\n        if best:\n            # This algorithm uses the last version in priority order that has\n            # the best precision. If the versions are not in conflict, that\n            # does not matter; otherwise, using the last one instead of the\n            # first one might be considered a surprise.\n            for v in versions:\n                if v.count(\".\") > version.count(\".\") or version == \"\":\n                    version = v\n        else:\n            for v in versions:\n                if v != \"\":\n                    version = v\n                    break\n        if pretty and version and self.codename():\n            version = f\"{version} ({self.codename()})\"\n        return version\n\n    def version_parts(self, best: bool = False) -> Tuple[str, str, str]:\n        \"\"\"\n        Return the version of the OS distribution, as a tuple of version\n        numbers.\n\n        For details, see :func:`distro.version_parts`.\n        \"\"\"\n        version_str = self.version(best=best)\n        if version_str:\n            version_regex = re.compile(r\"(\\d+)\\.?(\\d+)?\\.?(\\d+)?\")\n            matches = version_regex.match(version_str)\n            if matches:\n                major, minor, build_number = matches.groups()\n                return major, minor or \"\", build_number or \"\"\n        return \"\", \"\", \"\"\n\n    def major_version(self, best: bool = False) -> str:\n        \"\"\"\n        Return the major version number of the current distribution.\n\n        For details, see :func:`distro.major_version`.\n        \"\"\"\n        return self.version_parts(best)[0]\n\n    def minor_version(self, best: bool = False) -> str:\n        \"\"\"\n        Return the minor version number of the current distribution.\n\n        For details, see :func:`distro.minor_version`.\n        \"\"\"\n        return self.version_parts(best)[1]\n\n    def build_number(self, best: bool = False) -> str:\n        \"\"\"\n        Return the build number of the current distribution.\n\n        For details, see :func:`distro.build_number`.\n        \"\"\"\n        return self.version_parts(best)[2]\n\n    def like(self) -> str:\n        \"\"\"\n        Return the IDs of distributions that are like the OS distribution.\n\n        For details, see :func:`distro.like`.\n        \"\"\"\n        return self.os_release_attr(\"id_like\") or \"\"\n\n    def codename(self) -> str:\n        \"\"\"\n        Return the codename of the OS distribution.\n\n        For details, see :func:`distro.codename`.\n        \"\"\"\n        try:\n            # Handle os_release specially since distros might purposefully set\n            # this to empty string to have no codename\n            return self._os_release_info[\"codename\"]\n        except KeyError:\n            return (\n                self.lsb_release_attr(\"codename\")\n                or self.distro_release_attr(\"codename\")\n                or \"\"\n            )\n\n    def info(self, pretty: bool = False, best: bool = False) -> InfoDict:\n        \"\"\"\n        Return certain machine-readable information about the OS\n        distribution.\n\n        For details, see :func:`distro.info`.\n        \"\"\"\n        return dict(\n            id=self.id(),\n            version=self.version(pretty, best),\n            version_parts=dict(\n                major=self.major_version(best),\n                minor=self.minor_version(best),\n                build_number=self.build_number(best),\n            ),\n            like=self.like(),\n            codename=self.codename(),\n        )\n\n    def os_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the os-release file data source of the OS distribution.\n\n        For details, see :func:`distro.os_release_info`.\n        \"\"\"\n        return self._os_release_info\n\n    def lsb_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the lsb_release command data source of the OS\n        distribution.\n\n        For details, see :func:`distro.lsb_release_info`.\n        \"\"\"\n        return self._lsb_release_info\n\n    def distro_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the distro release file data source of the OS\n        distribution.\n\n        For details, see :func:`distro.distro_release_info`.\n        \"\"\"\n        return self._distro_release_info\n\n    def uname_info(self) -> Dict[str, str]:\n        \"\"\"\n        Return a dictionary containing key-value pairs for the information\n        items from the uname command data source of the OS distribution.\n\n        For details, see :func:`distro.uname_info`.\n        \"\"\"\n        return self._uname_info\n\n    def oslevel_info(self) -> str:\n        \"\"\"\n        Return AIX' oslevel command output.\n        \"\"\"\n        return self._oslevel_info\n\n    def os_release_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the os-release file data\n        source of the OS distribution.\n\n        For details, see :func:`distro.os_release_attr`.\n        \"\"\"\n        return self._os_release_info.get(attribute, \"\")\n\n    def lsb_release_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the lsb_release command\n        output data source of the OS distribution.\n\n        For details, see :func:`distro.lsb_release_attr`.\n        \"\"\"\n        return self._lsb_release_info.get(attribute, \"\")\n\n    def distro_release_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the distro release file\n        data source of the OS distribution.\n\n        For details, see :func:`distro.distro_release_attr`.\n        \"\"\"\n        return self._distro_release_info.get(attribute, \"\")\n\n    def uname_attr(self, attribute: str) -> str:\n        \"\"\"\n        Return a single named information item from the uname command\n        output data source of the OS distribution.\n\n        For details, see :func:`distro.uname_attr`.\n        \"\"\"\n        return self._uname_info.get(attribute, \"\")\n\n    @cached_property\n    def _os_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Get the information items from the specified os-release file.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if os.path.isfile(self.os_release_file):\n            with open(self.os_release_file, encoding=\"utf-8\") as release_file:\n                return self._parse_os_release_content(release_file)\n        return {}\n\n    @staticmethod\n    def _parse_os_release_content(lines: TextIO) -> Dict[str, str]:\n        \"\"\"\n        Parse the lines of an os-release file.\n\n        Parameters:\n\n        * lines: Iterable through the lines in the os-release file.\n                 Each line must be a unicode string or a UTF-8 encoded byte\n                 string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        props = {}\n        lexer = shlex.shlex(lines, posix=True)\n        lexer.whitespace_split = True\n\n        tokens = list(lexer)\n        for token in tokens:\n            # At this point, all shell-like parsing has been done (i.e.\n            # comments processed, quotes and backslash escape sequences\n            # processed, multi-line values assembled, trailing newlines\n            # stripped, etc.), so the tokens are now either:\n            # * variable assignments: var=value\n            # * commands or their arguments (not allowed in os-release)\n            # Ignore any tokens that are not variable assignments\n            if \"=\" in token:\n                k, v = token.split(\"=\", 1)\n                props[k.lower()] = v\n\n        if \"version\" in props:\n            # extract release codename (if any) from version attribute\n            match = re.search(r\"\\((\\D+)\\)|,\\s*(\\D+)\", props[\"version\"])\n            if match:\n                release_codename = match.group(1) or match.group(2)\n                props[\"codename\"] = props[\"release_codename\"] = release_codename\n\n        if \"version_codename\" in props:\n            # os-release added a version_codename field.  Use that in\n            # preference to anything else Note that some distros purposefully\n            # do not have code names.  They should be setting\n            # version_codename=\"\"\n            props[\"codename\"] = props[\"version_codename\"]\n        elif \"ubuntu_codename\" in props:\n            # Same as above but a non-standard field name used on older Ubuntus\n            props[\"codename\"] = props[\"ubuntu_codename\"]\n\n        return props\n\n    @cached_property\n    def _lsb_release_info(self) -> Dict[str, str]:\n        \"\"\"\n        Get the information items from the lsb_release command output.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        if not self.include_lsb:\n            return {}\n        try:\n            cmd = (\"lsb_release\", \"-a\")\n            stdout = subprocess.check_output(cmd, stderr=subprocess.DEVNULL)\n        # Command not found or lsb_release returned error\n        except (OSError, subprocess.CalledProcessError):\n            return {}\n        content = self._to_str(stdout).splitlines()\n        return self._parse_lsb_release_content(content)\n\n    @staticmethod\n    def _parse_lsb_release_content(lines: Iterable[str]) -> Dict[str, str]:\n        \"\"\"\n        Parse the output of the lsb_release command.\n\n        Parameters:\n\n        * lines: Iterable through the lines of the lsb_release output.\n                 Each line must be a unicode string or a UTF-8 encoded byte\n                 string.\n\n        Returns:\n            A dictionary containing all information items.\n        \"\"\"\n        props = {}\n        for line in lines:\n            kv = line.strip(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/requests/auth.py",
      "line_number": 1,
      "details": "\"\"\"\nrequests.auth\n~~~~~~~~~~~~~\n\nThis module contains the authentication handlers for Requests.\n\"\"\"\n\nimport hashlib\nimport os\nimport re\nimport threading\nimport time\nimport warnings\nfrom base64 import b64encode\n\nfrom ._internal_utils import to_native_string\nfrom .compat import basestring, str, urlparse\nfrom .cookies import extract_cookies_to_jar\nfrom .utils import parse_dict_header\n\nCONTENT_TYPE_FORM_URLENCODED = \"application/x-www-form-urlencoded\"\nCONTENT_TYPE_MULTI_PART = \"multipart/form-data\"\n\n\ndef _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n\n    if not isinstance(password, basestring):\n        warnings.warn(\n            \"Non-string passwords will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(type(password)),\n            category=DeprecationWarning,\n        )\n        password = str(password)\n    # -- End Removal --\n\n    if isinstance(username, str):\n        username = username.encode(\"latin1\")\n\n    if isinstance(password, str):\n        password = password.encode(\"latin1\")\n\n    authstr = \"Basic \" + to_native_string(\n        b64encode(b\":\".join((username, password))).strip()\n    )\n\n    return authstr\n\n\nclass AuthBase:\n    \"\"\"Base class that all auth implementations derive from\"\"\"\n\n    def __call__(self, r):\n        raise NotImplementedError(\"Auth hooks must be callable.\")\n\n\nclass HTTPBasicAuth(AuthBase):\n    \"\"\"Attaches HTTP Basic Authentication to the given Request object.\"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __eq__(self, other):\n        return all(\n            [\n                self.username == getattr(other, \"username\", None),\n                self.password == getattr(other, \"password\", None),\n            ]\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __call__(self, r):\n        r.headers[\"Authorization\"] = _basic_auth_str(self.username, self.password)\n        return r\n\n\nclass HTTPProxyAuth(HTTPBasicAuth):\n    \"\"\"Attaches HTTP Proxy Authentication to a given Request object.\"\"\"\n\n    def __call__(self, r):\n        r.headers[\"Proxy-Authorization\"] = _basic_auth_str(self.username, self.password)\n        return r\n\n\nclass HTTPDigestAuth(AuthBase):\n    \"\"\"Attaches HTTP Digest Authentication to the given Request object.\"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        # Keep state in per-thread local storage\n        self._thread_local = threading.local()\n\n    def init_per_thread_state(self):\n        # Ensure state is initialized just once per-thread\n        if not hasattr(self._thread_local, \"init\"):\n            self._thread_local.init = True\n            self._thread_local.last_nonce = \"\"\n            self._thread_local.nonce_count = 0\n            self._thread_local.chal = {}\n            self._thread_local.pos = None\n            self._thread_local.num_401_calls = None\n\n    def build_digest_header(self, method, url):\n        \"\"\"\n        :rtype: str\n        \"\"\"\n\n        realm = self._thread_local.chal[\"realm\"]\n        nonce = self._thread_local.chal[\"nonce\"]\n        qop = self._thread_local.chal.get(\"qop\")\n        algorithm = self._thread_local.chal.get(\"algorithm\")\n        opaque = self._thread_local.chal.get(\"opaque\")\n        hash_utf8 = None\n\n        if algorithm is None:\n            _algorithm = \"MD5\"\n        else:\n            _algorithm = algorithm.upper()\n        # lambdas assume digest modules are imported at the top level\n        if _algorithm == \"MD5\" or _algorithm == \"MD5-SESS\":\n\n            def md5_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.md5(x).hexdigest()\n\n            hash_utf8 = md5_utf8\n        elif _algorithm == \"SHA\":\n\n            def sha_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha1(x).hexdigest()\n\n            hash_utf8 = sha_utf8\n        elif _algorithm == \"SHA-256\":\n\n            def sha256_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha256(x).hexdigest()\n\n            hash_utf8 = sha256_utf8\n        elif _algorithm == \"SHA-512\":\n\n            def sha512_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha512(x).hexdigest()\n\n            hash_utf8 = sha512_utf8\n\n        KD = lambda s, d: hash_utf8(f\"{s}:{d}\")  # noqa:E731\n\n        if hash_utf8 is None:\n            return None\n\n        # XXX not implemented yet\n        entdig = None\n        p_parsed = urlparse(url)\n        #: path is request-uri defined in RFC 2616 which should not be empty\n        path = p_parsed.path or \"/\"\n        if p_parsed.query:\n            path += f\"?{p_parsed.query}\"\n\n        A1 = f\"{self.username}:{realm}:{self.password}\"\n        A2 = f\"{method}:{path}\"\n\n        HA1 = hash_utf8(A1)\n        HA2 = hash_utf8(A2)\n\n        if nonce == self._thread_local.last_nonce:\n            self._thread_local.nonce_count += 1\n        else:\n            self._thread_local.nonce_count = 1\n        ncvalue = f\"{self._thread_local.nonce_count:08x}\"\n        s = str(self._thread_local.nonce_count).encode(\"utf-8\")\n        s += nonce.encode(\"utf-8\")\n        s += time.ctime().encode(\"utf-8\")\n        s += os.urandom(8)\n\n        cnonce = hashlib.sha1(s).hexdigest()[:16]\n        if _algorithm == \"MD5-SESS\":\n            HA1 = hash_utf8(f\"{HA1}:{nonce}:{cnonce}\")\n\n        if not qop:\n            respdig = KD(HA1, f\"{nonce}:{HA2}\")\n        elif qop == \"auth\" or \"auth\" in qop.split(\",\"):\n            noncebit = f\"{nonce}:{ncvalue}:{cnonce}:auth:{HA2}\"\n            respdig = KD(HA1, noncebit)\n        else:\n            # XXX handle auth-int.\n            return None\n\n        self._thread_local.last_nonce = nonce\n\n        # XXX should the partial digests be encoded too?\n        base = (\n            f'username=\"{self.username}\", realm=\"{realm}\", nonce=\"{nonce}\", '\n            f'uri=\"{path}\", response=\"{respdig}\"'\n        )\n        if opaque:\n            base += f', opaque=\"{opaque}\"'\n        if algorithm:\n            base += f', algorithm=\"{algorithm}\"'\n        if entdig:\n            base += f', digest=\"{entdig}\"'\n        if qop:\n            base += f', qop=\"auth\", nc={ncvalue}, cnonce=\"{cnonce}\"'\n\n        return f\"Digest {base}\"\n\n    def handle_redirect(self, r, **kwargs):\n        \"\"\"Reset num_401_calls counter on redirects.\"\"\"\n        if r.is_redirect:\n            self._thread_local.num_401_calls = 1\n\n    def handle_401(self, r, **kwargs):\n        \"\"\"\n        Takes the given response and tries digest-auth, if needed.\n\n        :rtype: requests.Response\n        \"\"\"\n\n        # If response is not 4xx, do not auth\n        # See https://github.com/psf/requests/issues/3772\n        if not 400 <= r.status_code < 500:\n            self._thread_local.num_401_calls = 1\n            return r\n\n        if self._thread_local.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            r.request.body.seek(self._thread_local.pos)\n        s_auth = r.headers.get(\"www-authenticate\", \"\")\n\n        if \"digest\" in s_auth.lower() and self._thread_local.num_401_calls < 2:\n\n            self._thread_local.num_401_calls += 1\n            pat = re.compile(r\"digest \", flags=re.IGNORECASE)\n            self._thread_local.chal = parse_dict_header(pat.sub(\"\", s_auth, count=1))\n\n            # Consume content and release the original connection\n            # to allow our new request to reuse the same one.\n            r.content\n            r.close()\n            prep = r.request.copy()\n            extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n            prep.prepare_cookies(prep._cookies)\n\n            prep.headers[\"Authorization\"] = self.build_digest_header(\n                prep.method, prep.url\n            )\n            _r = r.connection.send(prep, **kwargs)\n            _r.history.append(r)\n            _r.request = prep\n\n            return _r\n\n        self._thread_local.num_401_calls = 1\n        return r\n\n    def __call__(self, r):\n        # Initialize per-thread state, if needed\n        self.init_per_thread_state()\n        # If we have a saved nonce, skip the 401\n        if self._thread_local.last_nonce:\n            r.headers[\"Authorization\"] = self.build_digest_header(r.method, r.url)\n        try:\n            self._thread_local.pos = r.body.tell()\n        except AttributeError:\n            # In the case of HTTPDigestAuth being reused and the body of\n            # the previous request was a file-like object, pos has the\n            # file position of the previous body. Ensure it's set to\n            # None.\n            self._thread_local.pos = None\n        r.register_hook(\"response\", self.handle_401)\n        r.register_hook(\"response\", self.handle_redirect)\n        self._thread_local.num_401_calls = 1\n\n        return r\n\n    def __eq__(self, other):\n        return all(\n            [\n                self.username == getattr(other, \"username\", None),\n                self.password == getattr(other, \"password\", None),\n            ]\n        )\n\n    def __ne__(self, other):\n        return not self == other"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/requests/auth.py",
      "line_number": 1,
      "details": "\"\"\"\nrequests.auth\n~~~~~~~~~~~~~\n\nThis module contains the authentication handlers for Requests.\n\"\"\"\n\nimport hashlib\nimport os\nimport re\nimport threading\nimport time\nimport warnings\nfrom base64 import b64encode\n\nfrom ._internal_utils import to_native_string\nfrom .compat import basestring, str, urlparse\nfrom .cookies import extract_cookies_to_jar\nfrom .utils import parse_dict_header\n\nCONTENT_TYPE_FORM_URLENCODED = \"application/x-www-form-urlencoded\"\nCONTENT_TYPE_MULTI_PART = \"multipart/form-data\"\n\n\ndef _basic_auth_str(username, password):\n    \"\"\"Returns a Basic Auth string.\"\"\"\n\n    # \"I want us to put a big-ol' comment on top of it that\n    # says that this behaviour is dumb but we need to preserve\n    # it because people are relying on it.\"\n    #    - Lukasa\n    #\n    # These are here solely to maintain backwards compatibility\n    # for things like ints. This will be removed in 3.0.0.\n    if not isinstance(username, basestring):\n        warnings.warn(\n            \"Non-string usernames will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(username),\n            category=DeprecationWarning,\n        )\n        username = str(username)\n\n    if not isinstance(password, basestring):\n        warnings.warn(\n            \"Non-string passwords will no longer be supported in Requests \"\n            \"3.0.0. Please convert the object you've passed in ({!r}) to \"\n            \"a string or bytes object in the near future to avoid \"\n            \"problems.\".format(type(password)),\n            category=DeprecationWarning,\n        )\n        password = str(password)\n    # -- End Removal --\n\n    if isinstance(username, str):\n        username = username.encode(\"latin1\")\n\n    if isinstance(password, str):\n        password = password.encode(\"latin1\")\n\n    authstr = \"Basic \" + to_native_string(\n        b64encode(b\":\".join((username, password))).strip()\n    )\n\n    return authstr\n\n\nclass AuthBase:\n    \"\"\"Base class that all auth implementations derive from\"\"\"\n\n    def __call__(self, r):\n        raise NotImplementedError(\"Auth hooks must be callable.\")\n\n\nclass HTTPBasicAuth(AuthBase):\n    \"\"\"Attaches HTTP Basic Authentication to the given Request object.\"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def __eq__(self, other):\n        return all(\n            [\n                self.username == getattr(other, \"username\", None),\n                self.password == getattr(other, \"password\", None),\n            ]\n        )\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __call__(self, r):\n        r.headers[\"Authorization\"] = _basic_auth_str(self.username, self.password)\n        return r\n\n\nclass HTTPProxyAuth(HTTPBasicAuth):\n    \"\"\"Attaches HTTP Proxy Authentication to a given Request object.\"\"\"\n\n    def __call__(self, r):\n        r.headers[\"Proxy-Authorization\"] = _basic_auth_str(self.username, self.password)\n        return r\n\n\nclass HTTPDigestAuth(AuthBase):\n    \"\"\"Attaches HTTP Digest Authentication to the given Request object.\"\"\"\n\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        # Keep state in per-thread local storage\n        self._thread_local = threading.local()\n\n    def init_per_thread_state(self):\n        # Ensure state is initialized just once per-thread\n        if not hasattr(self._thread_local, \"init\"):\n            self._thread_local.init = True\n            self._thread_local.last_nonce = \"\"\n            self._thread_local.nonce_count = 0\n            self._thread_local.chal = {}\n            self._thread_local.pos = None\n            self._thread_local.num_401_calls = None\n\n    def build_digest_header(self, method, url):\n        \"\"\"\n        :rtype: str\n        \"\"\"\n\n        realm = self._thread_local.chal[\"realm\"]\n        nonce = self._thread_local.chal[\"nonce\"]\n        qop = self._thread_local.chal.get(\"qop\")\n        algorithm = self._thread_local.chal.get(\"algorithm\")\n        opaque = self._thread_local.chal.get(\"opaque\")\n        hash_utf8 = None\n\n        if algorithm is None:\n            _algorithm = \"MD5\"\n        else:\n            _algorithm = algorithm.upper()\n        # lambdas assume digest modules are imported at the top level\n        if _algorithm == \"MD5\" or _algorithm == \"MD5-SESS\":\n\n            def md5_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.md5(x).hexdigest()\n\n            hash_utf8 = md5_utf8\n        elif _algorithm == \"SHA\":\n\n            def sha_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha1(x).hexdigest()\n\n            hash_utf8 = sha_utf8\n        elif _algorithm == \"SHA-256\":\n\n            def sha256_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha256(x).hexdigest()\n\n            hash_utf8 = sha256_utf8\n        elif _algorithm == \"SHA-512\":\n\n            def sha512_utf8(x):\n                if isinstance(x, str):\n                    x = x.encode(\"utf-8\")\n                return hashlib.sha512(x).hexdigest()\n\n            hash_utf8 = sha512_utf8\n\n        KD = lambda s, d: hash_utf8(f\"{s}:{d}\")  # noqa:E731\n\n        if hash_utf8 is None:\n            return None\n\n        # XXX not implemented yet\n        entdig = None\n        p_parsed = urlparse(url)\n        #: path is request-uri defined in RFC 2616 which should not be empty\n        path = p_parsed.path or \"/\"\n        if p_parsed.query:\n            path += f\"?{p_parsed.query}\"\n\n        A1 = f\"{self.username}:{realm}:{self.password}\"\n        A2 = f\"{method}:{path}\"\n\n        HA1 = hash_utf8(A1)\n        HA2 = hash_utf8(A2)\n\n        if nonce == self._thread_local.last_nonce:\n            self._thread_local.nonce_count += 1\n        else:\n            self._thread_local.nonce_count = 1\n        ncvalue = f\"{self._thread_local.nonce_count:08x}\"\n        s = str(self._thread_local.nonce_count).encode(\"utf-8\")\n        s += nonce.encode(\"utf-8\")\n        s += time.ctime().encode(\"utf-8\")\n        s += os.urandom(8)\n\n        cnonce = hashlib.sha1(s).hexdigest()[:16]\n        if _algorithm == \"MD5-SESS\":\n            HA1 = hash_utf8(f\"{HA1}:{nonce}:{cnonce}\")\n\n        if not qop:\n            respdig = KD(HA1, f\"{nonce}:{HA2}\")\n        elif qop == \"auth\" or \"auth\" in qop.split(\",\"):\n            noncebit = f\"{nonce}:{ncvalue}:{cnonce}:auth:{HA2}\"\n            respdig = KD(HA1, noncebit)\n        else:\n            # XXX handle auth-int.\n            return None\n\n        self._thread_local.last_nonce = nonce\n\n        # XXX should the partial digests be encoded too?\n        base = (\n            f'username=\"{self.username}\", realm=\"{realm}\", nonce=\"{nonce}\", '\n            f'uri=\"{path}\", response=\"{respdig}\"'\n        )\n        if opaque:\n            base += f', opaque=\"{opaque}\"'\n        if algorithm:\n            base += f', algorithm=\"{algorithm}\"'\n        if entdig:\n            base += f', digest=\"{entdig}\"'\n        if qop:\n            base += f', qop=\"auth\", nc={ncvalue}, cnonce=\"{cnonce}\"'\n\n        return f\"Digest {base}\"\n\n    def handle_redirect(self, r, **kwargs):\n        \"\"\"Reset num_401_calls counter on redirects.\"\"\"\n        if r.is_redirect:\n            self._thread_local.num_401_calls = 1\n\n    def handle_401(self, r, **kwargs):\n        \"\"\"\n        Takes the given response and tries digest-auth, if needed.\n\n        :rtype: requests.Response\n        \"\"\"\n\n        # If response is not 4xx, do not auth\n        # See https://github.com/psf/requests/issues/3772\n        if not 400 <= r.status_code < 500:\n            self._thread_local.num_401_calls = 1\n            return r\n\n        if self._thread_local.pos is not None:\n            # Rewind the file position indicator of the body to where\n            # it was to resend the request.\n            r.request.body.seek(self._thread_local.pos)\n        s_auth = r.headers.get(\"www-authenticate\", \"\")\n\n        if \"digest\" in s_auth.lower() and self._thread_local.num_401_calls < 2:\n\n            self._thread_local.num_401_calls += 1\n            pat = re.compile(r\"digest \", flags=re.IGNORECASE)\n            self._thread_local.chal = parse_dict_header(pat.sub(\"\", s_auth, count=1))\n\n            # Consume content and release the original connection\n            # to allow our new request to reuse the same one.\n            r.content\n            r.close()\n            prep = r.request.copy()\n            extract_cookies_to_jar(prep._cookies, r.request, r.raw)\n            prep.prepare_cookies(prep._cookies)\n\n            prep.headers[\"Authorization\"] = self.build_digest_header(\n                prep.method, prep.url\n            )\n            _r = r.connection.send(prep, **kwargs)\n            _r.history.append(r)\n            _r.request = prep\n\n            return _r\n\n        self._thread_local.num_401_calls = 1\n        return r\n\n    def __call__(self, r):\n        # Initialize per-thread state, if needed\n        self.init_per_thread_state()\n        # If we have a saved nonce, skip the 401\n        if self._thread_local.last_nonce:\n            r.headers[\"Authorization\"] = self.build_digest_header(r.method, r.url)\n        try:\n            self._thread_local.pos = r.body.tell()\n        except AttributeError:\n            # In the case of HTTPDigestAuth being reused and the body of\n            # the previous request was a file-like object, pos has the\n            # file position of the previous body. Ensure it's set to\n            # None.\n            self._thread_local.pos = None\n        r.register_hook(\"response\", self.handle_401)\n        r.register_hook(\"response\", self.handle_redirect)\n        self._thread_local.num_401_calls = 1\n\n        return r\n\n    def __eq__(self, other):\n        return all(\n            [\n                self.username == getattr(other, \"username\", None),\n                self.password == getattr(other, \"password\", None),\n            ]\n        )\n\n    def __ne__(self, other):\n        return not self == other"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/resolvelib/providers.py",
      "line_number": 1,
      "details": "class AbstractProvider(object):\n    \"\"\"Delegate class to provide the required interface for the resolver.\"\"\"\n\n    def identify(self, requirement_or_candidate):\n        \"\"\"Given a requirement, return an identifier for it.\n\n        This is used to identify a requirement, e.g. whether two requirements\n        should have their specifier parts merged.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_preference(\n        self,\n        identifier,\n        resolutions,\n        candidates,\n        information,\n        backtrack_causes,\n    ):\n        \"\"\"Produce a sort key for given requirement based on preference.\n\n        The preference is defined as \"I think this requirement should be\n        resolved first\". The lower the return value is, the more preferred\n        this group of arguments is.\n\n        :param identifier: An identifier as returned by ``identify()``. This\n            identifies the dependency matches which should be returned.\n        :param resolutions: Mapping of candidates currently pinned by the\n            resolver. Each key is an identifier, and the value is a candidate.\n            The candidate may conflict with requirements from ``information``.\n        :param candidates: Mapping of each dependency's possible candidates.\n            Each value is an iterator of candidates.\n        :param information: Mapping of requirement information of each package.\n            Each value is an iterator of *requirement information*.\n        :param backtrack_causes: Sequence of requirement information that were\n            the requirements that caused the resolver to most recently backtrack.\n\n        A *requirement information* instance is a named tuple with two members:\n\n        * ``requirement`` specifies a requirement contributing to the current\n          list of candidates.\n        * ``parent`` specifies the candidate that provides (depended on) the\n          requirement, or ``None`` to indicate a root requirement.\n\n        The preference could depend on various issues, including (not\n        necessarily in this order):\n\n        * Is this package pinned in the current resolution result?\n        * How relaxed is the requirement? Stricter ones should probably be\n          worked on first? (I don't know, actually.)\n        * How many possibilities are there to satisfy this requirement? Those\n          with few left should likely be worked on first, I guess?\n        * Are there any known conflicts for this requirement? We should\n          probably work on those with the most known conflicts.\n\n        A sortable value should be returned (this will be used as the ``key``\n        parameter of the built-in sorting function). The smaller the value is,\n        the more preferred this requirement is (i.e. the sorting function\n        is called with ``reverse=False``).\n        \"\"\"\n        raise NotImplementedError\n\n    def find_matches(self, identifier, requirements, incompatibilities):\n        \"\"\"Find all possible candidates that satisfy the given constraints.\n\n        :param identifier: An identifier as returned by ``identify()``. This\n            identifies the dependency matches of which should be returned.\n        :param requirements: A mapping of requirements that all returned\n            candidates must satisfy. Each key is an identifier, and the value\n            an iterator of requirements for that dependency.\n        :param incompatibilities: A mapping of known incompatibilities of\n            each dependency. Each key is an identifier, and the value an\n            iterator of incompatibilities known to the resolver. All\n            incompatibilities *must* be excluded from the return value.\n\n        This should try to get candidates based on the requirements' types.\n        For VCS, local, and archive requirements, the one-and-only match is\n        returned, and for a \"named\" requirement, the index(es) should be\n        consulted to find concrete candidates for this requirement.\n\n        The return value should produce candidates ordered by preference; the\n        most preferred candidate should come first. The return type may be one\n        of the following:\n\n        * A callable that returns an iterator that yields candidates.\n        * An collection of candidates.\n        * An iterable of candidates. This will be consumed immediately into a\n          list of candidates.\n        \"\"\"\n        raise NotImplementedError\n\n    def is_satisfied_by(self, requirement, candidate):\n        \"\"\"Whether the given requirement can be satisfied by a candidate.\n\n        The candidate is guaranteed to have been generated from the\n        requirement.\n\n        A boolean should be returned to indicate whether ``candidate`` is a\n        viable solution to the requirement.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_dependencies(self, candidate):\n        \"\"\"Get dependencies of a candidate.\n\n        This should return a collection of requirements that `candidate`\n        specifies as its dependencies.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass AbstractResolver(object):\n    \"\"\"The thing that performs the actual resolution work.\"\"\"\n\n    base_exception = Exception\n\n    def __init__(self, provider, reporter):\n        self.provider = provider\n        self.reporter = reporter\n\n    def resolve(self, requirements, **kwargs):\n        \"\"\"Take a collection of constraints, spit out the resolution result.\n\n        This returns a representation of the final resolution state, with one\n        guarenteed attribute ``mapping`` that contains resolved candidates as\n        values. The keys are their respective identifiers.\n\n        :param requirements: A collection of constraints.\n        :param kwargs: Additional keyword arguments that subclasses may accept.\n\n        :raises: ``self.base_exception`` or its subclass.\n        \"\"\"\n        raise NotImplementedError"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/_inspect.py",
      "line_number": 5,
      "details": "\" in docs else \" \")\n                            doc = highlighter(docs)\n                            doc.stylize(\"inspect.doc\")\n                            _signature_text.append(doc)\n\n                    add_row(key_text, _signature_text)\n            else:\n                add_row(key_text, Pretty(value, highlighter=highlighter))\n        if items_table.row_count:\n            yield items_table\n        elif not_shown_count:\n            yield Text.from_markup(\n                f\"[b cyan]{not_shown_count}[/][i] attribute(s) not shown.[/i] \"\n                f\"Run [b][magenta]inspect[/]([not b]inspect[/])[/b] for options.\"\n            )\n\n    def _get_formatted_doc(self, object_: Any) -> Optional[str]:\n        \"\"\"\n        Extract the docstring of an object, process it and returns it.\n        The processing consists in cleaning up the doctring's indentation,\n        taking only its 1st paragraph if `self.help` is not True,\n        and escape its control codes.\n\n        Args:\n            object_ (Any): the object to get the docstring from.\n\n        Returns:\n            Optional[str]: the processed docstring, or None if no docstring was found.\n        \"\"\"\n        docs = getdoc(object_)\n        if docs is None:\n            return None\n        docs = cleandoc(docs).strip()\n        if not self.help:\n            docs = _first_paragraph(docs)\n        return escape_control_codes(docs)\n\n\ndef get_object_types_mro(obj: Union[object, Type[Any]]) -> Tuple[type, ...]:\n    \"\"\"Returns the MRO of an object's class, or of the object itself if it's a class.\"\"\"\n    if not hasattr(obj, \"__mro__\"):\n        # N.B. we cannot use `if type(obj) is type` here because it doesn't work with\n        # some types of classes, such as the ones that use abc.ABCMeta.\n        obj = type(obj)\n    return getattr(obj, \"__mro__\", ())\n\n\ndef get_object_types_mro_as_strings(obj: object) -> Collection[str]:\n    \"\"\"\n    Returns the MRO of an object's class as full qualified names, or of the object itself if it's a class.\n\n    Examples:\n        `object_types_mro_as_strings(JSONDecoder)` will return `['json.decoder.JSONDecoder', 'builtins.object']`\n    \"\"\"\n    return [\n        f'{getattr(type_, \"__module__\", \"\")}.{getattr(type_, \"__qualname__\", \"\")}'\n        for type_ in get_object_types_mro(obj)\n    ]\n\n\ndef is_object_one_of_types(\n    obj: object, fully_qualified_types_names: Collection[str]\n) -> bool:\n    \"\"\"\n    Returns `True` if the given object's class (or the object itself, if it's a class) has one of the\n    fully qualified names in its MRO.\n    \"\"\"\n    for type_name in get_object_types_mro_as_strings(obj):\n        if type_name in fully_qualified_types_names:\n            return True\n    return False"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/color.py",
      "line_number": 1,
      "details": "import platform\nimport re\nfrom colorsys import rgb_to_hls\nfrom enum import IntEnum\nfrom functools import lru_cache\nfrom typing import TYPE_CHECKING, NamedTuple, Optional, Tuple\n\nfrom ._palettes import EIGHT_BIT_PALETTE, STANDARD_PALETTE, WINDOWS_PALETTE\nfrom .color_triplet import ColorTriplet\nfrom .repr import Result, rich_repr\nfrom .terminal_theme import DEFAULT_TERMINAL_THEME\n\nif TYPE_CHECKING:  # pragma: no cover\n    from .terminal_theme import TerminalTheme\n    from .text import Text\n\n\nWINDOWS = platform.system() == \"Windows\"\n\n\nclass ColorSystem(IntEnum):\n    \"\"\"One of the 3 color system supported by terminals.\"\"\"\n\n    STANDARD = 1\n    EIGHT_BIT = 2\n    TRUECOLOR = 3\n    WINDOWS = 4\n\n    def __repr__(self) -> str:\n        return f\"ColorSystem.{self.name}\"\n\n    def __str__(self) -> str:\n        return repr(self)\n\n\nclass ColorType(IntEnum):\n    \"\"\"Type of color stored in Color class.\"\"\"\n\n    DEFAULT = 0\n    STANDARD = 1\n    EIGHT_BIT = 2\n    TRUECOLOR = 3\n    WINDOWS = 4\n\n    def __repr__(self) -> str:\n        return f\"ColorType.{self.name}\"\n\n\nANSI_COLOR_NAMES = {\n    \"black\": 0,\n    \"red\": 1,\n    \"green\": 2,\n    \"yellow\": 3,\n    \"blue\": 4,\n    \"magenta\": 5,\n    \"cyan\": 6,\n    \"white\": 7,\n    \"bright_black\": 8,\n    \"bright_red\": 9,\n    \"bright_green\": 10,\n    \"bright_yellow\": 11,\n    \"bright_blue\": 12,\n    \"bright_magenta\": 13,\n    \"bright_cyan\": 14,\n    \"bright_white\": 15,\n    \"grey0\": 16,\n    \"gray0\": 16,\n    \"navy_blue\": 17,\n    \"dark_blue\": 18,\n    \"blue3\": 20,\n    \"blue1\": 21,\n    \"dark_green\": 22,\n    \"deep_sky_blue4\": 25,\n    \"dodger_blue3\": 26,\n    \"dodger_blue2\": 27,\n    \"green4\": 28,\n    \"spring_green4\": 29,\n    \"turquoise4\": 30,\n    \"deep_sky_blue3\": 32,\n    \"dodger_blue1\": 33,\n    \"green3\": 40,\n    \"spring_green3\": 41,\n    \"dark_cyan\": 36,\n    \"light_sea_green\": 37,\n    \"deep_sky_blue2\": 38,\n    \"deep_sky_blue1\": 39,\n    \"spring_green2\": 47,\n    \"cyan3\": 43,\n    \"dark_turquoise\": 44,\n    \"turquoise2\": 45,\n    \"green1\": 46,\n    \"spring_green1\": 48,\n    \"medium_spring_green\": 49,\n    \"cyan2\": 50,\n    \"cyan1\": 51,\n    \"dark_red\": 88,\n    \"deep_pink4\": 125,\n    \"purple4\": 55,\n    \"purple3\": 56,\n    \"blue_violet\": 57,\n    \"orange4\": 94,\n    \"grey37\": 59,\n    \"gray37\": 59,\n    \"medium_purple4\": 60,\n    \"slate_blue3\": 62,\n    \"royal_blue1\": 63,\n    \"chartreuse4\": 64,\n    \"dark_sea_green4\": 71,\n    \"pale_turquoise4\": 66,\n    \"steel_blue\": 67,\n    \"steel_blue3\": 68,\n    \"cornflower_blue\": 69,\n    \"chartreuse3\": 76,\n    \"cadet_blue\": 73,\n    \"sky_blue3\": 74,\n    \"steel_blue1\": 81,\n    \"pale_green3\": 114,\n    \"sea_green3\": 78,\n    \"aquamarine3\": 79,\n    \"medium_turquoise\": 80,\n    \"chartreuse2\": 112,\n    \"sea_green2\": 83,\n    \"sea_green1\": 85,\n    \"aquamarine1\": 122,\n    \"dark_slate_gray2\": 87,\n    \"dark_magenta\": 91,\n    \"dark_violet\": 128,\n    \"purple\": 129,\n    \"light_pink4\": 95,\n    \"plum4\": 96,\n    \"medium_purple3\": 98,\n    \"slate_blue1\": 99,\n    \"yellow4\": 106,\n    \"wheat4\": 101,\n    \"grey53\": 102,\n    \"gray53\": 102,\n    \"light_slate_grey\": 103,\n    \"light_slate_gray\": 103,\n    \"medium_purple\": 104,\n    \"light_slate_blue\": 105,\n    \"dark_olive_green3\": 149,\n    \"dark_sea_green\": 108,\n    \"light_sky_blue3\": 110,\n    \"sky_blue2\": 111,\n    \"dark_sea_green3\": 150,\n    \"dark_slate_gray3\": 116,\n    \"sky_blue1\": 117,\n    \"chartreuse1\": 118,\n    \"light_green\": 120,\n    \"pale_green1\": 156,\n    \"dark_slate_gray1\": 123,\n    \"red3\": 160,\n    \"medium_violet_red\": 126,\n    \"magenta3\": 164,\n    \"dark_orange3\": 166,\n    \"indian_red\": 167,\n    \"hot_pink3\": 168,\n    \"medium_orchid3\": 133,\n    \"medium_orchid\": 134,\n    \"medium_purple2\": 140,\n    \"dark_goldenrod\": 136,\n    \"light_salmon3\": 173,\n    \"rosy_brown\": 138,\n    \"grey63\": 139,\n    \"gray63\": 139,\n    \"medium_purple1\": 141,\n    \"gold3\": 178,\n    \"dark_khaki\": 143,\n    \"navajo_white3\": 144,\n    \"grey69\": 145,\n    \"gray69\": 145,\n    \"light_steel_blue3\": 146,\n    \"light_steel_blue\": 147,\n    \"yellow3\": 184,\n    \"dark_sea_green2\": 157,\n    \"light_cyan3\": 152,\n    \"light_sky_blue1\": 153,\n    \"green_yellow\": 154,\n    \"dark_olive_green2\": 155,\n    \"dark_sea_green1\": 193,\n    \"pale_turquoise1\": 159,\n    \"deep_pink3\": 162,\n    \"magenta2\": 200,\n    \"hot_pink2\": 169,\n    \"orchid\": 170,\n    \"medium_orchid1\": 207,\n    \"orange3\": 172,\n    \"light_pink3\": 174,\n    \"pink3\": 175,\n    \"plum3\": 176,\n    \"violet\": 177,\n    \"light_goldenrod3\": 179,\n    \"tan\": 180,\n    \"misty_rose3\": 181,\n    \"thistle3\": 182,\n    \"plum2\": 183,\n    \"khaki3\": 185,\n    \"light_goldenrod2\": 222,\n    \"light_yellow3\": 187,\n    \"grey84\": 188,\n    \"gray84\": 188,\n    \"light_steel_blue1\": 189,\n    \"yellow2\": 190,\n    \"dark_olive_green1\": 192,\n    \"honeydew2\": 194,\n    \"light_cyan1\": 195,\n    \"red1\": 196,\n    \"deep_pink2\": 197,\n    \"deep_pink1\": 199,\n    \"magenta1\": 201,\n    \"orange_red1\": 202,\n    \"indian_red1\": 204,\n    \"hot_pink\": 206,\n    \"dark_orange\": 208,\n    \"salmon1\": 209,\n    \"light_coral\": 210,\n    \"pale_violet_red1\": 211,\n    \"orchid2\": 212,\n    \"orchid1\": 213,\n    \"orange1\": 214,\n    \"sandy_brown\": 215,\n    \"light_salmon1\": 216,\n    \"light_pink1\": 217,\n    \"pink1\": 218,\n    \"plum1\": 219,\n    \"gold1\": 220,\n    \"navajo_white1\": 223,\n    \"misty_rose1\": 224,\n    \"thistle1\": 225,\n    \"yellow1\": 226,\n    \"light_goldenrod1\": 227,\n    \"khaki1\": 228,\n    \"wheat1\": 229,\n    \"cornsilk1\": 230,\n    \"grey100\": 231,\n    \"gray100\": 231,\n    \"grey3\": 232,\n    \"gray3\": 232,\n    \"grey7\": 233,\n    \"gray7\": 233,\n    \"grey11\": 234,\n    \"gray11\": 234,\n    \"grey15\": 235,\n    \"gray15\": 235,\n    \"grey19\": 236,\n    \"gray19\": 236,\n    \"grey23\": 237,\n    \"gray23\": 237,\n    \"grey27\": 238,\n    \"gray27\": 238,\n    \"grey30\": 239,\n    \"gray30\": 239,\n    \"grey35\": 240,\n    \"gray35\": 240,\n    \"grey39\": 241,\n    \"gray39\": 241,\n    \"grey42\": 242,\n    \"gray42\": 242,\n    \"grey46\": 243,\n    \"gray46\": 243,\n    \"grey50\": 244,\n    \"gray50\": 244,\n    \"grey54\": 245,\n    \"gray54\": 245,\n    \"grey58\": 246,\n    \"gray58\": 246,\n    \"grey62\": 247,\n    \"gray62\": 247,\n    \"grey66\": 248,\n    \"gray66\": 248,\n    \"grey70\": 249,\n    \"gray70\": 249,\n    \"grey74\": 250,\n    \"gray74\": 250,\n    \"grey78\": 251,\n    \"gray78\": 251,\n    \"grey82\": 252,\n    \"gray82\": 252,\n    \"grey85\": 253,\n    \"gray85\": 253,\n    \"grey89\": 254,\n    \"gray89\": 254,\n    \"grey93\": 255,\n    \"gray93\": 255,\n}\n\n\nclass ColorParseError(Exception):\n    \"\"\"The color could not be parsed.\"\"\"\n\n\nRE_COLOR = re.compile(\n    r\"\"\"^\n\\#([0-9a-f]{6})$|\ncolor\\(([0-9]{1,3})\\)$|\nrgb\\(([\\d\\s,]+)\\)$\n\"\"\",\n    re.VERBOSE,\n)\n\n\n@rich_repr\nclass Color(NamedTuple):\n    \"\"\"Terminal color definition.\"\"\"\n\n    name: str\n    \"\"\"The name of the color (typically the input to Color.parse).\"\"\"\n    type: ColorType\n    \"\"\"The type of the color.\"\"\"\n    number: Optional[int] = None\n    \"\"\"The color number, if a standard color, or None.\"\"\"\n    triplet: Optional[ColorTriplet] = None\n    \"\"\"A triplet of color components, if an RGB color.\"\"\"\n\n    def __rich__(self) -> \"Text\":\n        \"\"\"Displays the actual color if Rich printed.\"\"\"\n        from .style import Style\n        from .text import Text\n\n        return Text.assemble(\n            f\"<color {self.name!r} ({self.type.name.lower()})\",\n            (\"\u2b24\", Style(color=self)),\n            \" >\",\n        )\n\n    def __rich_repr__(self) -> Result:\n        yield self.name\n        yield self.type\n        yield \"number\", self.number, None\n        yield \"triplet\", self.triplet, None\n\n    @property\n    def system(self) -> ColorSystem:\n        \"\"\"Get the native color system for this color.\"\"\"\n        if self.type == ColorType.DEFAULT:\n            return ColorSystem.STANDARD\n        return ColorSystem(int(self.type))\n\n    @property\n    def is_system_defined(self) -> bool:\n        \"\"\"Check if the color is ultimately defined by the system.\"\"\"\n        return self.system not in (ColorSystem.EIGHT_BIT, ColorSystem.TRUECOLOR)\n\n    @property\n    def is_default(self) -> bool:\n        \"\"\"Check if the color is a default color.\"\"\"\n        return self.type == ColorType.DEFAULT\n\n    def get_truecolor(\n        self, theme: Optional[\"TerminalTheme\"] = None, foreground: bool = True\n    ) -> ColorTriplet:\n        \"\"\"Get an equivalent color triplet for this color.\n\n        Args:\n            theme (TerminalTheme, optional): Optional terminal theme, or None to use default. Defaults to None.\n            foreground (bool, optional): True for a foreground color, or False for background. Defaults to True.\n\n        Returns:\n            ColorTriplet: A color triplet containing RGB components.\n        \"\"\"\n\n        if theme is None:\n            theme = DEFAULT_TERMINAL_THEME\n        if self.type == ColorType.TRUECOLOR:\n            assert self.triplet is not None\n            return self.triplet\n        elif self.type == ColorType.EIGHT_BIT:\n            assert self.number is not None\n            return EIGHT_BIT_PALETTE[self.number]\n        elif self.type == ColorType.STANDARD:\n            assert self.number is not None\n            return theme.ansi_colors[self.number]\n        elif self.type == ColorType.WINDOWS:\n            assert self.number is not None\n            return WINDOWS_PALETTE[self.number]\n        else:  # self.type == ColorType.DEFAULT:\n            assert self.number is None\n            return theme.foreground_color if foreground else theme.background_color\n\n    @classmethod\n    def from_ansi(cls, number: int) -> \"Color\":\n        \"\"\"Create a Color number from it's 8-bit ansi number.\n\n        Args:\n            number (int): A number between 0-255 inclusive.\n\n        Returns:\n            Color: A new Color instance.\n        \"\"\"\n        return cls(\n            name=f\"color({number})\",\n            type=(ColorType.STANDARD if number < 16 else ColorType.EIGHT_BIT),\n            number=number,\n        )\n\n    @classmethod\n    def from_triplet(cls, triplet: \"ColorTriplet\") -> \"Color\":\n        \"\"\"Create a truecolor RGB color from a triplet of values.\n\n        Args:\n            triplet (ColorTriplet): A color triplet containing red, green and blue components.\n\n        Returns:\n            Color: A new color object.\n        \"\"\"\n        return cls(name=triplet.hex, type=ColorType.TRUECOLOR, triplet=triplet)\n\n    @classmethod\n    def from_rgb(cls, red: float, green: float, blue: float) -> \"Color\":\n        \"\"\"Create a truecolor from three color components in the range(0->255).\n\n        Args:\n            red (float): Red component in range 0-255.\n            green (float): Green component in range 0-255.\n            blue (float): Blue component in range 0-255.\n\n        Returns:\n            Color: A new color object.\n        \"\"\"\n        return cls.from_triplet(ColorTriplet(int(red), int(green), int(blue)))\n\n    @classmethod\n    def default(cls) -> \"Color\":\n        \"\"\"Get a Color instance representing the default color.\n\n        Returns:\n            Color: Default color.\n        \"\"\"\n        return cls(name=\"default\", type=ColorType.DEFAULT)\n\n    @classmethod\n    @lru_cache(maxsize=1024)\n    def parse(cls, color: str) -> \"Color\":\n        \"\"\"Parse a color definition.\"\"\"\n        original_color = color\n        color = color.lower().strip()\n\n        if color == \"default\":\n            return cls(color, type=ColorType.DEFAULT)\n\n        color_number = ANSI_COLOR_NAMES.get(color)\n        if color_number is not None:\n            return cls(\n                color,\n                type=(ColorType.STANDARD if color_number < 16 else ColorType.EIGHT_BIT),\n                number=color_number,\n            )\n\n        color_match = RE_COLOR.match(color)\n        if color_match is None:\n            raise ColorParseError(f\"{original_color!r} is not a valid color\")\n\n        color_24, color_8, color_rgb = color_match.groups()\n        if color_24:\n            triplet = ColorTriplet(\n                int(color_24[0:2], 16), int(color_24[2:4], 16), int(color_24[4:6], 16)\n            )\n            return cls(color, ColorType.TRUECOLOR, triplet=triplet)\n\n        elif color_8:\n            number = int(color_8)\n            if number > 255:\n                raise ColorParseError(f\"color number must be <= 255 in {color!r}\")\n            return cls(\n                color,\n                type=(ColorType.STANDARD if number < 16 else ColorType.EIGHT_BIT),\n                number=number,\n            )\n\n        else:  #  color_rgb:\n            components = color_rgb.split(\",\")\n            if len(components) != 3:\n                raise ColorParseError(\n                    f\"expected three components in {original_color!r}\"\n                )\n            red, green, blue = components\n            triplet = ColorTriplet(int(red), int(green), int(blue))\n            if not all(component <= 255 for component in triplet):\n                raise ColorParseError(\n                    f\"color components must be <= 255 in {original_color!r}\"\n                )\n            return cls(color, ColorType.TRUECOLOR, triplet=triplet)\n\n    @lru_cache(maxsize=1024)\n    def get_ansi_codes(self, foreground: bool = True) -> Tuple[str, ...]:\n        \"\"\"Get the ANSI escape codes for this color.\"\"\"\n        _type = self.type\n        if _type == ColorType.DEFAULT:\n            return (\"39\" if foreground else \"49\",)\n\n        elif _type == ColorType.WINDOWS:\n            number = self.number\n            assert number is not None\n            fore, back = (30, 40) if number < 8 else (82, 92)\n            return (str(fore + number if foreground else back + number),)\n\n        elif _type == ColorType.STANDARD:\n            number = self.number\n            assert number is not None\n            fore, back = (30, 40) if number < 8 else (82, 92)\n            return (str(fore + number if foreground else back + number),)\n\n        elif _type == ColorType.EIGHT_BIT:\n            assert self.number is not None\n            return (\"38\" if foreground else \"48\", \"5\", str(self.number))\n\n        else:  # self.standard == ColorStandard.TRUECOLOR:\n            assert self.triplet is not None\n            red, green, blue = self.triplet\n            return (\"38\" if foreground else \"48\", \"2\", str(red), str(green), str(blue))\n\n    @lru_cache(maxsize=1024)\n    def downgrade(self, system: ColorSystem) -> \"Color\":\n        \"\"\"Downgrade a color system to a system with fewer colors.\"\"\"\n\n        if self.type in (ColorType.DEFAULT, system):\n            return self\n        # Convert to 8-bit color from truecolor color\n        if system == ColorSystem.EIGHT_BIT and self.system == ColorSystem.TRUECOLOR:\n            assert self.triplet is not None\n            _h, l, s = rgb_to_hls(*self.triplet.normalized)\n            # If saturation is under 15% assume it is grayscale\n            if s < 0.15:\n                gray = round(l * 25.0)\n                if gray == 0:\n                    color_number = 16\n                elif gray == 25:\n                    color_number = 231\n                else:\n                    color_number = 231 + gray\n                return Color(self.name, ColorType.EIGHT_BIT, number=color_number)\n\n            red, green, blue = self.triplet\n            six_red = red / 95 if red < 95 else 1 + (red - 95) / 40\n            six_green = green / 95 if green < 95 else 1 + (green - 95) / 40\n            six_blue = blue / 95 if blue < 95 else 1 + (blue - 95) / 40\n\n            color_number = (\n                16 + 36 * round(six_red) + 6 * round(six_green) + round(six_blue)\n            )\n            return Color(self.name, ColorType.EIGHT_BIT, number=color_number)\n\n        # Convert to standard from truecolor or 8-bit\n        elif system == ColorSystem.STANDARD:\n            if self.system == ColorSystem.TRUECOLOR:\n                assert self.triplet is not None\n                triplet = self.triplet\n            else:  # self.system == ColorSystem.EIGHT_BIT\n                assert self.number is not None\n                triplet = ColorTriplet(*EIGHT_BIT_PALETTE[self.number])\n\n            color_number = STANDARD_PALETTE.match(triplet)\n            return Color(self.name, ColorType.STANDARD, number=color_number)\n\n        elif system == ColorSystem.WINDOWS:\n            if self.system == ColorSystem.TRUECOLOR:\n                assert self.triplet is not None\n                triplet = self.triplet\n            else:  # self.system == ColorSystem.EIGHT_BIT\n                assert self.number is not None\n                if self.number < 16:\n                    return Color(self.name, ColorType.WINDOWS, number=self.number)\n                triplet = ColorTriplet(*EIGHT_BIT_PALETTE[self.number])\n\n            color_number = WINDOWS_PALETTE.match(triplet)\n            return Color(self.name, ColorType.WINDOWS, number=color_number)\n\n        return self\n\n\ndef parse_rgb_hex(hex_color: str) -> ColorTriplet:\n    \"\"\"Parse six hex characters in to RGB triplet.\"\"\"\n    assert len(hex_color) == 6, \"must be 6 characters\"\n    color = ColorTriplet(\n        int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)\n    )\n    return color\n\n\ndef blend_rgb(\n    color1: ColorTriplet, color2: ColorTriplet, cross_fade: float = 0.5\n) -> ColorTriplet:\n    \"\"\"Blend one RGB color in to another.\"\"\"\n    r1, g1, b1 = color1\n    r2, g2, b2 = color2\n    new_color = ColorTriplet(\n        int(r1 + (r2 - r1) * cross_fade),\n        int(g1 + (g2 - g1) * cross_fade),\n        int(b1 + (b2 - b1) * cross_fade),\n    )\n    return new_color\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n\n    from .console import Console\n    from .table import Table\n    from .text import Text\n\n    console = Console()\n\n    table = Table(show_footer=False, show_edge=True)\n    table.add_column(\"Color\", width=10, overflow=\"ellipsis\")\n    table.add_column(\"Number\", justify=\"right\", style=\"yellow\")\n    table.add_column(\"Name\", style=\"green\")\n    table.add_column(\"Hex\", style=\"blue\")\n    table.add_column(\"RGB\", style=\"magenta\")\n\n    colors = sorted((v, k) for k, v in ANSI_COLOR_NAMES.items())\n    for color_number, name in colors:\n        if \"grey\" in name:\n            continue\n        color_cell = Text(\" \" * 10, style=f\"on {name}\")\n        if color_number < 16:\n            table.add_row(color_cell, f\"{color_number}\", Text(f'\"{name}\"'))\n        else:\n            color = EIGHT_BIT_PALETTE[color_number]  # type: ignore[has-type]\n            table.add_row(\n                color_cell, str(color_number), Text(f'\"{name}\"'), color.hex, color.rgb\n            )\n\n    console.print(table)"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/console.py",
      "line_number": 4,
      "details": "\")]\n                        if new_lines\n                        else [Segment(\" \" * render_options.max_width, style)]\n                    ]\n                    lines.extend(pad_line * extra_lines)\n\n            return lines\n\n    def render_str(\n        self,\n        text: str,\n        *,\n        style: Union[str, Style] = \"\",\n        justify: Optional[JustifyMethod] = None,\n        overflow: Optional[OverflowMethod] = None,\n        emoji: Optional[bool] = None,\n        markup: Optional[bool] = None,\n        highlight: Optional[bool] = None,\n        highlighter: Optional[HighlighterType] = None,\n    ) -> \"Text\":\n        \"\"\"Convert a string to a Text instance. This is called automatically if\n        you print or log a string.\n\n        Args:\n            text (str): Text to render.\n            style (Union[str, Style], optional): Style to apply to rendered text.\n            justify (str, optional): Justify method: \"default\", \"left\", \"center\", \"full\", or \"right\". Defaults to ``None``.\n            overflow (str, optional): Overflow method: \"crop\", \"fold\", or \"ellipsis\". Defaults to ``None``.\n            emoji (Optional[bool], optional): Enable emoji, or ``None`` to use Console default.\n            markup (Optional[bool], optional): Enable markup, or ``None`` to use Console default.\n            highlight (Optional[bool], optional): Enable highlighting, or ``None`` to use Console default.\n            highlighter (HighlighterType, optional): Optional highlighter to apply.\n        Returns:\n            ConsoleRenderable: Renderable object.\n\n        \"\"\"\n        emoji_enabled = emoji or (emoji is None and self._emoji)\n        markup_enabled = markup or (markup is None and self._markup)\n        highlight_enabled = highlight or (highlight is None and self._highlight)\n\n        if markup_enabled:\n            rich_text = render_markup(\n                text,\n                style=style,\n                emoji=emoji_enabled,\n                emoji_variant=self._emoji_variant,\n            )\n            rich_text.justify = justify\n            rich_text.overflow = overflow\n        else:\n            rich_text = Text(\n                _emoji_replace(text, default_variant=self._emoji_variant)\n                if emoji_enabled\n                else text,\n                justify=justify,\n                overflow=overflow,\n                style=style,\n            )\n\n        _highlighter = (highlighter or self.highlighter) if highlight_enabled else None\n        if _highlighter is not None:\n            highlight_text = _highlighter(str(rich_text))\n            highlight_text.copy_styles(rich_text)\n            return highlight_text\n\n        return rich_text\n\n    def get_style(\n        self, name: Union[str, Style], *, default: Optional[Union[Style, str]] = None\n    ) -> Style:\n        \"\"\"Get a Style instance by its theme name or parse a definition.\n\n        Args:\n            name (str): The name of a style or a style definition.\n\n        Returns:\n            Style: A Style object.\n\n        Raises:\n            MissingStyle: If no style could be parsed from name.\n\n        \"\"\"\n        if isinstance(name, Style):\n            return name\n\n        try:\n            style = self._theme_stack.get(name)\n            if style is None:\n                style = Style.parse(name)\n            return style.copy() if style.link else style\n        except errors.StyleSyntaxError as error:\n            if default is not None:\n                return self.get_style(default)\n            raise errors.MissingStyle(\n                f\"Failed to get style {name!r}; {error}\"\n            ) from None\n\n    def _collect_renderables(\n        self,\n        objects: Iterable[Any],\n        sep: str,\n        end: str,\n        *,\n        justify: Optional[JustifyMethod] = None,\n        emoji: Optional[bool] = None,\n        markup: Optional[bool] = None,\n        highlight: Optional[bool] = None,\n    ) -> List[ConsoleRenderable]:\n        \"\"\"Combine a number of renderables and text into one renderable.\n\n        Args:\n            objects (Iterable[Any]): Anything that Rich can render.\n            sep (str): String to write between print data.\n            end (str): String to write at end of print data.\n            justify (str, optional): One of \"left\", \"right\", \"center\", or \"full\". Defaults to ``None``.\n            emoji (Optional[bool], optional): Enable emoji code, or ``None`` to use console default.\n            markup (Optional[bool], optional): Enable markup, or ``None`` to use console default.\n            highlight (Optional[bool], optional): Enable automatic highlighting, or ``None`` to use console default.\n\n        Returns:\n            List[ConsoleRenderable]: A list of things to render.\n        \"\"\"\n        renderables: List[ConsoleRenderable] = []\n        _append = renderables.append\n        text: List[Text] = []\n        append_text = text.append\n\n        append = _append\n        if justify in (\"left\", \"center\", \"right\"):\n\n            def align_append(renderable: RenderableType) -> None:\n                _append(Align(renderable, cast(AlignMethod, justify)))\n\n            append = align_append\n\n        _highlighter: HighlighterType = _null_highlighter\n        if highlight or (highlight is None and self._highlight):\n            _highlighter = self.highlighter\n\n        def check_text() -> None:\n            if text:\n                sep_text = Text(sep, justify=justify, end=end)\n                append(sep_text.join(text))\n                text.clear()\n\n        for renderable in objects:\n            renderable = rich_cast(renderable)\n            if isinstance(renderable, str):\n                append_text(\n                    self.render_str(\n                        renderable, emoji=emoji, markup=markup, highlighter=_highlighter\n                    )\n                )\n            elif isinstance(renderable, Text):\n                append_text(renderable)\n            elif isinstance(renderable, ConsoleRenderable):\n                check_text()\n                append(renderable)\n            elif is_expandable(renderable):\n                check_text()\n                append(Pretty(renderable, highlighter=_highlighter))\n            else:\n                append_text(_highlighter(str(renderable)))\n\n        check_text()\n\n        if self.style is not None:\n            style = self.get_style(self.style)\n            renderables = [Styled(renderable, style) for renderable in renderables]\n\n        return renderables\n\n    def rule(\n        self,\n        title: TextType = \"\",\n        *,\n        characters: str = \"\u2500\",\n        style: Union[str, Style] = \"rule.line\",\n        align: AlignMethod = \"center\",\n    ) -> None:\n        \"\"\"Draw a line with optional centered title.\n\n        Args:\n            title (str, optional): Text to render over the rule. Defaults to \"\".\n            characters (str, optional): Character(s) to form the line. Defaults to \"\u2500\".\n            style (str, optional): Style of line. Defaults to \"rule.line\".\n            align (str, optional): How to align the title, one of \"left\", \"center\", or \"right\". Defaults to \"center\".\n        \"\"\"\n        from .rule import Rule\n\n        rule = Rule(title=title, characters=characters, style=style, align=align)\n        self.print(rule)\n\n    def control(self, *control: Control) -> None:\n        \"\"\"Insert non-printing control codes.\n\n        Args:\n            control_codes (str): Control codes, such as those that may move the cursor.\n        \"\"\"\n        if not self.is_dumb_terminal:\n            with self:\n                self._buffer.extend(_control.segment for _control in control)\n\n    def out(\n        self,\n        *objects: Any,\n        sep: str = \" \",\n        end: str = \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/control.py",
      "line_number": 1,
      "details": "import sys\nimport time\nfrom typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Union\n\nif sys.version_info >= (3, 8):\n    from typing import Final\nelse:\n    from pip._vendor.typing_extensions import Final  # pragma: no cover\n\nfrom .segment import ControlCode, ControlType, Segment\n\nif TYPE_CHECKING:\n    from .console import Console, ConsoleOptions, RenderResult\n\nSTRIP_CONTROL_CODES: Final = [\n    7,  # Bell\n    8,  # Backspace\n    11,  # Vertical tab\n    12,  # Form feed\n    13,  # Carriage return\n]\n_CONTROL_STRIP_TRANSLATE: Final = {\n    _codepoint: None for _codepoint in STRIP_CONTROL_CODES\n}\n\nCONTROL_ESCAPE: Final = {\n    7: \"\\\\a\",\n    8: \"\\\\b\",\n    11: \"\\\\v\",\n    12: \"\\\\f\",\n    13: \"\\\\r\",\n}\n\nCONTROL_CODES_FORMAT: Dict[int, Callable[..., str]] = {\n    ControlType.BELL: lambda: \"\\x07\",\n    ControlType.CARRIAGE_RETURN: lambda: \"\\r\",\n    ControlType.HOME: lambda: \"\\x1b[H\",\n    ControlType.CLEAR: lambda: \"\\x1b[2J\",\n    ControlType.ENABLE_ALT_SCREEN: lambda: \"\\x1b[?1049h\",\n    ControlType.DISABLE_ALT_SCREEN: lambda: \"\\x1b[?1049l\",\n    ControlType.SHOW_CURSOR: lambda: \"\\x1b[?25h\",\n    ControlType.HIDE_CURSOR: lambda: \"\\x1b[?25l\",\n    ControlType.CURSOR_UP: lambda param: f\"\\x1b[{param}A\",\n    ControlType.CURSOR_DOWN: lambda param: f\"\\x1b[{param}B\",\n    ControlType.CURSOR_FORWARD: lambda param: f\"\\x1b[{param}C\",\n    ControlType.CURSOR_BACKWARD: lambda param: f\"\\x1b[{param}D\",\n    ControlType.CURSOR_MOVE_TO_COLUMN: lambda param: f\"\\x1b[{param+1}G\",\n    ControlType.ERASE_IN_LINE: lambda param: f\"\\x1b[{param}K\",\n    ControlType.CURSOR_MOVE_TO: lambda x, y: f\"\\x1b[{y+1};{x+1}H\",\n    ControlType.SET_WINDOW_TITLE: lambda title: f\"\\x1b]0;{title}\\x07\",\n}\n\n\nclass Control:\n    \"\"\"A renderable that inserts a control code (non printable but may move cursor).\n\n    Args:\n        *codes (str): Positional arguments are either a :class:`~rich.segment.ControlType` enum or a\n            tuple of ControlType and an integer parameter\n    \"\"\"\n\n    __slots__ = [\"segment\"]\n\n    def __init__(self, *codes: Union[ControlType, ControlCode]) -> None:\n        control_codes: List[ControlCode] = [\n            (code,) if isinstance(code, ControlType) else code for code in codes\n        ]\n        _format_map = CONTROL_CODES_FORMAT\n        rendered_codes = \"\".join(\n            _format_map[code](*parameters) for code, *parameters in control_codes\n        )\n        self.segment = Segment(rendered_codes, None, control_codes)\n\n    @classmethod\n    def bell(cls) -> \"Control\":\n        \"\"\"Ring the 'bell'.\"\"\"\n        return cls(ControlType.BELL)\n\n    @classmethod\n    def home(cls) -> \"Control\":\n        \"\"\"Move cursor to 'home' position.\"\"\"\n        return cls(ControlType.HOME)\n\n    @classmethod\n    def move(cls, x: int = 0, y: int = 0) -> \"Control\":\n        \"\"\"Move cursor relative to current position.\n\n        Args:\n            x (int): X offset.\n            y (int): Y offset.\n\n        Returns:\n            ~Control: Control object.\n\n        \"\"\"\n\n        def get_codes() -> Iterable[ControlCode]:\n            control = ControlType\n            if x:\n                yield (\n                    control.CURSOR_FORWARD if x > 0 else control.CURSOR_BACKWARD,\n                    abs(x),\n                )\n            if y:\n                yield (\n                    control.CURSOR_DOWN if y > 0 else control.CURSOR_UP,\n                    abs(y),\n                )\n\n        control = cls(*get_codes())\n        return control\n\n    @classmethod\n    def move_to_column(cls, x: int, y: int = 0) -> \"Control\":\n        \"\"\"Move to the given column, optionally add offset to row.\n\n        Returns:\n            x (int): absolute x (column)\n            y (int): optional y offset (row)\n\n        Returns:\n            ~Control: Control object.\n        \"\"\"\n\n        return (\n            cls(\n                (ControlType.CURSOR_MOVE_TO_COLUMN, x),\n                (\n                    ControlType.CURSOR_DOWN if y > 0 else ControlType.CURSOR_UP,\n                    abs(y),\n                ),\n            )\n            if y\n            else cls((ControlType.CURSOR_MOVE_TO_COLUMN, x))\n        )\n\n    @classmethod\n    def move_to(cls, x: int, y: int) -> \"Control\":\n        \"\"\"Move cursor to absolute position.\n\n        Args:\n            x (int): x offset (column)\n            y (int): y offset (row)\n\n        Returns:\n            ~Control: Control object.\n        \"\"\"\n        return cls((ControlType.CURSOR_MOVE_TO, x, y))\n\n    @classmethod\n    def clear(cls) -> \"Control\":\n        \"\"\"Clear the screen.\"\"\"\n        return cls(ControlType.CLEAR)\n\n    @classmethod\n    def show_cursor(cls, show: bool) -> \"Control\":\n        \"\"\"Show or hide the cursor.\"\"\"\n        return cls(ControlType.SHOW_CURSOR if show else ControlType.HIDE_CURSOR)\n\n    @classmethod\n    def alt_screen(cls, enable: bool) -> \"Control\":\n        \"\"\"Enable or disable alt screen.\"\"\"\n        if enable:\n            return cls(ControlType.ENABLE_ALT_SCREEN, ControlType.HOME)\n        else:\n            return cls(ControlType.DISABLE_ALT_SCREEN)\n\n    @classmethod\n    def title(cls, title: str) -> \"Control\":\n        \"\"\"Set the terminal window title\n\n        Args:\n            title (str): The new terminal window title\n        \"\"\"\n        return cls((ControlType.SET_WINDOW_TITLE, title))\n\n    def __str__(self) -> str:\n        return self.segment.text\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RenderResult\":\n        if self.segment.text:\n            yield self.segment\n\n\ndef strip_control_codes(\n    text: str, _translate_table: Dict[int, None] = _CONTROL_STRIP_TRANSLATE\n) -> str:\n    \"\"\"Remove control codes from text.\n\n    Args:\n        text (str): A string possibly contain control codes.\n\n    Returns:\n        str: String with control codes removed.\n    \"\"\"\n    return text.translate(_translate_table)\n\n\ndef escape_control_codes(\n    text: str,\n    _translate_table: Dict[int, str] = CONTROL_ESCAPE,\n) -> str:\n    \"\"\"Replace control codes with their \"escaped\" equivalent in the given text.\n    (e.g. \"\\b\" becomes \"\\\\b\")\n\n    Args:\n        text (str): A string possibly containing control codes.\n\n    Returns:\n        str: String with control codes replaced with their escaped version.\n    \"\"\"\n    return text.translate(_translate_table)\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    from pip._vendor.rich.console import Console\n\n    console = Console()\n    console.print(\"Look at the title of your terminal window ^\")\n    # console.print(Control((ControlType.SET_WINDOW_TITLE, \"Hello, world!\")))\n    for i in range(10):\n        console.set_window_title(\"\ud83d\ude80 Loading\" + \".\" * i)\n        time.sleep(0.5)"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/live.py",
      "line_number": 1,
      "details": "import sys\nfrom threading import Event, RLock, Thread\nfrom types import TracebackType\nfrom typing import IO, Any, Callable, List, Optional, TextIO, Type, cast\n\nfrom . import get_console\nfrom .console import Console, ConsoleRenderable, RenderableType, RenderHook\nfrom .control import Control\nfrom .file_proxy import FileProxy\nfrom .jupyter import JupyterMixin\nfrom .live_render import LiveRender, VerticalOverflowMethod\nfrom .screen import Screen\nfrom .text import Text\n\n\nclass _RefreshThread(Thread):\n    \"\"\"A thread that calls refresh() at regular intervals.\"\"\"\n\n    def __init__(self, live: \"Live\", refresh_per_second: float) -> None:\n        self.live = live\n        self.refresh_per_second = refresh_per_second\n        self.done = Event()\n        super().__init__(daemon=True)\n\n    def stop(self) -> None:\n        self.done.set()\n\n    def run(self) -> None:\n        while not self.done.wait(1 / self.refresh_per_second):\n            with self.live._lock:\n                if not self.done.is_set():\n                    self.live.refresh()\n\n\nclass Live(JupyterMixin, RenderHook):\n    \"\"\"Renders an auto-updating live display of any given renderable.\n\n    Args:\n        renderable (RenderableType, optional): The renderable to live display. Defaults to displaying nothing.\n        console (Console, optional): Optional Console instance. Default will an internal Console instance writing to stdout.\n        screen (bool, optional): Enable alternate screen mode. Defaults to False.\n        auto_refresh (bool, optional): Enable auto refresh. If disabled, you will need to call `refresh()` or `update()` with refresh flag. Defaults to True\n        refresh_per_second (float, optional): Number of times per second to refresh the live display. Defaults to 4.\n        transient (bool, optional): Clear the renderable on exit (has no effect when screen=True). Defaults to False.\n        redirect_stdout (bool, optional): Enable redirection of stdout, so ``print`` may be used. Defaults to True.\n        redirect_stderr (bool, optional): Enable redirection of stderr. Defaults to True.\n        vertical_overflow (VerticalOverflowMethod, optional): How to handle renderable when it is too tall for the console. Defaults to \"ellipsis\".\n        get_renderable (Callable[[], RenderableType], optional): Optional callable to get renderable. Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        renderable: Optional[RenderableType] = None,\n        *,\n        console: Optional[Console] = None,\n        screen: bool = False,\n        auto_refresh: bool = True,\n        refresh_per_second: float = 4,\n        transient: bool = False,\n        redirect_stdout: bool = True,\n        redirect_stderr: bool = True,\n        vertical_overflow: VerticalOverflowMethod = \"ellipsis\",\n        get_renderable: Optional[Callable[[], RenderableType]] = None,\n    ) -> None:\n        assert refresh_per_second > 0, \"refresh_per_second must be > 0\"\n        self._renderable = renderable\n        self.console = console if console is not None else get_console()\n        self._screen = screen\n        self._alt_screen = False\n\n        self._redirect_stdout = redirect_stdout\n        self._redirect_stderr = redirect_stderr\n        self._restore_stdout: Optional[IO[str]] = None\n        self._restore_stderr: Optional[IO[str]] = None\n\n        self._lock = RLock()\n        self.ipy_widget: Optional[Any] = None\n        self.auto_refresh = auto_refresh\n        self._started: bool = False\n        self.transient = True if screen else transient\n\n        self._refresh_thread: Optional[_RefreshThread] = None\n        self.refresh_per_second = refresh_per_second\n\n        self.vertical_overflow = vertical_overflow\n        self._get_renderable = get_renderable\n        self._live_render = LiveRender(\n            self.get_renderable(), vertical_overflow=vertical_overflow\n        )\n\n    @property\n    def is_started(self) -> bool:\n        \"\"\"Check if live display has been started.\"\"\"\n        return self._started\n\n    def get_renderable(self) -> RenderableType:\n        renderable = (\n            self._get_renderable()\n            if self._get_renderable is not None\n            else self._renderable\n        )\n        return renderable or \"\"\n\n    def start(self, refresh: bool = False) -> None:\n        \"\"\"Start live rendering display.\n\n        Args:\n            refresh (bool, optional): Also refresh. Defaults to False.\n        \"\"\"\n        with self._lock:\n            if self._started:\n                return\n            self.console.set_live(self)\n            self._started = True\n            if self._screen:\n                self._alt_screen = self.console.set_alt_screen(True)\n            self.console.show_cursor(False)\n            self._enable_redirect_io()\n            self.console.push_render_hook(self)\n            if refresh:\n                try:\n                    self.refresh()\n                except Exception:\n                    # If refresh fails, we want to stop the redirection of sys.stderr,\n                    # so the error stacktrace is properly displayed in the terminal.\n                    # (or, if the code that calls Rich captures the exception and wants to display something,\n                    # let this be displayed in the terminal).\n                    self.stop()\n                    raise\n            if self.auto_refresh:\n                self._refresh_thread = _RefreshThread(self, self.refresh_per_second)\n                self._refresh_thread.start()\n\n    def stop(self) -> None:\n        \"\"\"Stop live rendering display.\"\"\"\n        with self._lock:\n            if not self._started:\n                return\n            self.console.clear_live()\n            self._started = False\n\n            if self.auto_refresh and self._refresh_thread is not None:\n                self._refresh_thread.stop()\n                self._refresh_thread = None\n            # allow it to fully render on the last even if overflow\n            self.vertical_overflow = \"visible\"\n            with self.console:\n                try:\n                    if not self._alt_screen and not self.console.is_jupyter:\n                        self.refresh()\n                finally:\n                    self._disable_redirect_io()\n                    self.console.pop_render_hook()\n                    if not self._alt_screen and self.console.is_terminal:\n                        self.console.line()\n                    self.console.show_cursor(True)\n                    if self._alt_screen:\n                        self.console.set_alt_screen(False)\n\n                    if self.transient and not self._alt_screen:\n                        self.console.control(self._live_render.restore_cursor())\n                    if self.ipy_widget is not None and self.transient:\n                        self.ipy_widget.close()  # pragma: no cover\n\n    def __enter__(self) -> \"Live\":\n        self.start(refresh=self._renderable is not None)\n        return self\n\n    def __exit__(\n        self,\n        exc_type: Optional[Type[BaseException]],\n        exc_val: Optional[BaseException],\n        exc_tb: Optional[TracebackType],\n    ) -> None:\n        self.stop()\n\n    def _enable_redirect_io(self) -> None:\n        \"\"\"Enable redirecting of stdout / stderr.\"\"\"\n        if self.console.is_terminal or self.console.is_jupyter:\n            if self._redirect_stdout and not isinstance(sys.stdout, FileProxy):\n                self._restore_stdout = sys.stdout\n                sys.stdout = cast(\"TextIO\", FileProxy(self.console, sys.stdout))\n            if self._redirect_stderr and not isinstance(sys.stderr, FileProxy):\n                self._restore_stderr = sys.stderr\n                sys.stderr = cast(\"TextIO\", FileProxy(self.console, sys.stderr))\n\n    def _disable_redirect_io(self) -> None:\n        \"\"\"Disable redirecting of stdout / stderr.\"\"\"\n        if self._restore_stdout:\n            sys.stdout = cast(\"TextIO\", self._restore_stdout)\n            self._restore_stdout = None\n        if self._restore_stderr:\n            sys.stderr = cast(\"TextIO\", self._restore_stderr)\n            self._restore_stderr = None\n\n    @property\n    def renderable(self) -> RenderableType:\n        \"\"\"Get the renderable that is being displayed\n\n        Returns:\n            RenderableType: Displayed renderable.\n        \"\"\"\n        renderable = self.get_renderable()\n        return Screen(renderable) if self._alt_screen else renderable\n\n    def update(self, renderable: RenderableType, *, refresh: bool = False) -> None:\n        \"\"\"Update the renderable that is being displayed\n\n        Args:\n            renderable (RenderableType): New renderable to use.\n            refresh (bool, optional): Refresh the display. Defaults to False.\n        \"\"\"\n        if isinstance(renderable, str):\n            renderable = self.console.render_str(renderable)\n        with self._lock:\n            self._renderable = renderable\n            if refresh:\n                self.refresh()\n\n    def refresh(self) -> None:\n        \"\"\"Update the display of the Live Render.\"\"\"\n        with self._lock:\n            self._live_render.set_renderable(self.renderable)\n            if self.console.is_jupyter:  # pragma: no cover\n                try:\n                    from IPython.display import display\n                    from ipywidgets import Output\n                except ImportError:\n                    import warnings\n\n                    warnings.warn('install \"ipywidgets\" for Jupyter support')\n                else:\n                    if self.ipy_widget is None:\n                        self.ipy_widget = Output()\n                        display(self.ipy_widget)\n\n                    with self.ipy_widget:\n                        self.ipy_widget.clear_output(wait=True)\n                        self.console.print(self._live_render.renderable)\n            elif self.console.is_terminal and not self.console.is_dumb_terminal:\n                with self.console:\n                    self.console.print(Control())\n            elif (\n                not self._started and not self.transient\n            ):  # if it is finished allow files or dumb-terminals to see final result\n                with self.console:\n                    self.console.print(Control())\n\n    def process_renderables(\n        self, renderables: List[ConsoleRenderable]\n    ) -> List[ConsoleRenderable]:\n        \"\"\"Process renderables to restore cursor and display progress.\"\"\"\n        self._live_render.vertical_overflow = self.vertical_overflow\n        if self.console.is_interactive:\n            # lock needs acquiring as user can modify live_render renderable at any time unlike in Progress.\n            with self._lock:\n                reset = (\n                    Control.home()\n                    if self._alt_screen\n                    else self._live_render.position_cursor()\n                )\n                renderables = [reset, *renderables, self._live_render]\n        elif (\n            not self._started and not self.transient\n        ):  # if it is finished render the final output for files or dumb_terminals\n            renderables = [*renderables, self._live_render]\n\n        return renderables\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    import random\n    import time\n    from itertools import cycle\n    from typing import Dict, List, Tuple\n\n    from .align import Align\n    from .console import Console\n    from .live import Live as Live\n    from .panel import Panel\n    from .rule import Rule\n    from .syntax import Syntax\n    from .table import Table\n\n    console = Console()\n\n    syntax = Syntax(\n        '''def loop_last(values: Iterable[T]) -> Iterable[Tuple[bool, T]]:\n    \"\"\"Iterate and generate a tuple with a flag for last value.\"\"\"\n    iter_values = iter(values)\n    try:\n        previous_value = next(iter_values)\n    except StopIteration:\n        return\n    for value in iter_values:\n        yield False, previous_value\n        previous_value = value\n    yield True, previous_value''',\n        \"python\",\n        line_numbers=True,\n    )\n\n    table = Table(\"foo\", \"bar\", \"baz\")\n    table.add_row(\"1\", \"2\", \"3\")\n\n    progress_renderables = [\n        \"You can make the terminal shorter and taller to see the live table hide\"\n        \"Text may be printed while the progress bars are rendering.\",\n        Panel(\"In fact, [i]any[/i] renderable will work\"),\n        \"Such as [magenta]tables[/]...\",\n        table,\n        \"Pretty printed structures...\",\n        {\"type\": \"example\", \"text\": \"Pretty printed\"},\n        \"Syntax...\",\n        syntax,\n        Rule(\"Give it a try!\"),\n    ]\n\n    examples = cycle(progress_renderables)\n\n    exchanges = [\n        \"SGD\",\n        \"MYR\",\n        \"EUR\",\n        \"USD\",\n        \"AUD\",\n        \"JPY\",\n        \"CNH\",\n        \"HKD\",\n        \"CAD\",\n        \"INR\",\n        \"DKK\",\n        \"GBP\",\n        \"RUB\",\n        \"NZD\",\n        \"MXN\",\n        \"IDR\",\n        \"TWD\",\n        \"THB\",\n        \"VND\",\n    ]\n    with Live(console=console) as live_table:\n        exchange_rate_dict: Dict[Tuple[str, str], float] = {}\n\n        for index in range(100):\n            select_exchange = exchanges[index % len(exchanges)]\n\n            for exchange in exchanges:\n                if exchange == select_exchange:\n                    continue\n                time.sleep(0.4)\n                if random.randint(0, 10) < 1:\n                    console.log(next(examples))\n                exchange_rate_dict[(select_exchange, exchange)] = 200 / (\n                    (random.random() * 320) + 1\n                )\n                if len(exchange_rate_dict) > len(exchanges) - 1:\n                    exchange_rate_dict.pop(list(exchange_rate_dict.keys())[0])\n                table = Table(title=\"Exchange Rates\")\n\n                table.add_column(\"Source Currency\")\n                table.add_column(\"Destination Currency\")\n                table.add_column(\"Exchange Rate\")\n\n                for ((source, dest), exchange_rate) in exchange_rate_dict.items():\n                    table.add_row(\n                        source,\n                        dest,\n                        Text(\n                            f\"{exchange_rate:.4f}\",\n                            style=\"red\" if exchange_rate < 1.0 else \"green\",\n                        ),\n                    )\n\n                live_table.update(Align.center(table))"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py",
      "line_number": 2,
      "details": "\")\n\n\ndef _safe_isinstance(\n    obj: object, class_or_tuple: Union[type, Tuple[type, ...]]\n) -> bool:\n    \"\"\"isinstance can fail in rare cases, for example types with no __class__\"\"\"\n    try:\n        return isinstance(obj, class_or_tuple)\n    except Exception:\n        return False\n\n\ndef install(\n    console: Optional[\"Console\"] = None,\n    overflow: \"OverflowMethod\" = \"ignore\",\n    crop: bool = False,\n    indent_guides: bool = False,\n    max_length: Optional[int] = None,\n    max_string: Optional[int] = None,\n    max_depth: Optional[int] = None,\n    expand_all: bool = False,\n) -> None:\n    \"\"\"Install automatic pretty printing in the Python REPL.\n\n    Args:\n        console (Console, optional): Console instance or ``None`` to use global console. Defaults to None.\n        overflow (Optional[OverflowMethod], optional): Overflow method. Defaults to \"ignore\".\n        crop (Optional[bool], optional): Enable cropping of long lines. Defaults to False.\n        indent_guides (bool, optional): Enable indentation guides. Defaults to False.\n        max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to None.\n        max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to None.\n        max_depth (int, optional): Maximum depth of nested data structures, or None for no maximum. Defaults to None.\n        expand_all (bool, optional): Expand all containers. Defaults to False.\n        max_frames (int): Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n    \"\"\"\n    from pip._vendor.rich import get_console\n\n    console = console or get_console()\n    assert console is not None\n\n    def display_hook(value: Any) -> None:\n        \"\"\"Replacement sys.displayhook which prettifies objects with Rich.\"\"\"\n        if value is not None:\n            assert console is not None\n            builtins._ = None  # type: ignore[attr-defined]\n            console.print(\n                value\n                if _safe_isinstance(value, RichRenderable)\n                else Pretty(\n                    value,\n                    overflow=overflow,\n                    indent_guides=indent_guides,\n                    max_length=max_length,\n                    max_string=max_string,\n                    max_depth=max_depth,\n                    expand_all=expand_all,\n                ),\n                crop=crop,\n            )\n            builtins._ = value  # type: ignore[attr-defined]\n\n    if \"get_ipython\" in globals():\n        ip = get_ipython()  # type: ignore[name-defined]\n        from IPython.core.formatters import BaseFormatter\n\n        class RichFormatter(BaseFormatter):  # type: ignore[misc]\n            pprint: bool = True\n\n            def __call__(self, value: Any) -> Any:\n                if self.pprint:\n                    return _ipy_display_hook(\n                        value,\n                        console=get_console(),\n                        overflow=overflow,\n                        indent_guides=indent_guides,\n                        max_length=max_length,\n                        max_string=max_string,\n                        max_depth=max_depth,\n                        expand_all=expand_all,\n                    )\n                else:\n                    return repr(value)\n\n        # replace plain text formatter with rich formatter\n        rich_formatter = RichFormatter()\n        ip.display_formatter.formatters[\"text/plain\"] = rich_formatter\n    else:\n        sys.displayhook = display_hook\n\n\nclass Pretty(JupyterMixin):\n    \"\"\"A rich renderable that pretty prints an object.\n\n    Args:\n        _object (Any): An object to pretty print.\n        highlighter (HighlighterType, optional): Highlighter object to apply to result, or None for ReprHighlighter. Defaults to None.\n        indent_size (int, optional): Number of spaces in indent. Defaults to 4.\n        justify (JustifyMethod, optional): Justify method, or None for default. Defaults to None.\n        overflow (OverflowMethod, optional): Overflow method, or None for default. Defaults to None.\n        no_wrap (Optional[bool], optional): Disable word wrapping. Defaults to False.\n        indent_guides (bool, optional): Enable indentation guides. Defaults to False.\n        max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to None.\n        max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to None.\n        max_depth (int, optional): Maximum depth of nested data structures, or None for no maximum. Defaults to None.\n        expand_all (bool, optional): Expand all containers. Defaults to False.\n        margin (int, optional): Subtrace a margin from width to force containers to expand earlier. Defaults to 0.\n        insert_line (bool, optional): Insert a new line if the output has multiple new lines. Defaults to False.\n    \"\"\"\n\n    def __init__(\n        self,\n        _object: Any,\n        highlighter: Optional[\"HighlighterType\"] = None,\n        *,\n        indent_size: int = 4,\n        justify: Optional[\"JustifyMethod\"] = None,\n        overflow: Optional[\"OverflowMethod\"] = None,\n        no_wrap: Optional[bool] = False,\n        indent_guides: bool = False,\n        max_length: Optional[int] = None,\n        max_string: Optional[int] = None,\n        max_depth: Optional[int] = None,\n        expand_all: bool = False,\n        margin: int = 0,\n        insert_line: bool = False,\n    ) -> None:\n        self._object = _object\n        self.highlighter = highlighter or ReprHighlighter()\n        self.indent_size = indent_size\n        self.justify: Optional[\"JustifyMethod\"] = justify\n        self.overflow: Optional[\"OverflowMethod\"] = overflow\n        self.no_wrap = no_wrap\n        self.indent_guides = indent_guides\n        self.max_length = max_length\n        self.max_string = max_string\n        self.max_depth = max_depth\n        self.expand_all = expand_all\n        self.margin = margin\n        self.insert_line = insert_line\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> \"RenderResult\":\n        pretty_str = pretty_repr(\n            self._object,\n            max_width=options.max_width - self.margin,\n            indent_size=self.indent_size,\n            max_length=self.max_length,\n            max_string=self.max_string,\n            max_depth=self.max_depth,\n            expand_all=self.expand_all,\n        )\n        pretty_text = Text.from_ansi(\n            pretty_str,\n            justify=self.justify or options.justify,\n            overflow=self.overflow or options.overflow,\n            no_wrap=pick_bool(self.no_wrap, options.no_wrap),\n            style=\"pretty\",\n        )\n        pretty_text = (\n            self.highlighter(pretty_text)\n            if pretty_text\n            else Text(\n                f\"{type(self._object)}.__repr__ returned empty string\",\n                style=\"dim italic\",\n            )\n        )\n        if self.indent_guides and not options.ascii_only:\n            pretty_text = pretty_text.with_indent_guides(\n                self.indent_size, style=\"repr.indent\"\n            )\n        if self.insert_line and \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/pretty.py",
      "line_number": 4,
      "details": "\".join(str(line) for line in lines)\n        return repr_str\n\n\n@dataclass\nclass _Line:\n    \"\"\"A line in repr output.\"\"\"\n\n    parent: Optional[\"_Line\"] = None\n    is_root: bool = False\n    node: Optional[Node] = None\n    text: str = \"\"\n    suffix: str = \"\"\n    whitespace: str = \"\"\n    expanded: bool = False\n    last: bool = False\n\n    @property\n    def expandable(self) -> bool:\n        \"\"\"Check if the line may be expanded.\"\"\"\n        return bool(self.node is not None and self.node.children)\n\n    def check_length(self, max_length: int) -> bool:\n        \"\"\"Check this line fits within a given number of cells.\"\"\"\n        start_length = (\n            len(self.whitespace) + cell_len(self.text) + cell_len(self.suffix)\n        )\n        assert self.node is not None\n        return self.node.check_length(start_length, max_length)\n\n    def expand(self, indent_size: int) -> Iterable[\"_Line\"]:\n        \"\"\"Expand this line by adding children on their own line.\"\"\"\n        node = self.node\n        assert node is not None\n        whitespace = self.whitespace\n        assert node.children\n        if node.key_repr:\n            new_line = yield _Line(\n                text=f\"{node.key_repr}{node.key_separator}{node.open_brace}\",\n                whitespace=whitespace,\n            )\n        else:\n            new_line = yield _Line(text=node.open_brace, whitespace=whitespace)\n        child_whitespace = self.whitespace + \" \" * indent_size\n        tuple_of_one = node.is_tuple and len(node.children) == 1\n        for last, child in loop_last(node.children):\n            separator = \",\" if tuple_of_one else node.separator\n            line = _Line(\n                parent=new_line,\n                node=child,\n                whitespace=child_whitespace,\n                suffix=separator,\n                last=last and not tuple_of_one,\n            )\n            yield line\n\n        yield _Line(\n            text=node.close_brace,\n            whitespace=whitespace,\n            suffix=self.suffix,\n            last=self.last,\n        )\n\n    def __str__(self) -> str:\n        if self.last:\n            return f\"{self.whitespace}{self.text}{self.node or ''}\"\n        else:\n            return (\n                f\"{self.whitespace}{self.text}{self.node or ''}{self.suffix.rstrip()}\"\n            )\n\n\ndef _is_namedtuple(obj: Any) -> bool:\n    \"\"\"Checks if an object is most likely a namedtuple. It is possible\n    to craft an object that passes this check and isn't a namedtuple, but\n    there is only a minuscule chance of this happening unintentionally.\n\n    Args:\n        obj (Any): The object to test\n\n    Returns:\n        bool: True if the object is a namedtuple. False otherwise.\n    \"\"\"\n    try:\n        fields = getattr(obj, \"_fields\", None)\n    except Exception:\n        # Being very defensive - if we cannot get the attr then its not a namedtuple\n        return False\n    return isinstance(obj, tuple) and isinstance(fields, tuple)\n\n\ndef traverse(\n    _object: Any,\n    max_length: Optional[int] = None,\n    max_string: Optional[int] = None,\n    max_depth: Optional[int] = None,\n) -> Node:\n    \"\"\"Traverse object and generate a tree.\n\n    Args:\n        _object (Any): Object to be traversed.\n        max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to None.\n        max_string (int, optional): Maximum length of string before truncating, or None to disable truncating.\n            Defaults to None.\n        max_depth (int, optional): Maximum depth of data structures, or None for no maximum.\n            Defaults to None.\n\n    Returns:\n        Node: The root of a tree structure which can be used to render a pretty repr.\n    \"\"\"\n\n    def to_repr(obj: Any) -> str:\n        \"\"\"Get repr string for an object, but catch errors.\"\"\"\n        if (\n            max_string is not None\n            and _safe_isinstance(obj, (bytes, str))\n            and len(obj) > max_string\n        ):\n            truncated = len(obj) - max_string\n            obj_repr = f\"{obj[:max_string]!r}+{truncated}\"\n        else:\n            try:\n                obj_repr = repr(obj)\n            except Exception as error:\n                obj_repr = f\"<repr-error {str(error)!r}>\"\n        return obj_repr\n\n    visited_ids: Set[int] = set()\n    push_visited = visited_ids.add\n    pop_visited = visited_ids.remove\n\n    def _traverse(obj: Any, root: bool = False, depth: int = 0) -> Node:\n        \"\"\"Walk the object depth first.\"\"\"\n\n        obj_id = id(obj)\n        if obj_id in visited_ids:\n            # Recursion detected\n            return Node(value_repr=\"...\")\n\n        obj_type = type(obj)\n        children: List[Node]\n        reached_max_depth = max_depth is not None and depth >= max_depth\n\n        def iter_rich_args(rich_args: Any) -> Iterable[Union[Any, Tuple[str, Any]]]:\n            for arg in rich_args:\n                if _safe_isinstance(arg, tuple):\n                    if len(arg) == 3:\n                        key, child, default = arg\n                        if default == child:\n                            continue\n                        yield key, child\n                    elif len(arg) == 2:\n                        key, child = arg\n                        yield key, child\n                    elif len(arg) == 1:\n                        yield arg[0]\n                else:\n                    yield arg\n\n        try:\n            fake_attributes = hasattr(\n                obj, \"awehoi234_wdfjwljet234_234wdfoijsdfmmnxpi492\"\n            )\n        except Exception:\n            fake_attributes = False\n\n        rich_repr_result: Optional[RichReprResult] = None\n        if not fake_attributes:\n            try:\n                if hasattr(obj, \"__rich_repr__\") and not isclass(obj):\n                    rich_repr_result = obj.__rich_repr__()\n            except Exception:\n                pass\n\n        if rich_repr_result is not None:\n            push_visited(obj_id)\n            angular = getattr(obj.__rich_repr__, \"angular\", False)\n            args = list(iter_rich_args(rich_repr_result))\n            class_name = obj.__class__.__name__\n\n            if args:\n                children = []\n                append = children.append\n\n                if reached_max_depth:\n                    if angular:\n                        node = Node(value_repr=f\"<{class_name}...>\")\n                    else:\n                        node = Node(value_repr=f\"{class_name}(...)\")\n                else:\n                    if angular:\n                        node = Node(\n                            open_brace=f\"<{class_name} \",\n                            close_brace=\">\",\n                            children=children,\n                            last=root,\n                            separator=\" \",\n                        )\n                    else:\n                        node = Node(\n                            open_brace=f\"{class_name}(\",\n                            close_brace=\")\",\n                            children=children,\n                            last=root,\n                        )\n                    for last, arg in loop_last(args):\n                        if _safe_isinstance(arg, tuple):\n                            key, child = arg\n                            child_node = _traverse(child, depth=depth + 1)\n                            child_node.last = last\n                            child_node.key_repr = key\n                            child_node.key_separator = \"=\"\n                            append(child_node)\n                        else:\n                            child_node = _traverse(arg, depth=depth + 1)\n                            child_node.last = last\n                            append(child_node)\n            else:\n                node = Node(\n                    value_repr=f\"<{class_name}>\" if angular else f\"{class_name}()\",\n                    children=[],\n                    last=root,\n                )\n            pop_visited(obj_id)\n        elif _is_attr_object(obj) and not fake_attributes:\n            push_visited(obj_id)\n            children = []\n            append = children.append\n\n            attr_fields = _get_attr_fields(obj)\n            if attr_fields:\n                if reached_max_depth:\n                    node = Node(value_repr=f\"{obj.__class__.__name__}(...)\")\n                else:\n                    node = Node(\n                        open_brace=f\"{obj.__class__.__name__}(\",\n                        close_brace=\")\",\n                        children=children,\n                        last=root,\n                    )\n\n                    def iter_attrs() -> Iterable[\n                        Tuple[str, Any, Optional[Callable[[Any], str]]]\n                    ]:\n                        \"\"\"Iterate over attr fields and values.\"\"\"\n                        for attr in attr_fields:\n                            if attr.repr:\n                                try:\n                                    value = getattr(obj, attr.name)\n                                except Exception as error:\n                                    # Can happen, albeit rarely\n                                    yield (attr.name, error, None)\n                                else:\n                                    yield (\n                                        attr.name,\n                                        value,\n                                        attr.repr if callable(attr.repr) else None,\n                                    )\n\n                    for last, (name, value, repr_callable) in loop_last(iter_attrs()):\n                        if repr_callable:\n                            child_node = Node(value_repr=str(repr_callable(value)))\n                        else:\n                            child_node = _traverse(value, depth=depth + 1)\n                        child_node.last = last\n                        child_node.key_repr = name\n                        child_node.key_separator = \"=\"\n                        append(child_node)\n            else:\n                node = Node(\n                    value_repr=f\"{obj.__class__.__name__}()\", children=[], last=root\n                )\n            pop_visited(obj_id)\n        elif (\n            is_dataclass(obj)\n            and not _safe_isinstance(obj, type)\n            and not fake_attributes\n            and _is_dataclass_repr(obj)\n        ):\n            push_visited(obj_id)\n            children = []\n            append = children.append\n            if reached_max_depth:\n                node = Node(value_repr=f\"{obj.__class__.__name__}(...)\")\n            else:\n                node = Node(\n                    open_brace=f\"{obj.__class__.__name__}(\",\n                    close_brace=\")\",\n                    children=children,\n                    last=root,\n                    empty=f\"{obj.__class__.__name__}()\",\n                )\n\n                for last, field in loop_last(\n                    field for field in fields(obj) if field.repr\n                ):\n                    child_node = _traverse(getattr(obj, field.name), depth=depth + 1)\n                    child_node.key_repr = field.name\n                    child_node.last = last\n                    child_node.key_separator = \"=\"\n                    append(child_node)\n\n            pop_visited(obj_id)\n        elif _is_namedtuple(obj) and _has_default_namedtuple_repr(obj):\n            push_visited(obj_id)\n            class_name = obj.__class__.__name__\n            if reached_max_depth:\n                # If we've reached the max depth, we still show the class name, but not its contents\n                node = Node(\n                    value_repr=f\"{class_name}(...)\",\n                )\n            else:\n                children = []\n                append = children.append\n                node = Node(\n                    open_brace=f\"{class_name}(\",\n                    close_brace=\")\",\n                    children=children,\n                    empty=f\"{class_name}()\",\n                )\n                for last, (key, value) in loop_last(obj._asdict().items()):\n                    child_node = _traverse(value, depth=depth + 1)\n                    child_node.key_repr = key\n                    child_node.last = last\n                    child_node.key_separator = \"=\"\n                    append(child_node)\n            pop_visited(obj_id)\n        elif _safe_isinstance(obj, _CONTAINERS):\n            for container_type in _CONTAINERS:\n                if _safe_isinstance(obj, container_type):\n                    obj_type = container_type\n                    break\n\n            push_visited(obj_id)\n\n            open_brace, close_brace, empty = _BRACES[obj_type](obj)\n\n            if reached_max_depth:\n                node = Node(value_repr=f\"{open_brace}...{close_brace}\")\n            elif obj_type.__repr__ != type(obj).__repr__:\n                node = Node(value_repr=to_repr(obj), last=root)\n            elif obj:\n                children = []\n                node = Node(\n                    open_brace=open_brace,\n                    close_brace=close_brace,\n                    children=children,\n                    last=root,\n                )\n                append = children.append\n                num_items = len(obj)\n                last_item_index = num_items - 1\n\n                if _safe_isinstance(obj, _MAPPING_CONTAINERS):\n                    iter_items = iter(obj.items())\n                    if max_length is not None:\n                        iter_items = islice(iter_items, max_length)\n                    for index, (key, child) in enumerate(iter_items):\n                        child_node = _traverse(child, depth=depth + 1)\n                        child_node.key_repr = to_repr(key)\n                        child_node.last = index == last_item_index\n                        append(child_node)\n                else:\n                    iter_values = iter(obj)\n                    if max_length is not None:\n                        iter_values = islice(iter_values, max_length)\n                    for index, child in enumerate(iter_values):\n                        child_node = _traverse(child, depth=depth + 1)\n                        child_node.last = index == last_item_index\n                        append(child_node)\n                if max_length is not None and num_items > max_length:\n                    append(Node(value_repr=f\"... +{num_items - max_length}\", last=True))\n            else:\n                node = Node(empty=empty, children=[], last=root)\n\n            pop_visited(obj_id)\n        else:\n            node = Node(value_repr=to_repr(obj), last=root)\n        node.is_tuple = _safe_isinstance(obj, tuple)\n        node.is_namedtuple = _is_namedtuple(obj)\n        return node\n\n    node = _traverse(_object, root=True)\n    return node\n\n\ndef pretty_repr(\n    _object: Any,\n    *,\n    max_width: int = 80,\n    indent_size: int = 4,\n    max_length: Optional[int] = None,\n    max_string: Optional[int] = None,\n    max_depth: Optional[int] = None,\n    expand_all: bool = False,\n) -> str:\n    \"\"\"Prettify repr string by expanding on to new lines to fit within a given width.\n\n    Args:\n        _object (Any): Object to repr.\n        max_width (int, optional): Desired maximum width of repr string. Defaults to 80.\n        indent_size (int, optional): Number of spaces to indent. Defaults to 4.\n        max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to None.\n        max_string (int, optional): Maximum length of string before truncating, or None to disable truncating.\n            Defaults to None.\n        max_depth (int, optional): Maximum depth of nested data structure, or None for no depth.\n            Defaults to None.\n        expand_all (bool, optional): Expand all containers regardless of available width. Defaults to False.\n\n    Returns:\n        str: A possibly multi-line representation of the object.\n    \"\"\"\n\n    if _safe_isinstance(_object, Node):\n        node = _object\n    else:\n        node = traverse(\n            _object, max_length=max_length, max_string=max_string, max_depth=max_depth\n        )\n    repr_str: str = node.render(\n        max_width=max_width, indent_size=indent_size, expand_all=expand_all\n    )\n    return repr_str\n\n\ndef pprint(\n    _object: Any,\n    *,\n    console: Optional[\"Console\"] = None,\n    indent_guides: bool = True,\n    max_length: Optional[int] = None,\n    max_string: Optional[int] = None,\n    max_depth: Optional[int] = None,\n    expand_all: bool = False,\n) -> None:\n    \"\"\"A convenience function for pretty printing.\n\n    Args:\n        _object (Any): Object to pretty print.\n        console (Console, optional): Console instance, or None to use default. Defaults to None.\n        max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to None.\n        max_string (int, optional): Maximum length of strings before truncating, or None to disable. Defaults to None.\n        max_depth (int, optional): Maximum depth for nested data structures, or None for unlimited depth. Defaults to None.\n        indent_guides (bool, optional): Enable indentation guides. Defaults to True.\n        expand_all (bool, optional): Expand all containers. Defaults to False.\n    \"\"\"\n    _console = get_console() if console is None else console\n    _console.print(\n        Pretty(\n            _object,\n            max_length=max_length,\n            max_string=max_string,\n            max_depth=max_depth,\n            indent_guides=indent_guides,\n            expand_all=expand_all,\n            overflow=\"ignore\",\n        ),\n        soft_wrap=True,\n    )\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n\n    class BrokenRepr:\n        def __repr__(self) -> str:\n            1 / 0\n            return \"this will fail\"\n\n    from typing import NamedTuple\n\n    class StockKeepingUnit(NamedTuple):\n        name: str\n        description: str\n        price: float\n        category: str\n        reviews: List[str]\n\n    d = defaultdict(int)\n    d[\"foo\"] = 5\n    data = {\n        \"foo\": [\n            1,\n            \"Hello World!\",\n            100.123,\n            323.232,\n            432324.0,\n            {5, 6, 7, (1, 2, 3, 4), 8},\n        ],\n        \"bar\": frozenset({1, 2, 3}),\n        \"defaultdict\": defaultdict(\n            list, {\"crumble\": [\"apple\", \"rhubarb\", \"butter\", \"sugar\", \"flour\"]}\n        ),\n        \"counter\": Counter(\n            [\n                \"apple\",\n                \"orange\",\n                \"pear\",\n                \"kumquat\",\n                \"kumquat\",\n                \"durian\" * 100,\n            ]\n        ),\n        \"atomic\": (False, True, None),\n        \"namedtuple\": StockKeepingUnit(\n            \"Sparkling British Spring Water\",\n            \"Carbonated spring water\",\n            0.9,\n            \"water\",\n            [\"its amazing!\", \"its terrible!\"],\n        ),\n        \"Broken\": BrokenRepr(),\n    }\n    data[\"foo\"].append(data)  # type: ignore[attr-defined]\n\n    from pip._vendor.rich import print\n\n    # print(Pretty(data, indent_guides=True, max_string=20))\n\n    class Thing:\n        def __repr__(self) -> str:\n            return \"Hello\\x1b[38;5;239m World!\"\n\n    print(Pretty(Thing()))"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/scope.py",
      "line_number": 1,
      "details": "from collections.abc import Mapping\nfrom typing import TYPE_CHECKING, Any, Optional, Tuple\n\nfrom .highlighter import ReprHighlighter\nfrom .panel import Panel\nfrom .pretty import Pretty\nfrom .table import Table\nfrom .text import Text, TextType\n\nif TYPE_CHECKING:\n    from .console import ConsoleRenderable\n\n\ndef render_scope(\n    scope: \"Mapping[str, Any]\",\n    *,\n    title: Optional[TextType] = None,\n    sort_keys: bool = True,\n    indent_guides: bool = False,\n    max_length: Optional[int] = None,\n    max_string: Optional[int] = None,\n) -> \"ConsoleRenderable\":\n    \"\"\"Render python variables in a given scope.\n\n    Args:\n        scope (Mapping): A mapping containing variable names and values.\n        title (str, optional): Optional title. Defaults to None.\n        sort_keys (bool, optional): Enable sorting of items. Defaults to True.\n        indent_guides (bool, optional): Enable indentation guides. Defaults to False.\n        max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to None.\n        max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to None.\n\n    Returns:\n        ConsoleRenderable: A renderable object.\n    \"\"\"\n    highlighter = ReprHighlighter()\n    items_table = Table.grid(padding=(0, 1), expand=False)\n    items_table.add_column(justify=\"right\")\n\n    def sort_items(item: Tuple[str, Any]) -> Tuple[bool, str]:\n        \"\"\"Sort special variables first, then alphabetically.\"\"\"\n        key, _ = item\n        return (not key.startswith(\"__\"), key.lower())\n\n    items = sorted(scope.items(), key=sort_items) if sort_keys else scope.items()\n    for key, value in items:\n        key_text = Text.assemble(\n            (key, \"scope.key.special\" if key.startswith(\"__\") else \"scope.key\"),\n            (\" =\", \"scope.equals\"),\n        )\n        items_table.add_row(\n            key_text,\n            Pretty(\n                value,\n                highlighter=highlighter,\n                indent_guides=indent_guides,\n                max_length=max_length,\n                max_string=max_string,\n            ),\n        )\n    return Panel.fit(\n        items_table,\n        title=title,\n        border_style=\"scope.border\",\n        padding=(0, 1),\n    )\n\n\nif __name__ == \"__main__\":  # pragma: no cover\n    from pip._vendor.rich import print\n\n    print()\n\n    def test(foo: float, bar: float) -> None:\n        list_of_things = [1, 2, 3, None, 4, True, False, \"Hello World\"]\n        dict_of_things = {\n            \"version\": \"1.1\",\n            \"method\": \"confirmFruitPurchase\",\n            \"params\": [[\"apple\", \"orange\", \"mangoes\", \"pomelo\"], 1.123],\n            \"id\": \"194521489\",\n        }\n        print(render_scope(locals(), title=\"[i]locals\", sort_keys=False))\n\n    test(20.3423, 3.1427)\n    print()"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/style.py",
      "line_number": 1,
      "details": "import sys\nfrom functools import lru_cache\nfrom marshal import dumps, loads\nfrom random import randint\nfrom typing import Any, Dict, Iterable, List, Optional, Type, Union, cast\n\nfrom . import errors\nfrom .color import Color, ColorParseError, ColorSystem, blend_rgb\nfrom .repr import Result, rich_repr\nfrom .terminal_theme import DEFAULT_TERMINAL_THEME, TerminalTheme\n\n# Style instances and style definitions are often interchangeable\nStyleType = Union[str, \"Style\"]\n\n\nclass _Bit:\n    \"\"\"A descriptor to get/set a style attribute bit.\"\"\"\n\n    __slots__ = [\"bit\"]\n\n    def __init__(self, bit_no: int) -> None:\n        self.bit = 1 << bit_no\n\n    def __get__(self, obj: \"Style\", objtype: Type[\"Style\"]) -> Optional[bool]:\n        if obj._set_attributes & self.bit:\n            return obj._attributes & self.bit != 0\n        return None\n\n\n@rich_repr\nclass Style:\n    \"\"\"A terminal style.\n\n    A terminal style consists of a color (`color`), a background color (`bgcolor`), and a number of attributes, such\n    as bold, italic etc. The attributes have 3 states: they can either be on\n    (``True``), off (``False``), or not set (``None``).\n\n    Args:\n        color (Union[Color, str], optional): Color of terminal text. Defaults to None.\n        bgcolor (Union[Color, str], optional): Color of terminal background. Defaults to None.\n        bold (bool, optional): Enable bold text. Defaults to None.\n        dim (bool, optional): Enable dim text. Defaults to None.\n        italic (bool, optional): Enable italic text. Defaults to None.\n        underline (bool, optional): Enable underlined text. Defaults to None.\n        blink (bool, optional): Enabled blinking text. Defaults to None.\n        blink2 (bool, optional): Enable fast blinking text. Defaults to None.\n        reverse (bool, optional): Enabled reverse text. Defaults to None.\n        conceal (bool, optional): Enable concealed text. Defaults to None.\n        strike (bool, optional): Enable strikethrough text. Defaults to None.\n        underline2 (bool, optional): Enable doubly underlined text. Defaults to None.\n        frame (bool, optional): Enable framed text. Defaults to None.\n        encircle (bool, optional): Enable encircled text. Defaults to None.\n        overline (bool, optional): Enable overlined text. Defaults to None.\n        link (str, link): Link URL. Defaults to None.\n\n    \"\"\"\n\n    _color: Optional[Color]\n    _bgcolor: Optional[Color]\n    _attributes: int\n    _set_attributes: int\n    _hash: Optional[int]\n    _null: bool\n    _meta: Optional[bytes]\n\n    __slots__ = [\n        \"_color\",\n        \"_bgcolor\",\n        \"_attributes\",\n        \"_set_attributes\",\n        \"_link\",\n        \"_link_id\",\n        \"_ansi\",\n        \"_style_definition\",\n        \"_hash\",\n        \"_null\",\n        \"_meta\",\n    ]\n\n    # maps bits on to SGR parameter\n    _style_map = {\n        0: \"1\",\n        1: \"2\",\n        2: \"3\",\n        3: \"4\",\n        4: \"5\",\n        5: \"6\",\n        6: \"7\",\n        7: \"8\",\n        8: \"9\",\n        9: \"21\",\n        10: \"51\",\n        11: \"52\",\n        12: \"53\",\n    }\n\n    STYLE_ATTRIBUTES = {\n        \"dim\": \"dim\",\n        \"d\": \"dim\",\n        \"bold\": \"bold\",\n        \"b\": \"bold\",\n        \"italic\": \"italic\",\n        \"i\": \"italic\",\n        \"underline\": \"underline\",\n        \"u\": \"underline\",\n        \"blink\": \"blink\",\n        \"blink2\": \"blink2\",\n        \"reverse\": \"reverse\",\n        \"r\": \"reverse\",\n        \"conceal\": \"conceal\",\n        \"c\": \"conceal\",\n        \"strike\": \"strike\",\n        \"s\": \"strike\",\n        \"underline2\": \"underline2\",\n        \"uu\": \"underline2\",\n        \"frame\": \"frame\",\n        \"encircle\": \"encircle\",\n        \"overline\": \"overline\",\n        \"o\": \"overline\",\n    }\n\n    def __init__(\n        self,\n        *,\n        color: Optional[Union[Color, str]] = None,\n        bgcolor: Optional[Union[Color, str]] = None,\n        bold: Optional[bool] = None,\n        dim: Optional[bool] = None,\n        italic: Optional[bool] = None,\n        underline: Optional[bool] = None,\n        blink: Optional[bool] = None,\n        blink2: Optional[bool] = None,\n        reverse: Optional[bool] = None,\n        conceal: Optional[bool] = None,\n        strike: Optional[bool] = None,\n        underline2: Optional[bool] = None,\n        frame: Optional[bool] = None,\n        encircle: Optional[bool] = None,\n        overline: Optional[bool] = None,\n        link: Optional[str] = None,\n        meta: Optional[Dict[str, Any]] = None,\n    ):\n        self._ansi: Optional[str] = None\n        self._style_definition: Optional[str] = None\n\n        def _make_color(color: Union[Color, str]) -> Color:\n            return color if isinstance(color, Color) else Color.parse(color)\n\n        self._color = None if color is None else _make_color(color)\n        self._bgcolor = None if bgcolor is None else _make_color(bgcolor)\n        self._set_attributes = sum(\n            (\n                bold is not None,\n                dim is not None and 2,\n                italic is not None and 4,\n                underline is not None and 8,\n                blink is not None and 16,\n                blink2 is not None and 32,\n                reverse is not None and 64,\n                conceal is not None and 128,\n                strike is not None and 256,\n                underline2 is not None and 512,\n                frame is not None and 1024,\n                encircle is not None and 2048,\n                overline is not None and 4096,\n            )\n        )\n        self._attributes = (\n            sum(\n                (\n                    bold and 1 or 0,\n                    dim and 2 or 0,\n                    italic and 4 or 0,\n                    underline and 8 or 0,\n                    blink and 16 or 0,\n                    blink2 and 32 or 0,\n                    reverse and 64 or 0,\n                    conceal and 128 or 0,\n                    strike and 256 or 0,\n                    underline2 and 512 or 0,\n                    frame and 1024 or 0,\n                    encircle and 2048 or 0,\n                    overline and 4096 or 0,\n                )\n            )\n            if self._set_attributes\n            else 0\n        )\n\n        self._link = link\n        self._meta = None if meta is None else dumps(meta)\n        self._link_id = (\n            f\"{randint(0, 999999)}{hash(self._meta)}\" if (link or meta) else \"\"\n        )\n        self._hash: Optional[int] = None\n        self._null = not (self._set_attributes or color or bgcolor or link or meta)\n\n    @classmethod\n    def null(cls) -> \"Style\":\n        \"\"\"Create an 'null' style, equivalent to Style(), but more performant.\"\"\"\n        return NULL_STYLE\n\n    @classmethod\n    def from_color(\n        cls, color: Optional[Color] = None, bgcolor: Optional[Color] = None\n    ) -> \"Style\":\n        \"\"\"Create a new style with colors and no attributes.\n\n        Returns:\n            color (Optional[Color]): A (foreground) color, or None for no color. Defaults to None.\n            bgcolor (Optional[Color]): A (background) color, or None for no color. Defaults to None.\n        \"\"\"\n        style: Style = cls.__new__(Style)\n        style._ansi = None\n        style._style_definition = None\n        style._color = color\n        style._bgcolor = bgcolor\n        style._set_attributes = 0\n        style._attributes = 0\n        style._link = None\n        style._link_id = \"\"\n        style._meta = None\n        style._null = not (color or bgcolor)\n        style._hash = None\n        return style\n\n    @classmethod\n    def from_meta(cls, meta: Optional[Dict[str, Any]]) -> \"Style\":\n        \"\"\"Create a new style with meta data.\n\n        Returns:\n            meta (Optional[Dict[str, Any]]): A dictionary of meta data. Defaults to None.\n        \"\"\"\n        style: Style = cls.__new__(Style)\n        style._ansi = None\n        style._style_definition = None\n        style._color = None\n        style._bgcolor = None\n        style._set_attributes = 0\n        style._attributes = 0\n        style._link = None\n        style._meta = dumps(meta)\n        style._link_id = f\"{randint(0, 999999)}{hash(style._meta)}\"\n        style._hash = None\n        style._null = not (meta)\n        return style\n\n    @classmethod\n    def on(cls, meta: Optional[Dict[str, Any]] = None, **handlers: Any) -> \"Style\":\n        \"\"\"Create a blank style with meta information.\n\n        Example:\n            style = Style.on(click=self.on_click)\n\n        Args:\n            meta (Optional[Dict[str, Any]], optional): An optional dict of meta information.\n            **handlers (Any): Keyword arguments are translated in to handlers.\n\n        Returns:\n            Style: A Style with meta information attached.\n        \"\"\"\n        meta = {} if meta is None else meta\n        meta.update({f\"@{key}\": value for key, value in handlers.items()})\n        return cls.from_meta(meta)\n\n    bold = _Bit(0)\n    dim = _Bit(1)\n    italic = _Bit(2)\n    underline = _Bit(3)\n    blink = _Bit(4)\n    blink2 = _Bit(5)\n    reverse = _Bit(6)\n    conceal = _Bit(7)\n    strike = _Bit(8)\n    underline2 = _Bit(9)\n    frame = _Bit(10)\n    encircle = _Bit(11)\n    overline = _Bit(12)\n\n    @property\n    def link_id(self) -> str:\n        \"\"\"Get a link id, used in ansi code for links.\"\"\"\n        return self._link_id\n\n    def __str__(self) -> str:\n        \"\"\"Re-generate style definition from attributes.\"\"\"\n        if self._style_definition is None:\n            attributes: List[str] = []\n            append = attributes.append\n            bits = self._set_attributes\n            if bits & 0b0000000001111:\n                if bits & 1:\n                    append(\"bold\" if self.bold else \"not bold\")\n                if bits & (1 << 1):\n                    append(\"dim\" if self.dim else \"not dim\")\n                if bits & (1 << 2):\n                    append(\"italic\" if self.italic else \"not italic\")\n                if bits & (1 << 3):\n                    append(\"underline\" if self.underline else \"not underline\")\n            if bits & 0b0000111110000:\n                if bits & (1 << 4):\n                    append(\"blink\" if self.blink else \"not blink\")\n                if bits & (1 << 5):\n                    append(\"blink2\" if self.blink2 else \"not blink2\")\n                if bits & (1 << 6):\n                    append(\"reverse\" if self.reverse else \"not reverse\")\n                if bits & (1 << 7):\n                    append(\"conceal\" if self.conceal else \"not conceal\")\n                if bits & (1 << 8):\n                    append(\"strike\" if self.strike else \"not strike\")\n            if bits & 0b1111000000000:\n                if bits & (1 << 9):\n                    append(\"underline2\" if self.underline2 else \"not underline2\")\n                if bits & (1 << 10):\n                    append(\"frame\" if self.frame else \"not frame\")\n                if bits & (1 << 11):\n                    append(\"encircle\" if self.encircle else \"not encircle\")\n                if bits & (1 << 12):\n                    append(\"overline\" if self.overline else \"not overline\")\n            if self._color is not None:\n                append(self._color.name)\n            if self._bgcolor is not None:\n                append(\"on\")\n                append(self._bgcolor.name)\n            if self._link:\n                append(\"link\")\n                append(self._link)\n            self._style_definition = \" \".join(attributes) or \"none\"\n        return self._style_definition\n\n    def __bool__(self) -> bool:\n        \"\"\"A Style is false if it has no attributes, colors, or links.\"\"\"\n        return not self._null\n\n    def _make_ansi_codes(self, color_system: ColorSystem) -> str:\n        \"\"\"Generate ANSI codes for this style.\n\n        Args:\n            color_system (ColorSystem): Color system.\n\n        Returns:\n            str: String containing codes.\n        \"\"\"\n\n        if self._ansi is None:\n            sgr: List[str] = []\n            append = sgr.append\n            _style_map = self._style_map\n            attributes = self._attributes & self._set_attributes\n            if attributes:\n                if attributes & 1:\n                    append(_style_map[0])\n                if attributes & 2:\n                    append(_style_map[1])\n                if attributes & 4:\n                    append(_style_map[2])\n                if attributes & 8:\n                    append(_style_map[3])\n                if attributes & 0b0000111110000:\n                    for bit in range(4, 9):\n                        if attributes & (1 << bit):\n                            append(_style_map[bit])\n                if attributes & 0b1111000000000:\n                    for bit in range(9, 13):\n                        if attributes & (1 << bit):\n                            append(_style_map[bit])\n            if self._color is not None:\n                sgr.extend(self._color.downgrade(color_system).get_ansi_codes())\n            if self._bgcolor is not None:\n                sgr.extend(\n                    self._bgcolor.downgrade(color_system).get_ansi_codes(\n                        foreground=False\n                    )\n                )\n            self._ansi = \";\".join(sgr)\n        return self._ansi\n\n    @classmethod\n    @lru_cache(maxsize=1024)\n    def normalize(cls, style: str) -> str:\n        \"\"\"Normalize a style definition so that styles with the same effect have the same string\n        representation.\n\n        Args:\n            style (str): A style definition.\n\n        Returns:\n            str: Normal form of style definition.\n        \"\"\"\n        try:\n            return str(cls.parse(style))\n        except errors.StyleSyntaxError:\n            return style.strip().lower()\n\n    @classmethod\n    def pick_first(cls, *values: Optional[StyleType]) -> StyleType:\n        \"\"\"Pick first non-None style.\"\"\"\n        for value in values:\n            if value is not None:\n                return value\n        raise ValueError(\"expected at least one non-None style\")\n\n    def __rich_repr__(self) -> Result:\n        yield \"color\", self.color, None\n        yield \"bgcolor\", self.bgcolor, None\n        yield \"bold\", self.bold, None,\n        yield \"dim\", self.dim, None,\n        yield \"italic\", self.italic, None\n        yield \"underline\", self.underline, None,\n        yield \"blink\", self.blink, None\n        yield \"blink2\", self.blink2, None\n        yield \"reverse\", self.reverse, None\n        yield \"conceal\", self.conceal, None\n        yield \"strike\", self.strike, None\n        yield \"underline2\", self.underline2, None\n        yield \"frame\", self.frame, None\n        yield \"encircle\", self.encircle, None\n        yield \"link\", self.link, None\n        if self._meta:\n            yield \"meta\", self.meta\n\n    def __eq__(self, other: Any) -> bool:\n        if not isinstance(other, Style):\n            return NotImplemented\n        return self.__hash__() == other.__hash__()\n\n    def __ne__(self, other: Any) -> bool:\n        if not isinstance(other, Style):\n            return NotImplemented\n        return self.__hash__() != other.__hash__()\n\n    def __hash__(self) -> int:\n        if self._hash is not None:\n            return self._hash\n        self._hash = hash(\n            (\n                self._color,\n                self._bgcolor,\n                self._attributes,\n                self._set_attributes,\n                self._link,\n                self._meta,\n            )\n        )\n        return self._hash\n\n    @property\n    def color(self) -> Optional[Color]:\n        \"\"\"The foreground color or None if it is not set.\"\"\"\n        return self._color\n\n    @property\n    def bgcolor(self) -> Optional[Color]:\n        \"\"\"The background color or None if it is not set.\"\"\"\n        return self._bgcolor\n\n    @property\n    def link(self) -> Optional[str]:\n        \"\"\"Link text, if set.\"\"\"\n        return self._link\n\n    @property\n    def transparent_background(self) -> bool:\n        \"\"\"Check if the style specified a transparent background.\"\"\"\n        return self.bgcolor is None or self.bgcolor.is_default\n\n    @property\n    def background_style(self) -> \"Style\":\n        \"\"\"A Style with background only.\"\"\"\n        return Style(bgcolor=self.bgcolor)\n\n    @property\n    def meta(self) -> Dict[str, Any]:\n        \"\"\"Get meta information (can not be changed after construction).\"\"\"\n        return {} if self._meta is None else cast(Dict[str, Any], loads(self._meta))\n\n    @property\n    def without_color(self) -> \"Style\":\n        \"\"\"Get a copy of the style with color removed.\"\"\"\n        if self._null:\n            return NULL_STYLE\n        style: Style = self.__new__(Style)\n        style._ansi = None\n        style._style_definition = None\n        style._color = None\n        style._bgcolor = None\n        style._attributes = self._attributes\n        style._set_attributes = self._set_attributes\n        style._link = self._link\n        style._link_id = f\"{randint(0, 999999)}\" if self._link else \"\"\n        style._null = False\n        style._meta = None\n        style._hash = None\n        return style\n\n    @classmethod\n    @lru_cache(maxsize=4096)\n    def parse(cls, style_definition: str) -> \"Style\":\n        \"\"\"Parse a style definition.\n\n        Args:\n            style_definition (str): A string containing a style.\n\n        Raises:\n            errors.StyleSyntaxError: If the style definition syntax is invalid.\n\n        Returns:\n            `Style`: A Style instance.\n        \"\"\"\n        if style_definition.strip() == \"none\" or not style_definition:\n            return cls.null()\n\n        STYLE_ATTRIBUTES = cls.STYLE_ATTRIBUTES\n        color: Optional[str] = None\n        bgcolor: Optional[str] = None\n        attributes: Dict[str, Optional[Any]] = {}\n        link: Optional[str] = None\n\n        words = iter(style_definition.split())\n        for original_word in words:\n            word = original_word.lower()\n            if word == \"on\":\n                word = next(words, \"\")\n                if not word:\n                    raise errors.StyleSyntaxError(\"color expected after 'on'\")\n                try:\n                    Color.parse(word) is None\n                except ColorParseError as error:\n                    raise errors.StyleSyntaxError(\n                        f\"unable to parse {word!r} as background color; {error}\"\n                    ) from None\n                bgcolor = word\n\n            elif word == \"not\":\n                word = next(words, \"\")\n                attribute = STYLE_ATTRIBUTES.get(word)\n                if attribute is None:\n                    raise errors.StyleSyntaxError(\n                        f\"expected style attribute after 'not', found {word!r}\"\n                    )\n                attributes[attribute] = False\n\n            elif word == \"link\":\n                word = next(words, \"\")\n                if not word:\n                    raise errors.StyleSyntaxError(\"URL expected after 'link'\")\n                link = word\n\n            elif word in STYLE_ATTRIBUTES:\n                attributes[STYLE_ATTRIBUTES[word]] = True\n\n            else:\n                try:\n                    Color.parse(word)\n                except ColorParseError as error:\n                    raise errors.StyleSyntaxError(\n                        f\"unable to parse {word!r} as color; {error}\"\n                    ) from None\n                color = word\n        style = Style(color=color, bgcolor=bgcolor, link=link, **attributes)\n        return style\n\n    @lru_cache(maxsize=1024)\n    def get_html_style(self, theme: Optional[TerminalTheme] = None) -> str:\n        \"\"\"Get a CSS style rule.\"\"\"\n        theme = theme or DEFAULT_TERMINAL_THEME\n        css: List[str] = []\n        append = css.append\n\n        color = self.color\n        bgcolor = self.bgcolor\n        if self.reverse:\n            color, bgcolor = bgcolor, color\n        if self.dim:\n            foreground_color = (\n                theme.foreground_color if color is None else color.get_truecolor(theme)\n            )\n            color = Color.from_triplet(\n                blend_rgb(foreground_color, theme.background_color, 0.5)\n            )\n        if color is not None:\n            theme_color = color.get_truecolor(theme)\n            append(f\"color: {theme_color.hex}\")\n            append(f\"text-decoration-color: {theme_color.hex}\")\n        if bgcolor is not None:\n            theme_color = bgcolor.get_truecolor(theme, foreground=False)\n            append(f\"background-color: {theme_color.hex}\")\n        if self.bold:\n            append(\"font-weight: bold\")\n        if self.italic:\n            append(\"font-style: italic\")\n        if self.underline:\n            append(\"text-decoration: underline\")\n        if self.strike:\n            append(\"text-decoration: line-through\")\n        if self.overline:\n            append(\"text-decoration: overline\")\n        return \"; \".join(css)\n\n    @classmethod\n    def combine(cls, styles: Iterable[\"Style\"]) -> \"Style\":\n        \"\"\"Combine styles and get result.\n\n        Args:\n            styles (Iterable[Style]): Styles to combine.\n\n        Returns:\n            Style: A new style instance.\n        \"\"\"\n        iter_styles = iter(styles)\n        return sum(iter_styles, next(iter_styles))\n\n    @classmethod\n    def chain(cls, *styles: \"Style\") -> \"Style\":\n        \"\"\"Combine styles from positional argument in to a single style.\n\n        Args:\n            *styles (Iterable[Style]): Styles to combine.\n\n        Returns:\n            Style: A new style instance.\n        \"\"\"\n        iter_styles = iter(styles)\n        return sum(iter_styles, next(iter_styles))\n\n    def copy(self) -> \"Style\":\n        \"\"\"Get a copy of this style.\n\n        Returns:\n            Style: A new Style instance with identical attributes.\n        \"\"\"\n        if self._null:\n            return NULL_STYLE\n        style: Style = self.__new__(Style)\n        style._ansi = self._ansi\n        style._style_definition = self._style_definition\n        style._color = self._color\n        style._bgcolor = self._bgcolor\n        style._attributes = self._attributes\n        style._set_attributes = self._set_attributes\n        style._link = self._link\n        style._link_id = f\"{randint(0, 999999)}\" if self._link else \"\"\n        style._hash = self._hash\n        style._null = False\n        style._meta = self._meta\n        return style\n\n    @lru_cache(maxsize=128)\n    def clear_meta_and_links(self) -> \"Style\":\n        \"\"\"Get a copy of this style with link and meta information removed.\n\n        Returns:\n            Style: New style object.\n        \"\"\"\n        if self._null:\n            return NULL_STYLE\n        style: Style = self.__new__(Style)\n        style._ansi = self._ansi\n        style._style_definition = self._style_definition\n        style._color = self._color\n        style._bgcolor = self._bgcolor\n        style._attributes = self._attributes\n        style._set_attributes = self._set_attributes\n        style._link = None\n        style._link_id = \"\"\n        style._hash = self._hash\n        style._null = False\n        style._meta = None\n        return style\n\n    def update_link(self, link: Optional[str] = None) -> \"Style\":\n        \"\"\"Get a copy with a different value for link.\n\n        Args:\n            link (str, optional): New value for link. Defaults to None.\n\n        Returns:\n            Style: A new Style instance.\n        \"\"\"\n        style: Style = self.__new__(Style)\n        style._ansi = self._ansi\n        style._style_definition = self._style_definition\n        style._color = self._color\n        style._bgcolor = self._bgcolor\n        style._attributes = self._attributes\n        style._set_attributes = self._set_attributes\n        style._link = link\n        style._link_id = f\"{randint(0, 999999)}\" if link else \"\"\n        style._hash = None\n        style._null = False\n        style._meta = self._meta\n        return style\n\n    def render(\n        self,\n        text: str = \"\",\n        *,\n        color_system: Optional[ColorSystem] = ColorSystem.TRUECOLOR,\n        legacy_windows: bool = False,\n    ) -> str:\n        \"\"\"Render the ANSI codes for the style.\n\n        Args:\n            text (str, optional): A string to style. Defaults to \"\".\n            color_system (Optional[ColorSystem], optional): Color system to render to. Defaults to ColorSystem.TRUECOLOR.\n\n        Returns:\n            str: A string containing ANSI style codes.\n        \"\"\"\n        if not text or color_system is None:\n            return text\n        attrs = self._ansi or self._make_ansi_codes(color_system)\n        rendered = f\"\\x1b[{attrs}m{text}\\x1b[0m\" if attrs else text\n        if self._link and not legacy_windows:\n            rendered = (\n                f\"\\x1b]8;id={self._link_id};{self._link}\\x1b\\\\{rendered}\\x1b]8;;\\x1b\\\\\"\n            )\n        return rendered\n\n    def test(self, text: Optional[str] = None) -> None:\n        \"\"\"Write text with style directly to terminal.\n\n        This method is for testing purposes only.\n\n        Args:\n            text (Optional[str], optional): Text to style or None for style name.\n\n        \"\"\"\n        text = text or str(self)\n        sys.stdout.write(f\"{self.render(text)}"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py",
      "line_number": 1,
      "details": "import os.path\nimport platform\nimport re\nimport sys\nimport textwrap\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import (\n    Any,\n    Dict,\n    Iterable,\n    List,\n    NamedTuple,\n    Optional,\n    Sequence,\n    Set,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom pip._vendor.pygments.lexer import Lexer\nfrom pip._vendor.pygments.lexers import get_lexer_by_name, guess_lexer_for_filename\nfrom pip._vendor.pygments.style import Style as PygmentsStyle\nfrom pip._vendor.pygments.styles import get_style_by_name\nfrom pip._vendor.pygments.token import (\n    Comment,\n    Error,\n    Generic,\n    Keyword,\n    Name,\n    Number,\n    Operator,\n    String,\n    Token,\n    Whitespace,\n)\nfrom pip._vendor.pygments.util import ClassNotFound\n\nfrom pip._vendor.rich.containers import Lines\nfrom pip._vendor.rich.padding import Padding, PaddingDimensions\n\nfrom ._loop import loop_first\nfrom .cells import cell_len\nfrom .color import Color, blend_rgb\nfrom .console import Console, ConsoleOptions, JustifyMethod, RenderResult\nfrom .jupyter import JupyterMixin\nfrom .measure import Measurement\nfrom .segment import Segment, Segments\nfrom .style import Style, StyleType\nfrom .text import Text\n\nTokenType = Tuple[str, ...]\n\nWINDOWS = platform.system() == \"Windows\"\nDEFAULT_THEME = \"monokai\"\n\n# The following styles are based on https://github.com/pygments/pygments/blob/master/pygments/formatters/terminal.py\n# A few modifications were made\n\nANSI_LIGHT: Dict[TokenType, Style] = {\n    Token: Style(),\n    Whitespace: Style(color=\"white\"),\n    Comment: Style(dim=True),\n    Comment.Preproc: Style(color=\"cyan\"),\n    Keyword: Style(color=\"blue\"),\n    Keyword.Type: Style(color=\"cyan\"),\n    Operator.Word: Style(color=\"magenta\"),\n    Name.Builtin: Style(color=\"cyan\"),\n    Name.Function: Style(color=\"green\"),\n    Name.Namespace: Style(color=\"cyan\", underline=True),\n    Name.Class: Style(color=\"green\", underline=True),\n    Name.Exception: Style(color=\"cyan\"),\n    Name.Decorator: Style(color=\"magenta\", bold=True),\n    Name.Variable: Style(color=\"red\"),\n    Name.Constant: Style(color=\"red\"),\n    Name.Attribute: Style(color=\"cyan\"),\n    Name.Tag: Style(color=\"bright_blue\"),\n    String: Style(color=\"yellow\"),\n    Number: Style(color=\"blue\"),\n    Generic.Deleted: Style(color=\"bright_red\"),\n    Generic.Inserted: Style(color=\"green\"),\n    Generic.Heading: Style(bold=True),\n    Generic.Subheading: Style(color=\"magenta\", bold=True),\n    Generic.Prompt: Style(bold=True),\n    Generic.Error: Style(color=\"bright_red\"),\n    Error: Style(color=\"red\", underline=True),\n}\n\nANSI_DARK: Dict[TokenType, Style] = {\n    Token: Style(),\n    Whitespace: Style(color=\"bright_black\"),\n    Comment: Style(dim=True),\n    Comment.Preproc: Style(color=\"bright_cyan\"),\n    Keyword: Style(color=\"bright_blue\"),\n    Keyword.Type: Style(color=\"bright_cyan\"),\n    Operator.Word: Style(color=\"bright_magenta\"),\n    Name.Builtin: Style(color=\"bright_cyan\"),\n    Name.Function: Style(color=\"bright_green\"),\n    Name.Namespace: Style(color=\"bright_cyan\", underline=True),\n    Name.Class: Style(color=\"bright_green\", underline=True),\n    Name.Exception: Style(color=\"bright_cyan\"),\n    Name.Decorator: Style(color=\"bright_magenta\", bold=True),\n    Name.Variable: Style(color=\"bright_red\"),\n    Name.Constant: Style(color=\"bright_red\"),\n    Name.Attribute: Style(color=\"bright_cyan\"),\n    Name.Tag: Style(color=\"bright_blue\"),\n    String: Style(color=\"yellow\"),\n    Number: Style(color=\"bright_blue\"),\n    Generic.Deleted: Style(color=\"bright_red\"),\n    Generic.Inserted: Style(color=\"bright_green\"),\n    Generic.Heading: Style(bold=True),\n    Generic.Subheading: Style(color=\"bright_magenta\", bold=True),\n    Generic.Prompt: Style(bold=True),\n    Generic.Error: Style(color=\"bright_red\"),\n    Error: Style(color=\"red\", underline=True),\n}\n\nRICH_SYNTAX_THEMES = {\"ansi_light\": ANSI_LIGHT, \"ansi_dark\": ANSI_DARK}\nNUMBERS_COLUMN_DEFAULT_PADDING = 2\n\n\nclass SyntaxTheme(ABC):\n    \"\"\"Base class for a syntax theme.\"\"\"\n\n    @abstractmethod\n    def get_style_for_token(self, token_type: TokenType) -> Style:\n        \"\"\"Get a style for a given Pygments token.\"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n    @abstractmethod\n    def get_background_style(self) -> Style:\n        \"\"\"Get the background color.\"\"\"\n        raise NotImplementedError  # pragma: no cover\n\n\nclass PygmentsSyntaxTheme(SyntaxTheme):\n    \"\"\"Syntax theme that delegates to Pygments theme.\"\"\"\n\n    def __init__(self, theme: Union[str, Type[PygmentsStyle]]) -> None:\n        self._style_cache: Dict[TokenType, Style] = {}\n        if isinstance(theme, str):\n            try:\n                self._pygments_style_class = get_style_by_name(theme)\n            except ClassNotFound:\n                self._pygments_style_class = get_style_by_name(\"default\")\n        else:\n            self._pygments_style_class = theme\n\n        self._background_color = self._pygments_style_class.background_color\n        self._background_style = Style(bgcolor=self._background_color)\n\n    def get_style_for_token(self, token_type: TokenType) -> Style:\n        \"\"\"Get a style from a Pygments class.\"\"\"\n        try:\n            return self._style_cache[token_type]\n        except KeyError:\n            try:\n                pygments_style = self._pygments_style_class.style_for_token(token_type)\n            except KeyError:\n                style = Style.null()\n            else:\n                color = pygments_style[\"color\"]\n                bgcolor = pygments_style[\"bgcolor\"]\n                style = Style(\n                    color=\"#\" + color if color else \"#000000\",\n                    bgcolor=\"#\" + bgcolor if bgcolor else self._background_color,\n                    bold=pygments_style[\"bold\"],\n                    italic=pygments_style[\"italic\"],\n                    underline=pygments_style[\"underline\"],\n                )\n            self._style_cache[token_type] = style\n        return style\n\n    def get_background_style(self) -> Style:\n        return self._background_style\n\n\nclass ANSISyntaxTheme(SyntaxTheme):\n    \"\"\"Syntax theme to use standard colors.\"\"\"\n\n    def __init__(self, style_map: Dict[TokenType, Style]) -> None:\n        self.style_map = style_map\n        self._missing_style = Style.null()\n        self._background_style = Style.null()\n        self._style_cache: Dict[TokenType, Style] = {}\n\n    def get_style_for_token(self, token_type: TokenType) -> Style:\n        \"\"\"Look up style in the style map.\"\"\"\n        try:\n            return self._style_cache[token_type]\n        except KeyError:\n            # Styles form a hierarchy\n            # We need to go from most to least specific\n            # e.g. (\"foo\", \"bar\", \"baz\") to (\"foo\", \"bar\")  to (\"foo\",)\n            get_style = self.style_map.get\n            token = tuple(token_type)\n            style = self._missing_style\n            while token:\n                _style = get_style(token)\n                if _style is not None:\n                    style = _style\n                    break\n                token = token[:-1]\n            self._style_cache[token_type] = style\n            return style\n\n    def get_background_style(self) -> Style:\n        return self._background_style\n\n\nSyntaxPosition = Tuple[int, int]\n\n\nclass _SyntaxHighlightRange(NamedTuple):\n    \"\"\"\n    A range to highlight in a Syntax object.\n    `start` and `end` are 2-integers tuples, where the first integer is the line number\n    (starting from 1) and the second integer is the column index (starting from 0).\n    \"\"\"\n\n    style: StyleType\n    start: SyntaxPosition\n    end: SyntaxPosition\n\n\nclass Syntax(JupyterMixin):\n    \"\"\"Construct a Syntax object to render syntax highlighted code.\n\n    Args:\n        code (str): Code to highlight.\n        lexer (Lexer | str): Lexer to use (see https://pygments.org/docs/lexers/)\n        theme (str, optional): Color theme, aka Pygments style (see https://pygments.org/docs/styles/#getting-a-list-of-available-styles). Defaults to \"monokai\".\n        dedent (bool, optional): Enable stripping of initial whitespace. Defaults to False.\n        line_numbers (bool, optional): Enable rendering of line numbers. Defaults to False.\n        start_line (int, optional): Starting number for line numbers. Defaults to 1.\n        line_range (Tuple[int | None, int | None], optional): If given should be a tuple of the start and end line to render.\n            A value of None in the tuple indicates the range is open in that direction.\n        highlight_lines (Set[int]): A set of line numbers to highlight.\n        code_width: Width of code to render (not including line numbers), or ``None`` to use all available width.\n        tab_size (int, optional): Size of tabs. Defaults to 4.\n        word_wrap (bool, optional): Enable word wrapping.\n        background_color (str, optional): Optional background color, or None to use theme color. Defaults to None.\n        indent_guides (bool, optional): Show indent guides. Defaults to False.\n        padding (PaddingDimensions): Padding to apply around the syntax. Defaults to 0 (no padding).\n    \"\"\"\n\n    _pygments_style_class: Type[PygmentsStyle]\n    _theme: SyntaxTheme\n\n    @classmethod\n    def get_theme(cls, name: Union[str, SyntaxTheme]) -> SyntaxTheme:\n        \"\"\"Get a syntax theme instance.\"\"\"\n        if isinstance(name, SyntaxTheme):\n            return name\n        theme: SyntaxTheme\n        if name in RICH_SYNTAX_THEMES:\n            theme = ANSISyntaxTheme(RICH_SYNTAX_THEMES[name])\n        else:\n            theme = PygmentsSyntaxTheme(name)\n        return theme\n\n    def __init__(\n        self,\n        code: str,\n        lexer: Union[Lexer, str],\n        *,\n        theme: Union[str, SyntaxTheme] = DEFAULT_THEME,\n        dedent: bool = False,\n        line_numbers: bool = False,\n        start_line: int = 1,\n        line_range: Optional[Tuple[Optional[int], Optional[int]]] = None,\n        highlight_lines: Optional[Set[int]] = None,\n        code_width: Optional[int] = None,\n        tab_size: int = 4,\n        word_wrap: bool = False,\n        background_color: Optional[str] = None,\n        indent_guides: bool = False,\n        padding: PaddingDimensions = 0,\n    ) -> None:\n        self.code = code\n        self._lexer = lexer\n        self.dedent = dedent\n        self.line_numbers = line_numbers\n        self.start_line = start_line\n        self.line_range = line_range\n        self.highlight_lines = highlight_lines or set()\n        self.code_width = code_width\n        self.tab_size = tab_size\n        self.word_wrap = word_wrap\n        self.background_color = background_color\n        self.background_style = (\n            Style(bgcolor=background_color) if background_color else Style()\n        )\n        self.indent_guides = indent_guides\n        self.padding = padding\n\n        self._theme = self.get_theme(theme)\n        self._stylized_ranges: List[_SyntaxHighlightRange] = []\n\n    @classmethod\n    def from_path(\n        cls,\n        path: str,\n        encoding: str = \"utf-8\",\n        lexer: Optional[Union[Lexer, str]] = None,\n        theme: Union[str, SyntaxTheme] = DEFAULT_THEME,\n        dedent: bool = False,\n        line_numbers: bool = False,\n        line_range: Optional[Tuple[int, int]] = None,\n        start_line: int = 1,\n        highlight_lines: Optional[Set[int]] = None,\n        code_width: Optional[int] = None,\n        tab_size: int = 4,\n        word_wrap: bool = False,\n        background_color: Optional[str] = None,\n        indent_guides: bool = False,\n        padding: PaddingDimensions = 0,\n    ) -> \"Syntax\":\n        \"\"\"Construct a Syntax object from a file.\n\n        Args:\n            path (str): Path to file to highlight.\n            encoding (str): Encoding of file.\n            lexer (str | Lexer, optional): Lexer to use. If None, lexer will be auto-detected from path/file content.\n            theme (str, optional): Color theme, aka Pygments style (see https://pygments.org/docs/styles/#getting-a-list-of-available-styles). Defaults to \"emacs\".\n            dedent (bool, optional): Enable stripping of initial whitespace. Defaults to True.\n            line_numbers (bool, optional): Enable rendering of line numbers. Defaults to False.\n            start_line (int, optional): Starting number for line numbers. Defaults to 1.\n            line_range (Tuple[int, int], optional): If given should be a tuple of the start and end line to render.\n            highlight_lines (Set[int]): A set of line numbers to highlight.\n            code_width: Width of code to render (not including line numbers), or ``None`` to use all available width.\n            tab_size (int, optional): Size of tabs. Defaults to 4.\n            word_wrap (bool, optional): Enable word wrapping of code.\n            background_color (str, optional): Optional background color, or None to use theme color. Defaults to None.\n            indent_guides (bool, optional): Show indent guides. Defaults to False.\n            padding (PaddingDimensions): Padding to apply around the syntax. Defaults to 0 (no padding).\n\n        Returns:\n            [Syntax]: A Syntax object that may be printed to the console\n        \"\"\"\n        code = Path(path).read_text(encoding=encoding)\n\n        if not lexer:\n            lexer = cls.guess_lexer(path, code=code)\n\n        return cls(\n            code,\n            lexer,\n            theme=theme,\n            dedent=dedent,\n            line_numbers=line_numbers,\n            line_range=line_range,\n            start_line=start_line,\n            highlight_lines=highlight_lines,\n            code_width=code_width,\n            tab_size=tab_size,\n            word_wrap=word_wrap,\n            background_color=background_color,\n            indent_guides=indent_guides,\n            padding=padding,\n        )\n\n    @classmethod\n    def guess_lexer(cls, path: str, code: Optional[str] = None) -> str:\n        \"\"\"Guess the alias of the Pygments lexer to use based on a path and an optional string of code.\n        If code is supplied, it will use a combination of the code and the filename to determine the\n        best lexer to use. For example, if the file is ``index.html`` and the file contains Django\n        templating syntax, then \"html+django\" will be returned. If the file is ``index.html``, and no\n        templating language is used, the \"html\" lexer will be used. If no string of code\n        is supplied, the lexer will be chosen based on the file extension..\n\n        Args:\n             path (AnyStr): The path to the file containing the code you wish to know the lexer for.\n             code (str, optional): Optional string of code that will be used as a fallback if no lexer\n                is found for the supplied path.\n\n        Returns:\n            str: The name of the Pygments lexer that best matches the supplied path/code.\n        \"\"\"\n        lexer: Optional[Lexer] = None\n        lexer_name = \"default\"\n        if code:\n            try:\n                lexer = guess_lexer_for_filename(path, code)\n            except ClassNotFound:\n                pass\n\n        if not lexer:\n            try:\n                _, ext = os.path.splitext(path)\n                if ext:\n                    extension = ext.lstrip(\".\").lower()\n                    lexer = get_lexer_by_name(extension)\n            except ClassNotFound:\n                pass\n\n        if lexer:\n            if lexer.aliases:\n                lexer_name = lexer.aliases[0]\n            else:\n                lexer_name = lexer.name\n\n        return lexer_name\n\n    def _get_base_style(self) -> Style:\n        \"\"\"Get the base style.\"\"\"\n        default_style = self._theme.get_background_style() + self.background_style\n        return default_style\n\n    def _get_token_color(self, token_type: TokenType) -> Optional[Color]:\n        \"\"\"Get a color (if any) for the given token.\n\n        Args:\n            token_type (TokenType): A token type tuple from Pygments.\n\n        Returns:\n            Optional[Color]: Color from theme, or None for no color.\n        \"\"\"\n        style = self._theme.get_style_for_token(token_type)\n        return style.color\n\n    @property\n    def lexer(self) -> Optional[Lexer]:\n        \"\"\"The lexer for this syntax, or None if no lexer was found.\n\n        Tries to find the lexer by name if a string was passed to the constructor.\n        \"\"\"\n\n        if isinstance(self._lexer, Lexer):\n            return self._lexer\n        try:\n            return get_lexer_by_name(\n                self._lexer,\n                stripnl=False,\n                ensurenl=True,\n                tabsize=self.tab_size,\n            )\n        except ClassNotFound:\n            return None\n\n    def highlight(\n        self,\n        code: str,\n        line_range: Optional[Tuple[Optional[int], Optional[int]]] = None,\n    ) -> Text:\n        \"\"\"Highlight code and return a Text instance.\n\n        Args:\n            code (str): Code to highlight.\n            line_range(Tuple[int, int], optional): Optional line range to highlight.\n\n        Returns:\n            Text: A text instance containing highlighted syntax.\n        \"\"\"\n\n        base_style = self._get_base_style()\n        justify: JustifyMethod = (\n            \"default\" if base_style.transparent_background else \"left\"\n        )\n\n        text = Text(\n            justify=justify,\n            style=base_style,\n            tab_size=self.tab_size,\n            no_wrap=not self.word_wrap,\n        )\n        _get_theme_style = self._theme.get_style_for_token\n\n        lexer = self.lexer\n\n        if lexer is None:\n            text.append(code)\n        else:\n            if line_range:\n                # More complicated path to only stylize a portion of the code\n                # This speeds up further operations as there are less spans to process\n                line_start, line_end = line_range\n\n                def line_tokenize() -> Iterable[Tuple[Any, str]]:\n                    \"\"\"Split tokens to one per line.\"\"\"\n                    assert lexer  # required to make MyPy happy - we know lexer is not None at this point\n\n                    for token_type, token in lexer.get_tokens(code):\n                        while token:\n                            line_token, new_line, token = token.partition(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py",
      "line_number": 6,
      "details": "\")\n            # Simple case of just rendering text\n            style = (\n                self._get_base_style()\n                + self._theme.get_style_for_token(Comment)\n                + Style(dim=True)\n                + self.background_style\n            )\n            if self.indent_guides and not options.ascii_only:\n                text = text.with_indent_guides(self.tab_size, style=style)\n                text.overflow = \"crop\"\n            if style.transparent_background:\n                yield from console.render(\n                    text, options=options.update(width=code_width)\n                )\n            else:\n                syntax_lines = console.render_lines(\n                    text,\n                    options.update(width=code_width, height=None, justify=\"left\"),\n                    style=self.background_style,\n                    pad=True,\n                    new_lines=True,\n                )\n                for syntax_line in syntax_lines:\n                    yield from syntax_line\n            return\n\n        start_line, end_line = self.line_range or (None, None)\n        line_offset = 0\n        if start_line:\n            line_offset = max(0, start_line - 1)\n        lines: Union[List[Text], Lines] = text.split(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/syntax.py",
      "line_number": 8,
      "details": "\")\n                .join(lines)\n                .with_indent_guides(self.tab_size, style=style + Style(italic=False))\n                .split(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/text.py",
      "line_number": 3,
      "details": "\",\n        tab_size: Optional[int] = 8,\n        spans: Optional[List[Span]] = None,\n    ) -> None:\n        sanitized_text = strip_control_codes(text)\n        self._text = [sanitized_text]\n        self.style = style\n        self.justify: Optional[\"JustifyMethod\"] = justify\n        self.overflow: Optional[\"OverflowMethod\"] = overflow\n        self.no_wrap = no_wrap\n        self.end = end\n        self.tab_size = tab_size\n        self._spans: List[Span] = spans or []\n        self._length: int = len(sanitized_text)\n\n    def __len__(self) -> int:\n        return self._length\n\n    def __bool__(self) -> bool:\n        return bool(self._length)\n\n    def __str__(self) -> str:\n        return self.plain\n\n    def __repr__(self) -> str:\n        return f\"<text {self.plain!r} {self._spans!r}>\"\n\n    def __add__(self, other: Any) -> \"Text\":\n        if isinstance(other, (str, Text)):\n            result = self.copy()\n            result.append(other)\n            return result\n        return NotImplemented\n\n    def __eq__(self, other: object) -> bool:\n        if not isinstance(other, Text):\n            return NotImplemented\n        return self.plain == other.plain and self._spans == other._spans\n\n    def __contains__(self, other: object) -> bool:\n        if isinstance(other, str):\n            return other in self.plain\n        elif isinstance(other, Text):\n            return other.plain in self.plain\n        return False\n\n    def __getitem__(self, slice: Union[int, slice]) -> \"Text\":\n        def get_text_at(offset: int) -> \"Text\":\n            _Span = Span\n            text = Text(\n                self.plain[offset],\n                spans=[\n                    _Span(0, 1, style)\n                    for start, end, style in self._spans\n                    if end > offset >= start\n                ],\n                end=\"\",\n            )\n            return text\n\n        if isinstance(slice, int):\n            return get_text_at(slice)\n        else:\n            start, stop, step = slice.indices(len(self.plain))\n            if step == 1:\n                lines = self.divide([start, stop])\n                return lines[1]\n            else:\n                # This would be a bit of work to implement efficiently\n                # For now, its not required\n                raise TypeError(\"slices with step!=1 are not supported\")\n\n    @property\n    def cell_len(self) -> int:\n        \"\"\"Get the number of cells required to render this text.\"\"\"\n        return cell_len(self.plain)\n\n    @property\n    def markup(self) -> str:\n        \"\"\"Get console markup to render this Text.\n\n        Returns:\n            str: A string potentially creating markup tags.\n        \"\"\"\n        from .markup import escape\n\n        output: List[str] = []\n\n        plain = self.plain\n        markup_spans = [\n            (0, False, self.style),\n            *((span.start, False, span.style) for span in self._spans),\n            *((span.end, True, span.style) for span in self._spans),\n            (len(plain), True, self.style),\n        ]\n        markup_spans.sort(key=itemgetter(0, 1))\n        position = 0\n        append = output.append\n        for offset, closing, style in markup_spans:\n            if offset > position:\n                append(escape(plain[position:offset]))\n                position = offset\n            if style:\n                append(f\"[/{style}]\" if closing else f\"[{style}]\")\n        markup = \"\".join(output)\n        return markup\n\n    @classmethod\n    def from_markup(\n        cls,\n        text: str,\n        *,\n        style: Union[str, Style] = \"\",\n        emoji: bool = True,\n        emoji_variant: Optional[EmojiVariant] = None,\n        justify: Optional[\"JustifyMethod\"] = None,\n        overflow: Optional[\"OverflowMethod\"] = None,\n        end: str = \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/text.py",
      "line_number": 10,
      "details": "\".\n            tab_size (int): Number of spaces per tab, or ``None`` to use ``console.tab_size``. Defaults to 8.\n            meta (Dict[str, Any], optional). Meta data to apply to text, or None for no meta data. Default to None\n\n        Returns:\n            Text: A new text instance.\n        \"\"\"\n        text = cls(\n            style=style,\n            justify=justify,\n            overflow=overflow,\n            no_wrap=no_wrap,\n            end=end,\n            tab_size=tab_size,\n        )\n        append = text.append\n        _Text = Text\n        for part in parts:\n            if isinstance(part, (_Text, str)):\n                append(part)\n            else:\n                append(*part)\n        if meta:\n            text.apply_meta(meta)\n        return text\n\n    @property\n    def plain(self) -> str:\n        \"\"\"Get the text as a single string.\"\"\"\n        if len(self._text) != 1:\n            self._text[:] = [\"\".join(self._text)]\n        return self._text[0]\n\n    @plain.setter\n    def plain(self, new_text: str) -> None:\n        \"\"\"Set the text to a new value.\"\"\"\n        if new_text != self.plain:\n            sanitized_text = strip_control_codes(new_text)\n            self._text[:] = [sanitized_text]\n            old_length = self._length\n            self._length = len(sanitized_text)\n            if old_length > self._length:\n                self._trim_spans()\n\n    @property\n    def spans(self) -> List[Span]:\n        \"\"\"Get a reference to the internal list of spans.\"\"\"\n        return self._spans\n\n    @spans.setter\n    def spans(self, spans: List[Span]) -> None:\n        \"\"\"Set spans.\"\"\"\n        self._spans = spans[:]\n\n    def blank_copy(self, plain: str = \"\") -> \"Text\":\n        \"\"\"Return a new Text instance with copied meta data (but not the string or spans).\"\"\"\n        copy_self = Text(\n            plain,\n            style=self.style,\n            justify=self.justify,\n            overflow=self.overflow,\n            no_wrap=self.no_wrap,\n            end=self.end,\n            tab_size=self.tab_size,\n        )\n        return copy_self\n\n    def copy(self) -> \"Text\":\n        \"\"\"Return a copy of this instance.\"\"\"\n        copy_self = Text(\n            self.plain,\n            style=self.style,\n            justify=self.justify,\n            overflow=self.overflow,\n            no_wrap=self.no_wrap,\n            end=self.end,\n            tab_size=self.tab_size,\n        )\n        copy_self._spans[:] = self._spans\n        return copy_self\n\n    def stylize(\n        self,\n        style: Union[str, Style],\n        start: int = 0,\n        end: Optional[int] = None,\n    ) -> None:\n        \"\"\"Apply a style to the text, or a portion of the text.\n\n        Args:\n            style (Union[str, Style]): Style instance or style definition to apply.\n            start (int): Start offset (negative indexing is supported). Defaults to 0.\n            end (Optional[int], optional): End offset (negative indexing is supported), or None for end of text. Defaults to None.\n        \"\"\"\n        if style:\n            length = len(self)\n            if start < 0:\n                start = length + start\n            if end is None:\n                end = length\n            if end < 0:\n                end = length + end\n            if start >= length or end <= start:\n                # Span not in text or not valid\n                return\n            self._spans.append(Span(start, min(length, end), style))\n\n    def stylize_before(\n        self,\n        style: Union[str, Style],\n        start: int = 0,\n        end: Optional[int] = None,\n    ) -> None:\n        \"\"\"Apply a style to the text, or a portion of the text. Styles will be applied before other styles already present.\n\n        Args:\n            style (Union[str, Style]): Style instance or style definition to apply.\n            start (int): Start offset (negative indexing is supported). Defaults to 0.\n            end (Optional[int], optional): End offset (negative indexing is supported), or None for end of text. Defaults to None.\n        \"\"\"\n        if style:\n            length = len(self)\n            if start < 0:\n                start = length + start\n            if end is None:\n                end = length\n            if end < 0:\n                end = length + end\n            if start >= length or end <= start:\n                # Span not in text or not valid\n                return\n            self._spans.insert(0, Span(start, min(length, end), style))\n\n    def apply_meta(\n        self, meta: Dict[str, Any], start: int = 0, end: Optional[int] = None\n    ) -> None:\n        \"\"\"Apply meta data to the text, or a portion of the text.\n\n        Args:\n            meta (Dict[str, Any]): A dict of meta information.\n            start (int): Start offset (negative indexing is supported). Defaults to 0.\n            end (Optional[int], optional): End offset (negative indexing is supported), or None for end of text. Defaults to None.\n\n        \"\"\"\n        style = Style.from_meta(meta)\n        self.stylize(style, start=start, end=end)\n\n    def on(self, meta: Optional[Dict[str, Any]] = None, **handlers: Any) -> \"Text\":\n        \"\"\"Apply event handlers (used by Textual project).\n\n        Example:\n            >>> from rich.text import Text\n            >>> text = Text(\"hello world\")\n            >>> text.on(click=\"view.toggle('world')\")\n\n        Args:\n            meta (Dict[str, Any]): Mapping of meta information.\n            **handlers: Keyword args are prefixed with \"@\" to defined handlers.\n\n        Returns:\n            Text: Self is returned to method may be chained.\n        \"\"\"\n        meta = {} if meta is None else meta\n        meta.update({f\"@{key}\": value for key, value in handlers.items()})\n        self.stylize(Style.from_meta(meta))\n        return self\n\n    def remove_suffix(self, suffix: str) -> None:\n        \"\"\"Remove a suffix if it exists.\n\n        Args:\n            suffix (str): Suffix to remove.\n        \"\"\"\n        if self.plain.endswith(suffix):\n            self.right_crop(len(suffix))\n\n    def get_style_at_offset(self, console: \"Console\", offset: int) -> Style:\n        \"\"\"Get the style of a character at give offset.\n\n        Args:\n            console (~Console): Console where text will be rendered.\n            offset (int): Offset in to text (negative indexing supported)\n\n        Returns:\n            Style: A Style instance.\n        \"\"\"\n        # TODO: This is a little inefficient, it is only used by full justify\n        if offset < 0:\n            offset = len(self) + offset\n        get_style = console.get_style\n        style = get_style(self.style).copy()\n        for start, end, span_style in self._spans:\n            if end > offset >= start:\n                style += get_style(span_style, default=\"\")\n        return style\n\n    def highlight_regex(\n        self,\n        re_highlight: str,\n        style: Optional[Union[GetStyleCallable, StyleType]] = None,\n        *,\n        style_prefix: str = \"\",\n    ) -> int:\n        \"\"\"Highlight text with a regular expression, where group names are\n        translated to styles.\n\n        Args:\n            re_highlight (str): A regular expression.\n            style (Union[GetStyleCallable, StyleType]): Optional style to apply to whole match, or a callable\n                which accepts the matched text and returns a style. Defaults to None.\n            style_prefix (str, optional): Optional prefix to add to style group names.\n\n        Returns:\n            int: Number of regex matches\n        \"\"\"\n        count = 0\n        append_span = self._spans.append\n        _Span = Span\n        plain = self.plain\n        for match in re.finditer(re_highlight, plain):\n            get_span = match.span\n            if style:\n                start, end = get_span()\n                match_style = style(plain[start:end]) if callable(style) else style\n                if match_style is not None and end > start:\n                    append_span(_Span(start, end, match_style))\n\n            count += 1\n            for name in match.groupdict().keys():\n                start, end = get_span(name)\n                if start != -1 and end > start:\n                    append_span(_Span(start, end, f\"{style_prefix}{name}\"))\n        return count\n\n    def highlight_words(\n        self,\n        words: Iterable[str],\n        style: Union[str, Style],\n        *,\n        case_sensitive: bool = True,\n    ) -> int:\n        \"\"\"Highlight words with a style.\n\n        Args:\n            words (Iterable[str]): Worlds to highlight.\n            style (Union[str, Style]): Style to apply.\n            case_sensitive (bool, optional): Enable case sensitive matchings. Defaults to True.\n\n        Returns:\n            int: Number of words highlighted.\n        \"\"\"\n        re_words = \"|\".join(re.escape(word) for word in words)\n        add_span = self._spans.append\n        count = 0\n        _Span = Span\n        for match in re.finditer(\n            re_words, self.plain, flags=0 if case_sensitive else re.IGNORECASE\n        ):\n            start, end = match.span(0)\n            add_span(_Span(start, end, style))\n            count += 1\n        return count\n\n    def rstrip(self) -> None:\n        \"\"\"Strip whitespace from end of text.\"\"\"\n        self.plain = self.plain.rstrip()\n\n    def rstrip_end(self, size: int) -> None:\n        \"\"\"Remove whitespace beyond a certain width at the end of the text.\n\n        Args:\n            size (int): The desired size of the text.\n        \"\"\"\n        text_length = len(self)\n        if text_length > size:\n            excess = text_length - size\n            whitespace_match = _re_whitespace.search(self.plain)\n            if whitespace_match is not None:\n                whitespace_count = len(whitespace_match.group(0))\n                self.right_crop(min(whitespace_count, excess))\n\n    def set_length(self, new_length: int) -> None:\n        \"\"\"Set new length of the text, clipping or padding is required.\"\"\"\n        length = len(self)\n        if length != new_length:\n            if length < new_length:\n                self.pad_right(new_length - length)\n            else:\n                self.right_crop(length - new_length)\n\n    def __rich_console__(\n        self, console: \"Console\", options: \"ConsoleOptions\"\n    ) -> Iterable[Segment]:\n        tab_size: int = console.tab_size or self.tab_size or 8\n        justify = self.justify or options.justify or DEFAULT_JUSTIFY\n\n        overflow = self.overflow or options.overflow or DEFAULT_OVERFLOW\n\n        lines = self.wrap(\n            console,\n            options.max_width,\n            justify=justify,\n            overflow=overflow,\n            tab_size=tab_size or 8,\n            no_wrap=pick_bool(self.no_wrap, options.no_wrap, False),\n        )\n        all_lines = Text(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/text.py",
      "line_number": 12,
      "details": "\", include_separator=True):\n            parts = line.split(\"\\t\", include_separator=True)\n            for part in parts:\n                if part.plain.endswith(\"\\t\"):\n                    part._text = [part.plain[:-1] + \" \"]\n                    append(part)\n                    pos += len(part)\n                    spaces = tab_size - ((pos - 1) % tab_size) - 1\n                    if spaces:\n                        append(\" \" * spaces, _style)\n                        pos += spaces\n                else:\n                    append(part)\n        self._text = [result.plain]\n        self._length = len(self.plain)\n        self._spans[:] = result._spans\n\n    def truncate(\n        self,\n        max_width: int,\n        *,\n        overflow: Optional[\"OverflowMethod\"] = None,\n        pad: bool = False,\n    ) -> None:\n        \"\"\"Truncate text if it is longer that a given width.\n\n        Args:\n            max_width (int): Maximum number of characters in text.\n            overflow (str, optional): Overflow method: \"crop\", \"fold\", or \"ellipsis\". Defaults to None, to use self.overflow.\n            pad (bool, optional): Pad with spaces if the length is less than max_width. Defaults to False.\n        \"\"\"\n        _overflow = overflow or self.overflow or DEFAULT_OVERFLOW\n        if _overflow != \"ignore\":\n            length = cell_len(self.plain)\n            if length > max_width:\n                if _overflow == \"ellipsis\":\n                    self.plain = set_cell_size(self.plain, max_width - 1) + \"\u2026\"\n                else:\n                    self.plain = set_cell_size(self.plain, max_width)\n            if pad and length < max_width:\n                spaces = max_width - length\n                self._text = [f\"{self.plain}{' ' * spaces}\"]\n                self._length = len(self.plain)\n\n    def _trim_spans(self) -> None:\n        \"\"\"Remove or modify any spans that are over the end of the text.\"\"\"\n        max_offset = len(self.plain)\n        _Span = Span\n        self._spans[:] = [\n            (\n                span\n                if span.end < max_offset\n                else _Span(span.start, min(max_offset, span.end), span.style)\n            )\n            for span in self._spans\n            if span.start < max_offset\n        ]\n\n    def pad(self, count: int, character: str = \" \") -> None:\n        \"\"\"Pad left and right with a given number of characters.\n\n        Args:\n            count (int): Width of padding.\n        \"\"\"\n        assert len(character) == 1, \"Character must be a string of length 1\"\n        if count:\n            pad_characters = character * count\n            self.plain = f\"{pad_characters}{self.plain}{pad_characters}\"\n            _Span = Span\n            self._spans[:] = [\n                _Span(start + count, end + count, style)\n                for start, end, style in self._spans\n            ]\n\n    def pad_left(self, count: int, character: str = \" \") -> None:\n        \"\"\"Pad the left with a given character.\n\n        Args:\n            count (int): Number of characters to pad.\n            character (str, optional): Character to pad with. Defaults to \" \".\n        \"\"\"\n        assert len(character) == 1, \"Character must be a string of length 1\"\n        if count:\n            self.plain = f\"{character * count}{self.plain}\"\n            _Span = Span\n            self._spans[:] = [\n                _Span(start + count, end + count, style)\n                for start, end, style in self._spans\n            ]\n\n    def pad_right(self, count: int, character: str = \" \") -> None:\n        \"\"\"Pad the right with a given character.\n\n        Args:\n            count (int): Number of characters to pad.\n            character (str, optional): Character to pad with. Defaults to \" \".\n        \"\"\"\n        assert len(character) == 1, \"Character must be a string of length 1\"\n        if count:\n            self.plain = f\"{self.plain}{character * count}\"\n\n    def align(self, align: AlignMethod, width: int, character: str = \" \") -> None:\n        \"\"\"Align text to a given width.\n\n        Args:\n            align (AlignMethod): One of \"left\", \"center\", or \"right\".\n            width (int): Desired width.\n            character (str, optional): Character to pad with. Defaults to \" \".\n        \"\"\"\n        self.truncate(width)\n        excess_space = width - cell_len(self.plain)\n        if excess_space:\n            if align == \"left\":\n                self.pad_right(excess_space, character)\n            elif align == \"center\":\n                left = excess_space // 2\n                self.pad_left(left, character)\n                self.pad_right(excess_space - left, character)\n            else:\n                self.pad_left(excess_space, character)\n\n    def append(\n        self, text: Union[\"Text\", str], style: Optional[Union[str, \"Style\"]] = None\n    ) -> \"Text\":\n        \"\"\"Add text with an optional style.\n\n        Args:\n            text (Union[Text, str]): A str or Text to append.\n            style (str, optional): A style name. Defaults to None.\n\n        Returns:\n            Text: Returns self for chaining.\n        \"\"\"\n\n        if not isinstance(text, (str, Text)):\n            raise TypeError(\"Only str or Text can be appended to Text\")\n\n        if len(text):\n            if isinstance(text, str):\n                sanitized_text = strip_control_codes(text)\n                self._text.append(sanitized_text)\n                offset = len(self)\n                text_length = len(sanitized_text)\n                if style is not None:\n                    self._spans.append(Span(offset, offset + text_length, style))\n                self._length += text_length\n            elif isinstance(text, Text):\n                _Span = Span\n                if style is not None:\n                    raise ValueError(\n                        \"style must not be set when appending Text instance\"\n                    )\n                text_length = self._length\n                if text.style is not None:\n                    self._spans.append(\n                        _Span(text_length, text_length + len(text), text.style)\n                    )\n                self._text.append(text.plain)\n                self._spans.extend(\n                    _Span(start + text_length, end + text_length, style)\n                    for start, end, style in text._spans\n                )\n                self._length += len(text)\n        return self\n\n    def append_text(self, text: \"Text\") -> \"Text\":\n        \"\"\"Append another Text instance. This method is more performant that Text.append, but\n        only works for Text.\n\n        Returns:\n            Text: Returns self for chaining.\n        \"\"\"\n        _Span = Span\n        text_length = self._length\n        if text.style is not None:\n            self._spans.append(_Span(text_length, text_length + len(text), text.style))\n        self._text.append(text.plain)\n        self._spans.extend(\n            _Span(start + text_length, end + text_length, style)\n            for start, end, style in text._spans\n        )\n        self._length += len(text)\n        return self\n\n    def append_tokens(\n        self, tokens: Iterable[Tuple[str, Optional[StyleType]]]\n    ) -> \"Text\":\n        \"\"\"Append iterable of str and style. Style may be a Style instance or a str style definition.\n\n        Args:\n            pairs (Iterable[Tuple[str, Optional[StyleType]]]): An iterable of tuples containing str content and style.\n\n        Returns:\n            Text: Returns self for chaining.\n        \"\"\"\n        append_text = self._text.append\n        append_span = self._spans.append\n        _Span = Span\n        offset = len(self)\n        for content, style in tokens:\n            append_text(content)\n            if style is not None:\n                append_span(_Span(offset, offset + len(content), style))\n            offset += len(content)\n        self._length = offset\n        return self\n\n    def copy_styles(self, text: \"Text\") -> None:\n        \"\"\"Copy styles from another Text instance.\n\n        Args:\n            text (Text): A Text instance to copy styles from, must be the same length.\n        \"\"\"\n        self._spans.extend(text._spans)\n\n    def split(\n        self,\n        separator: str = \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/text.py",
      "line_number": 14,
      "details": "\".\n            include_separator (bool, optional): Include the separator in the lines. Defaults to False.\n            allow_blank (bool, optional): Return a blank line if the text ends with a separator. Defaults to False.\n\n        Returns:\n            List[RichText]: A list of rich text, one per line of the original.\n        \"\"\"\n        assert separator, \"separator must not be empty\"\n\n        text = self.plain\n        if separator not in text:\n            return Lines([self.copy()])\n\n        if include_separator:\n            lines = self.divide(\n                match.end() for match in re.finditer(re.escape(separator), text)\n            )\n        else:\n\n            def flatten_spans() -> Iterable[int]:\n                for match in re.finditer(re.escape(separator), text):\n                    start, end = match.span()\n                    yield start\n                    yield end\n\n            lines = Lines(\n                line for line in self.divide(flatten_spans()) if line.plain != separator\n            )\n\n        if not allow_blank and text.endswith(separator):\n            lines.pop()\n\n        return lines\n\n    def divide(self, offsets: Iterable[int]) -> Lines:\n        \"\"\"Divide text in to a number of lines at given offsets.\n\n        Args:\n            offsets (Iterable[int]): Offsets used to divide text.\n\n        Returns:\n            Lines: New RichText instances between offsets.\n        \"\"\"\n        _offsets = list(offsets)\n\n        if not _offsets:\n            return Lines([self.copy()])\n\n        text = self.plain\n        text_length = len(text)\n        divide_offsets = [0, *_offsets, text_length]\n        line_ranges = list(zip(divide_offsets, divide_offsets[1:]))\n\n        style = self.style\n        justify = self.justify\n        overflow = self.overflow\n        _Text = Text\n        new_lines = Lines(\n            _Text(\n                text[start:end],\n                style=style,\n                justify=justify,\n                overflow=overflow,\n            )\n            for start, end in line_ranges\n        )\n        if not self._spans:\n            return new_lines\n\n        _line_appends = [line._spans.append for line in new_lines._lines]\n        line_count = len(line_ranges)\n        _Span = Span\n\n        for span_start, span_end, style in self._spans:\n\n            lower_bound = 0\n            upper_bound = line_count\n            start_line_no = (lower_bound + upper_bound) // 2\n\n            while True:\n                line_start, line_end = line_ranges[start_line_no]\n                if span_start < line_start:\n                    upper_bound = start_line_no - 1\n                elif span_start > line_end:\n                    lower_bound = start_line_no + 1\n                else:\n                    break\n                start_line_no = (lower_bound + upper_bound) // 2\n\n            if span_end < line_end:\n                end_line_no = start_line_no\n            else:\n                end_line_no = lower_bound = start_line_no\n                upper_bound = line_count\n\n                while True:\n                    line_start, line_end = line_ranges[end_line_no]\n                    if span_end < line_start:\n                        upper_bound = end_line_no - 1\n                    elif span_end > line_end:\n                        lower_bound = end_line_no + 1\n                    else:\n                        break\n                    end_line_no = (lower_bound + upper_bound) // 2\n\n            for line_no in range(start_line_no, end_line_no + 1):\n                line_start, line_end = line_ranges[line_no]\n                new_start = max(0, span_start - line_start)\n                new_end = min(span_end - line_start, line_end - line_start)\n                if new_end > new_start:\n                    _line_appends[line_no](_Span(new_start, new_end, style))\n\n        return new_lines\n\n    def right_crop(self, amount: int = 1) -> None:\n        \"\"\"Remove a number of characters from the end of the text.\"\"\"\n        max_offset = len(self.plain) - amount\n        _Span = Span\n        self._spans[:] = [\n            (\n                span\n                if span.end < max_offset\n                else _Span(span.start, min(max_offset, span.end), span.style)\n            )\n            for span in self._spans\n            if span.start < max_offset\n        ]\n        self._text = [self.plain[:-amount]]\n        self._length -= amount\n\n    def wrap(\n        self,\n        console: \"Console\",\n        width: int,\n        *,\n        justify: Optional[\"JustifyMethod\"] = None,\n        overflow: Optional[\"OverflowMethod\"] = None,\n        tab_size: int = 8,\n        no_wrap: Optional[bool] = None,\n    ) -> Lines:\n        \"\"\"Word wrap the text.\n\n        Args:\n            console (Console): Console instance.\n            width (int): Number of characters per line.\n            emoji (bool, optional): Also render emoji code. Defaults to True.\n            justify (str, optional): Justify method: \"default\", \"left\", \"center\", \"full\", \"right\". Defaults to \"default\".\n            overflow (str, optional): Overflow method: \"crop\", \"fold\", or \"ellipsis\". Defaults to None.\n            tab_size (int, optional): Default tab size. Defaults to 8.\n            no_wrap (bool, optional): Disable wrapping, Defaults to False.\n\n        Returns:\n            Lines: Number of lines.\n        \"\"\"\n        wrap_justify = justify or self.justify or DEFAULT_JUSTIFY\n        wrap_overflow = overflow or self.overflow or DEFAULT_OVERFLOW\n\n        no_wrap = pick_bool(no_wrap, self.no_wrap, False) or overflow == \"ignore\"\n\n        lines = Lines()\n        for line in self.split(allow_blank=True):\n            if \"\\t\" in line:\n                line.expand_tabs(tab_size)\n            if no_wrap:\n                new_lines = Lines([line])\n            else:\n                offsets = divide_line(str(line), width, fold=wrap_overflow == \"fold\")\n                new_lines = line.divide(offsets)\n            for line in new_lines:\n                line.rstrip_end(width)\n            if wrap_justify:\n                new_lines.justify(\n                    console, width, justify=wrap_justify, overflow=wrap_overflow\n                )\n            for line in new_lines:\n                line.truncate(width, overflow=wrap_overflow)\n            lines.extend(new_lines)\n        return lines\n\n    def fit(self, width: int) -> Lines:\n        \"\"\"Fit the text in to given width by chopping in to lines.\n\n        Args:\n            width (int): Maximum characters in a line.\n\n        Returns:\n            Lines: Lines container.\n        \"\"\"\n        lines: Lines = Lines()\n        append = lines.append\n        for line in self.split():\n            line.set_length(width)\n            append(line)\n        return lines\n\n    def detect_indentation(self) -> int:\n        \"\"\"Auto-detect indentation of code.\n\n        Returns:\n            int: Number of spaces used to indent code.\n        \"\"\"\n\n        _indentations = {\n            len(match.group(1))\n            for match in re.finditer(r\"^( *)(.*)$\", self.plain, flags=re.MULTILINE)\n        }\n\n        try:\n            indentation = (\n                reduce(gcd, [indent for indent in _indentations if not indent % 2]) or 1\n            )\n        except TypeError:\n            indentation = 1\n\n        return indentation\n\n    def with_indent_guides(\n        self,\n        indent_size: Optional[int] = None,\n        *,\n        character: str = \"\u2502\",\n        style: StyleType = \"dim green\",\n    ) -> \"Text\":\n        \"\"\"Adds indent guide lines to text.\n\n        Args:\n            indent_size (Optional[int]): Size of indentation, or None to auto detect. Defaults to None.\n            character (str, optional): Character to use for indentation. Defaults to \"\u2502\".\n            style (Union[Style, str], optional): Style of indent guides.\n\n        Returns:\n            Text: New text with indentation guides.\n        \"\"\"\n\n        _indent_size = self.detect_indentation() if indent_size is None else indent_size\n\n        text = self.copy()\n        text.expand_tabs()\n        indent_line = f\"{character}{' ' * (_indent_size - 1)}\"\n\n        re_indent = re.compile(r\"^( *)(.*)$\")\n        new_lines: List[Text] = []\n        add_line = new_lines.append\n        blank_lines = 0\n        for line in text.split(allow_blank=True):\n            match = re_indent.match(line.plain)\n            if not match or not match.group(2):\n                blank_lines += 1\n                continue\n            indent = match.group(1)\n            full_indents, remaining_space = divmod(len(indent), _indent_size)\n            new_indent = f\"{indent_line * full_indents}{' ' * remaining_space}\"\n            line.plain = new_indent + line.plain[len(new_indent) :]\n            line.stylize(style, 0, len(new_indent))\n            if blank_lines:\n                new_lines.extend([Text(new_indent, style=style)] * blank_lines)\n                blank_lines = 0\n            add_line(line)\n        if blank_lines:\n            new_lines.extend([Text(\"\", style=style)] * blank_lines)\n\n        new_text = text.blank_copy(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/rich/traceback.py",
      "line_number": 1,
      "details": "from __future__ import absolute_import\n\nimport linecache\nimport os\nimport platform\nimport sys\nfrom dataclasses import dataclass, field\nfrom traceback import walk_tb\nfrom types import ModuleType, TracebackType\nfrom typing import (\n    Any,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    Type,\n    Union,\n)\n\nfrom pip._vendor.pygments.lexers import guess_lexer_for_filename\nfrom pip._vendor.pygments.token import Comment, Keyword, Name, Number, Operator, String\nfrom pip._vendor.pygments.token import Text as TextToken\nfrom pip._vendor.pygments.token import Token\nfrom pip._vendor.pygments.util import ClassNotFound\n\nfrom . import pretty\nfrom ._loop import loop_last\nfrom .columns import Columns\nfrom .console import Console, ConsoleOptions, ConsoleRenderable, RenderResult, group\nfrom .constrain import Constrain\nfrom .highlighter import RegexHighlighter, ReprHighlighter\nfrom .panel import Panel\nfrom .scope import render_scope\nfrom .style import Style\nfrom .syntax import Syntax\nfrom .text import Text\nfrom .theme import Theme\n\nWINDOWS = platform.system() == \"Windows\"\n\nLOCALS_MAX_LENGTH = 10\nLOCALS_MAX_STRING = 80\n\n\ndef install(\n    *,\n    console: Optional[Console] = None,\n    width: Optional[int] = 100,\n    extra_lines: int = 3,\n    theme: Optional[str] = None,\n    word_wrap: bool = False,\n    show_locals: bool = False,\n    locals_max_length: int = LOCALS_MAX_LENGTH,\n    locals_max_string: int = LOCALS_MAX_STRING,\n    locals_hide_dunder: bool = True,\n    locals_hide_sunder: Optional[bool] = None,\n    indent_guides: bool = True,\n    suppress: Iterable[Union[str, ModuleType]] = (),\n    max_frames: int = 100,\n) -> Callable[[Type[BaseException], BaseException, Optional[TracebackType]], Any]:\n    \"\"\"Install a rich traceback handler.\n\n    Once installed, any tracebacks will be printed with syntax highlighting and rich formatting.\n\n\n    Args:\n        console (Optional[Console], optional): Console to write exception to. Default uses internal Console instance.\n        width (Optional[int], optional): Width (in characters) of traceback. Defaults to 100.\n        extra_lines (int, optional): Extra lines of code. Defaults to 3.\n        theme (Optional[str], optional): Pygments theme to use in traceback. Defaults to ``None`` which will pick\n            a theme appropriate for the platform.\n        word_wrap (bool, optional): Enable word wrapping of long lines. Defaults to False.\n        show_locals (bool, optional): Enable display of local variables. Defaults to False.\n        locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to 10.\n        locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.\n        locals_hide_dunder (bool, optional): Hide locals prefixed with double underscore. Defaults to True.\n        locals_hide_sunder (bool, optional): Hide locals prefixed with single underscore. Defaults to False.\n        indent_guides (bool, optional): Enable indent guides in code and locals. Defaults to True.\n        suppress (Sequence[Union[str, ModuleType]]): Optional sequence of modules or paths to exclude from traceback.\n\n    Returns:\n        Callable: The previous exception handler that was replaced.\n\n    \"\"\"\n    traceback_console = Console(stderr=True) if console is None else console\n\n    locals_hide_sunder = (\n        True\n        if (traceback_console.is_jupyter and locals_hide_sunder is None)\n        else locals_hide_sunder\n    )\n\n    def excepthook(\n        type_: Type[BaseException],\n        value: BaseException,\n        traceback: Optional[TracebackType],\n    ) -> None:\n        traceback_console.print(\n            Traceback.from_exception(\n                type_,\n                value,\n                traceback,\n                width=width,\n                extra_lines=extra_lines,\n                theme=theme,\n                word_wrap=word_wrap,\n                show_locals=show_locals,\n                locals_max_length=locals_max_length,\n                locals_max_string=locals_max_string,\n                locals_hide_dunder=locals_hide_dunder,\n                locals_hide_sunder=bool(locals_hide_sunder),\n                indent_guides=indent_guides,\n                suppress=suppress,\n                max_frames=max_frames,\n            )\n        )\n\n    def ipy_excepthook_closure(ip: Any) -> None:  # pragma: no cover\n        tb_data = {}  # store information about showtraceback call\n        default_showtraceback = ip.showtraceback  # keep reference of default traceback\n\n        def ipy_show_traceback(*args: Any, **kwargs: Any) -> None:\n            \"\"\"wrap the default ip.showtraceback to store info for ip._showtraceback\"\"\"\n            nonlocal tb_data\n            tb_data = kwargs\n            default_showtraceback(*args, **kwargs)\n\n        def ipy_display_traceback(\n            *args: Any, is_syntax: bool = False, **kwargs: Any\n        ) -> None:\n            \"\"\"Internally called traceback from ip._showtraceback\"\"\"\n            nonlocal tb_data\n            exc_tuple = ip._get_exc_info()\n\n            # do not display trace on syntax error\n            tb: Optional[TracebackType] = None if is_syntax else exc_tuple[2]\n\n            # determine correct tb_offset\n            compiled = tb_data.get(\"running_compiled_code\", False)\n            tb_offset = tb_data.get(\"tb_offset\", 1 if compiled else 0)\n            # remove ipython internal frames from trace with tb_offset\n            for _ in range(tb_offset):\n                if tb is None:\n                    break\n                tb = tb.tb_next\n\n            excepthook(exc_tuple[0], exc_tuple[1], tb)\n            tb_data = {}  # clear data upon usage\n\n        # replace _showtraceback instead of showtraceback to allow ipython features such as debugging to work\n        # this is also what the ipython docs recommends to modify when subclassing InteractiveShell\n        ip._showtraceback = ipy_display_traceback\n        # add wrapper to capture tb_data\n        ip.showtraceback = ipy_show_traceback\n        ip.showsyntaxerror = lambda *args, **kwargs: ipy_display_traceback(\n            *args, is_syntax=True, **kwargs\n        )\n\n    try:  # pragma: no cover\n        # if within ipython, use customized traceback\n        ip = get_ipython()  # type: ignore[name-defined]\n        ipy_excepthook_closure(ip)\n        return sys.excepthook\n    except Exception:\n        # otherwise use default system hook\n        old_excepthook = sys.excepthook\n        sys.excepthook = excepthook\n        return old_excepthook\n\n\n@dataclass\nclass Frame:\n    filename: str\n    lineno: int\n    name: str\n    line: str = \"\"\n    locals: Optional[Dict[str, pretty.Node]] = None\n\n\n@dataclass\nclass _SyntaxError:\n    offset: int\n    filename: str\n    line: str\n    lineno: int\n    msg: str\n\n\n@dataclass\nclass Stack:\n    exc_type: str\n    exc_value: str\n    syntax_error: Optional[_SyntaxError] = None\n    is_cause: bool = False\n    frames: List[Frame] = field(default_factory=list)\n\n\n@dataclass\nclass Trace:\n    stacks: List[Stack]\n\n\nclass PathHighlighter(RegexHighlighter):\n    highlights = [r\"(?P<dim>.*/)(?P<bold>.+)\"]\n\n\nclass Traceback:\n    \"\"\"A Console renderable that renders a traceback.\n\n    Args:\n        trace (Trace, optional): A `Trace` object produced from `extract`. Defaults to None, which uses\n            the last exception.\n        width (Optional[int], optional): Number of characters used to traceback. Defaults to 100.\n        extra_lines (int, optional): Additional lines of code to render. Defaults to 3.\n        theme (str, optional): Override pygments theme used in traceback.\n        word_wrap (bool, optional): Enable word wrapping of long lines. Defaults to False.\n        show_locals (bool, optional): Enable display of local variables. Defaults to False.\n        indent_guides (bool, optional): Enable indent guides in code and locals. Defaults to True.\n        locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n            Defaults to 10.\n        locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.\n        locals_hide_dunder (bool, optional): Hide locals prefixed with double underscore. Defaults to True.\n        locals_hide_sunder (bool, optional): Hide locals prefixed with single underscore. Defaults to False.\n        suppress (Sequence[Union[str, ModuleType]]): Optional sequence of modules or paths to exclude from traceback.\n        max_frames (int): Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n\n    \"\"\"\n\n    LEXERS = {\n        \"\": \"text\",\n        \".py\": \"python\",\n        \".pxd\": \"cython\",\n        \".pyx\": \"cython\",\n        \".pxi\": \"pyrex\",\n    }\n\n    def __init__(\n        self,\n        trace: Optional[Trace] = None,\n        *,\n        width: Optional[int] = 100,\n        extra_lines: int = 3,\n        theme: Optional[str] = None,\n        word_wrap: bool = False,\n        show_locals: bool = False,\n        locals_max_length: int = LOCALS_MAX_LENGTH,\n        locals_max_string: int = LOCALS_MAX_STRING,\n        locals_hide_dunder: bool = True,\n        locals_hide_sunder: bool = False,\n        indent_guides: bool = True,\n        suppress: Iterable[Union[str, ModuleType]] = (),\n        max_frames: int = 100,\n    ):\n        if trace is None:\n            exc_type, exc_value, traceback = sys.exc_info()\n            if exc_type is None or exc_value is None or traceback is None:\n                raise ValueError(\n                    \"Value for 'trace' required if not called in except: block\"\n                )\n            trace = self.extract(\n                exc_type, exc_value, traceback, show_locals=show_locals\n            )\n        self.trace = trace\n        self.width = width\n        self.extra_lines = extra_lines\n        self.theme = Syntax.get_theme(theme or \"ansi_dark\")\n        self.word_wrap = word_wrap\n        self.show_locals = show_locals\n        self.indent_guides = indent_guides\n        self.locals_max_length = locals_max_length\n        self.locals_max_string = locals_max_string\n        self.locals_hide_dunder = locals_hide_dunder\n        self.locals_hide_sunder = locals_hide_sunder\n\n        self.suppress: Sequence[str] = []\n        for suppress_entity in suppress:\n            if not isinstance(suppress_entity, str):\n                assert (\n                    suppress_entity.__file__ is not None\n                ), f\"{suppress_entity!r} must be a module with '__file__' attribute\"\n                path = os.path.dirname(suppress_entity.__file__)\n            else:\n                path = suppress_entity\n            path = os.path.normpath(os.path.abspath(path))\n            self.suppress.append(path)\n        self.max_frames = max(4, max_frames) if max_frames > 0 else 0\n\n    @classmethod\n    def from_exception(\n        cls,\n        exc_type: Type[Any],\n        exc_value: BaseException,\n        traceback: Optional[TracebackType],\n        *,\n        width: Optional[int] = 100,\n        extra_lines: int = 3,\n        theme: Optional[str] = None,\n        word_wrap: bool = False,\n        show_locals: bool = False,\n        locals_max_length: int = LOCALS_MAX_LENGTH,\n        locals_max_string: int = LOCALS_MAX_STRING,\n        locals_hide_dunder: bool = True,\n        locals_hide_sunder: bool = False,\n        indent_guides: bool = True,\n        suppress: Iterable[Union[str, ModuleType]] = (),\n        max_frames: int = 100,\n    ) -> \"Traceback\":\n        \"\"\"Create a traceback from exception info\n\n        Args:\n            exc_type (Type[BaseException]): Exception type.\n            exc_value (BaseException): Exception value.\n            traceback (TracebackType): Python Traceback object.\n            width (Optional[int], optional): Number of characters used to traceback. Defaults to 100.\n            extra_lines (int, optional): Additional lines of code to render. Defaults to 3.\n            theme (str, optional): Override pygments theme used in traceback.\n            word_wrap (bool, optional): Enable word wrapping of long lines. Defaults to False.\n            show_locals (bool, optional): Enable display of local variables. Defaults to False.\n            indent_guides (bool, optional): Enable indent guides in code and locals. Defaults to True.\n            locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n                Defaults to 10.\n            locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.\n            locals_hide_dunder (bool, optional): Hide locals prefixed with double underscore. Defaults to True.\n            locals_hide_sunder (bool, optional): Hide locals prefixed with single underscore. Defaults to False.\n            suppress (Iterable[Union[str, ModuleType]]): Optional sequence of modules or paths to exclude from traceback.\n            max_frames (int): Maximum number of frames to show in a traceback, 0 for no maximum. Defaults to 100.\n\n        Returns:\n            Traceback: A Traceback instance that may be printed.\n        \"\"\"\n        rich_traceback = cls.extract(\n            exc_type,\n            exc_value,\n            traceback,\n            show_locals=show_locals,\n            locals_max_length=locals_max_length,\n            locals_max_string=locals_max_string,\n            locals_hide_dunder=locals_hide_dunder,\n            locals_hide_sunder=locals_hide_sunder,\n        )\n\n        return cls(\n            rich_traceback,\n            width=width,\n            extra_lines=extra_lines,\n            theme=theme,\n            word_wrap=word_wrap,\n            show_locals=show_locals,\n            indent_guides=indent_guides,\n            locals_max_length=locals_max_length,\n            locals_max_string=locals_max_string,\n            locals_hide_dunder=locals_hide_dunder,\n            locals_hide_sunder=locals_hide_sunder,\n            suppress=suppress,\n            max_frames=max_frames,\n        )\n\n    @classmethod\n    def extract(\n        cls,\n        exc_type: Type[BaseException],\n        exc_value: BaseException,\n        traceback: Optional[TracebackType],\n        *,\n        show_locals: bool = False,\n        locals_max_length: int = LOCALS_MAX_LENGTH,\n        locals_max_string: int = LOCALS_MAX_STRING,\n        locals_hide_dunder: bool = True,\n        locals_hide_sunder: bool = False,\n    ) -> Trace:\n        \"\"\"Extract traceback information.\n\n        Args:\n            exc_type (Type[BaseException]): Exception type.\n            exc_value (BaseException): Exception value.\n            traceback (TracebackType): Python Traceback object.\n            show_locals (bool, optional): Enable display of local variables. Defaults to False.\n            locals_max_length (int, optional): Maximum length of containers before abbreviating, or None for no abbreviation.\n                Defaults to 10.\n            locals_max_string (int, optional): Maximum length of string before truncating, or None to disable. Defaults to 80.\n            locals_hide_dunder (bool, optional): Hide locals prefixed with double underscore. Defaults to True.\n            locals_hide_sunder (bool, optional): Hide locals prefixed with single underscore. Defaults to False.\n\n        Returns:\n            Trace: A Trace instance which you can use to construct a `Traceback`.\n        \"\"\"\n\n        stacks: List[Stack] = []\n        is_cause = False\n\n        from pip._vendor.rich import _IMPORT_CWD\n\n        def safe_str(_object: Any) -> str:\n            \"\"\"Don't allow exceptions from __str__ to propagate.\"\"\"\n            try:\n                return str(_object)\n            except Exception:\n                return \"<exception str() failed>\"\n\n        while True:\n            stack = Stack(\n                exc_type=safe_str(exc_type.__name__),\n                exc_value=safe_str(exc_value),\n                is_cause=is_cause,\n            )\n\n            if isinstance(exc_value, SyntaxError):\n                stack.syntax_error = _SyntaxError(\n                    offset=exc_value.offset or 0,\n                    filename=exc_value.filename or \"?\",\n                    lineno=exc_value.lineno or 0,\n                    line=exc_value.text or \"\",\n                    msg=exc_value.msg,\n                )\n\n            stacks.append(stack)\n            append = stack.frames.append\n\n            def get_locals(\n                iter_locals: Iterable[Tuple[str, object]]\n            ) -> Iterable[Tuple[str, object]]:\n                \"\"\"Extract locals from an iterator of key pairs.\"\"\"\n                if not (locals_hide_dunder or locals_hide_sunder):\n                    yield from iter_locals\n                    return\n                for key, value in iter_locals:\n                    if locals_hide_dunder and key.startswith(\"__\"):\n                        continue\n                    if locals_hide_sunder and key.startswith(\"_\"):\n                        continue\n                    yield key, value\n\n            for frame_summary, line_no in walk_tb(traceback):\n                filename = frame_summary.f_code.co_filename\n                if filename and not filename.startswith(\"<\"):\n                    if not os.path.isabs(filename):\n                        filename = os.path.join(_IMPORT_CWD, filename)\n                if frame_summary.f_locals.get(\"_rich_traceback_omit\", False):\n                    continue\n\n                frame = Frame(\n                    filename=filename or \"?\",\n                    lineno=line_no,\n                    name=frame_summary.f_code.co_name,\n                    locals={\n                        key: pretty.traverse(\n                            value,\n                            max_length=locals_max_length,\n                            max_string=locals_max_string,\n                        )\n                        for key, value in get_locals(frame_summary.f_locals.items())\n                    }\n                    if show_locals\n                    else None,\n                )\n                append(frame)\n                if frame_summary.f_locals.get(\"_rich_traceback_guard\", False):\n                    del stack.frames[:]\n\n            cause = getattr(exc_value, \"__cause__\", None)\n            if cause:\n                exc_type = cause.__class__\n                exc_value = cause\n                # __traceback__ can be None, e.g. for exceptions raised by the\n                # 'multiprocessing' module\n                traceback = cause.__traceback__\n                is_cause = True\n                continue\n\n            cause = exc_value.__context__\n            if cause and not getattr(exc_value, \"__suppress_context__\", False):\n                exc_type = cause.__class__\n                exc_value = cause\n                traceback = cause.__traceback__\n                is_cause = False\n                continue\n            # No cover, code is reached but coverage doesn't recognize it.\n            break  # pragma: no cover\n\n        trace = Trace(stacks=stacks)\n        return trace\n\n    def __rich_console__(\n        self, console: Console, options: ConsoleOptions\n    ) -> RenderResult:\n        theme = self.theme\n        background_style = theme.get_background_style()\n        token_style = theme.get_style_for_token\n\n        traceback_theme = Theme(\n            {\n                \"pretty\": token_style(TextToken),\n                \"pygments.text\": token_style(Token),\n                \"pygments.string\": token_style(String),\n                \"pygments.function\": token_style(Name.Function),\n                \"pygments.number\": token_style(Number),\n                \"repr.indent\": token_style(Comment) + Style(dim=True),\n                \"repr.str\": token_style(String),\n                \"repr.brace\": token_style(TextToken) + Style(bold=True),\n                \"repr.number\": token_style(Number),\n                \"repr.bool_true\": token_style(Keyword.Constant),\n                \"repr.bool_false\": token_style(Keyword.Constant),\n                \"repr.none\": token_style(Keyword.Constant),\n                \"scope.border\": token_style(String.Delimiter),\n                \"scope.equals\": token_style(Operator),\n                \"scope.key\": token_style(Name),\n                \"scope.key.special\": token_style(Name.Constant) + Style(dim=True),\n            },\n            inherit=False,\n        )\n\n        highlighter = ReprHighlighter()\n        for last, stack in loop_last(reversed(self.trace.stacks)):\n            if stack.frames:\n                stack_renderable: ConsoleRenderable = Panel(\n                    self._render_stack(stack),\n                    title=\"[traceback.title]Traceback [dim](most recent call last)\",\n                    style=background_style,\n                    border_style=\"traceback.border\",\n                    expand=True,\n                    padding=(0, 1),\n                )\n                stack_renderable = Constrain(stack_renderable, self.width)\n                with console.use_theme(traceback_theme):\n                    yield stack_renderable\n            if stack.syntax_error is not None:\n                with console.use_theme(traceback_theme):\n                    yield Constrain(\n                        Panel(\n                            self._render_syntax_error(stack.syntax_error),\n                            style=background_style,\n                            border_style=\"traceback.border.syntax_error\",\n                            expand=True,\n                            padding=(0, 1),\n                            width=self.width,\n                        ),\n                        self.width,\n                    )\n                yield Text.assemble(\n                    (f\"{stack.exc_type}: \", \"traceback.exc_type\"),\n                    highlighter(stack.syntax_error.msg),\n                )\n            elif stack.exc_value:\n                yield Text.assemble(\n                    (f\"{stack.exc_type}: \", \"traceback.exc_type\"),\n                    highlighter(stack.exc_value),\n                )\n            else:\n                yield Text.assemble((f\"{stack.exc_type}\", \"traceback.exc_type\"))\n\n            if not last:\n                if stack.is_cause:\n                    yield Text.from_markup(\n                        \""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/tenacity/wait.py",
      "line_number": 1,
      "details": "# Copyright 2016\u20132021 Julien Danjou\n# Copyright 2016 Joshua Harlow\n# Copyright 2013-2014 Ray Holder\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport abc\nimport random\nimport typing\n\nfrom pip._vendor.tenacity import _utils\n\nif typing.TYPE_CHECKING:\n    from pip._vendor.tenacity import RetryCallState\n\n\nclass wait_base(abc.ABC):\n    \"\"\"Abstract base class for wait strategies.\"\"\"\n\n    @abc.abstractmethod\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        pass\n\n    def __add__(self, other: \"wait_base\") -> \"wait_combine\":\n        return wait_combine(self, other)\n\n    def __radd__(self, other: \"wait_base\") -> typing.Union[\"wait_combine\", \"wait_base\"]:\n        # make it possible to use multiple waits with the built-in sum function\n        if other == 0:  # type: ignore[comparison-overlap]\n            return self\n        return self.__add__(other)\n\n\nWaitBaseT = typing.Union[wait_base, typing.Callable[[\"RetryCallState\"], typing.Union[float, int]]]\n\n\nclass wait_fixed(wait_base):\n    \"\"\"Wait strategy that waits a fixed amount of time between each retry.\"\"\"\n\n    def __init__(self, wait: _utils.time_unit_type) -> None:\n        self.wait_fixed = _utils.to_seconds(wait)\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        return self.wait_fixed\n\n\nclass wait_none(wait_fixed):\n    \"\"\"Wait strategy that doesn't wait at all before retrying.\"\"\"\n\n    def __init__(self) -> None:\n        super().__init__(0)\n\n\nclass wait_random(wait_base):\n    \"\"\"Wait strategy that waits a random amount of time between min/max.\"\"\"\n\n    def __init__(self, min: _utils.time_unit_type = 0, max: _utils.time_unit_type = 1) -> None:  # noqa\n        self.wait_random_min = _utils.to_seconds(min)\n        self.wait_random_max = _utils.to_seconds(max)\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        return self.wait_random_min + (random.random() * (self.wait_random_max - self.wait_random_min))\n\n\nclass wait_combine(wait_base):\n    \"\"\"Combine several waiting strategies.\"\"\"\n\n    def __init__(self, *strategies: wait_base) -> None:\n        self.wait_funcs = strategies\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        return sum(x(retry_state=retry_state) for x in self.wait_funcs)\n\n\nclass wait_chain(wait_base):\n    \"\"\"Chain two or more waiting strategies.\n\n    If all strategies are exhausted, the very last strategy is used\n    thereafter.\n\n    For example::\n\n        @retry(wait=wait_chain(*[wait_fixed(1) for i in range(3)] +\n                               [wait_fixed(2) for j in range(5)] +\n                               [wait_fixed(5) for k in range(4)))\n        def wait_chained():\n            print(\"Wait 1s for 3 attempts, 2s for 5 attempts and 5s\n                   thereafter.\")\n    \"\"\"\n\n    def __init__(self, *strategies: wait_base) -> None:\n        self.strategies = strategies\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        wait_func_no = min(max(retry_state.attempt_number, 1), len(self.strategies))\n        wait_func = self.strategies[wait_func_no - 1]\n        return wait_func(retry_state=retry_state)\n\n\nclass wait_incrementing(wait_base):\n    \"\"\"Wait an incremental amount of time after each attempt.\n\n    Starting at a starting value and incrementing by a value for each attempt\n    (and restricting the upper limit to some maximum value).\n    \"\"\"\n\n    def __init__(\n        self,\n        start: _utils.time_unit_type = 0,\n        increment: _utils.time_unit_type = 100,\n        max: _utils.time_unit_type = _utils.MAX_WAIT,  # noqa\n    ) -> None:\n        self.start = _utils.to_seconds(start)\n        self.increment = _utils.to_seconds(increment)\n        self.max = _utils.to_seconds(max)\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        result = self.start + (self.increment * (retry_state.attempt_number - 1))\n        return max(0, min(result, self.max))\n\n\nclass wait_exponential(wait_base):\n    \"\"\"Wait strategy that applies exponential backoff.\n\n    It allows for a customized multiplier and an ability to restrict the\n    upper and lower limits to some maximum and minimum value.\n\n    The intervals are fixed (i.e. there is no jitter), so this strategy is\n    suitable for balancing retries against latency when a required resource is\n    unavailable for an unknown duration, but *not* suitable for resolving\n    contention between multiple processes for a shared resource. Use\n    wait_random_exponential for the latter case.\n    \"\"\"\n\n    def __init__(\n        self,\n        multiplier: typing.Union[int, float] = 1,\n        max: _utils.time_unit_type = _utils.MAX_WAIT,  # noqa\n        exp_base: typing.Union[int, float] = 2,\n        min: _utils.time_unit_type = 0,  # noqa\n    ) -> None:\n        self.multiplier = multiplier\n        self.min = _utils.to_seconds(min)\n        self.max = _utils.to_seconds(max)\n        self.exp_base = exp_base\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        try:\n            exp = self.exp_base ** (retry_state.attempt_number - 1)\n            result = self.multiplier * exp\n        except OverflowError:\n            return self.max\n        return max(max(0, self.min), min(result, self.max))\n\n\nclass wait_random_exponential(wait_exponential):\n    \"\"\"Random wait with exponentially widening window.\n\n    An exponential backoff strategy used to mediate contention between multiple\n    uncoordinated processes for a shared resource in distributed systems. This\n    is the sense in which \"exponential backoff\" is meant in e.g. Ethernet\n    networking, and corresponds to the \"Full Jitter\" algorithm described in\n    this blog post:\n\n    https://aws.amazon.com/blogs/architecture/exponential-backoff-and-jitter/\n\n    Each retry occurs at a random time in a geometrically expanding interval.\n    It allows for a custom multiplier and an ability to restrict the upper\n    limit of the random interval to some maximum value.\n\n    Example::\n\n        wait_random_exponential(multiplier=0.5,  # initial window 0.5s\n                                max=60)          # max 60s timeout\n\n    When waiting for an unavailable resource to become available again, as\n    opposed to trying to resolve contention for a shared resource, the\n    wait_exponential strategy (which uses a fixed interval) may be preferable.\n\n    \"\"\"\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        high = super().__call__(retry_state=retry_state)\n        return random.uniform(0, high)\n\n\nclass wait_exponential_jitter(wait_base):\n    \"\"\"Wait strategy that applies exponential backoff and jitter.\n\n    It allows for a customized initial wait, maximum wait and jitter.\n\n    This implements the strategy described here:\n    https://cloud.google.com/storage/docs/retry-strategy\n\n    The wait time is min(initial * 2**n + random.uniform(0, jitter), maximum)\n    where n is the retry count.\n    \"\"\"\n\n    def __init__(\n        self,\n        initial: float = 1,\n        max: float = _utils.MAX_WAIT,  # noqa\n        exp_base: float = 2,\n        jitter: float = 1,\n    ) -> None:\n        self.initial = initial\n        self.max = max\n        self.exp_base = exp_base\n        self.jitter = jitter\n\n    def __call__(self, retry_state: \"RetryCallState\") -> float:\n        jitter = random.uniform(0, self.jitter)\n        try:\n            exp = self.exp_base ** (retry_state.attempt_number - 1)\n            result = self.initial * exp + jitter\n        except OverflowError:\n            result = self.max\n        return max(0, min(result, self.max))"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "venv/lib/python3.12/site-packages/pip/_vendor/colorama/tests/ansitowin32_test.py",
      "line_number": 1,
      "details": "# Copyright Jonathan Hartley 2013. BSD 3-Clause license, see LICENSE file.\nfrom io import StringIO, TextIOWrapper\nfrom unittest import TestCase, main\ntry:\n    from contextlib import ExitStack\nexcept ImportError:\n    # python 2\n    from contextlib2 import ExitStack\n\ntry:\n    from unittest.mock import MagicMock, Mock, patch\nexcept ImportError:\n    from mock import MagicMock, Mock, patch\n\nfrom ..ansitowin32 import AnsiToWin32, StreamWrapper\nfrom ..win32 import ENABLE_VIRTUAL_TERMINAL_PROCESSING\nfrom .utils import osname\n\n\nclass StreamWrapperTest(TestCase):\n\n    def testIsAProxy(self):\n        mockStream = Mock()\n        wrapper = StreamWrapper(mockStream, None)\n        self.assertTrue( wrapper.random_attr is mockStream.random_attr )\n\n    def testDelegatesWrite(self):\n        mockStream = Mock()\n        mockConverter = Mock()\n        wrapper = StreamWrapper(mockStream, mockConverter)\n        wrapper.write('hello')\n        self.assertTrue(mockConverter.write.call_args, (('hello',), {}))\n\n    def testDelegatesContext(self):\n        mockConverter = Mock()\n        s = StringIO()\n        with StreamWrapper(s, mockConverter) as fp:\n            fp.write(u'hello')\n        self.assertTrue(s.closed)\n\n    def testProxyNoContextManager(self):\n        mockStream = MagicMock()\n        mockStream.__enter__.side_effect = AttributeError()\n        mockConverter = Mock()\n        with self.assertRaises(AttributeError) as excinfo:\n            with StreamWrapper(mockStream, mockConverter) as wrapper:\n                wrapper.write('hello')\n\n    def test_closed_shouldnt_raise_on_closed_stream(self):\n        stream = StringIO()\n        stream.close()\n        wrapper = StreamWrapper(stream, None)\n        self.assertEqual(wrapper.closed, True)\n\n    def test_closed_shouldnt_raise_on_detached_stream(self):\n        stream = TextIOWrapper(StringIO())\n        stream.detach()\n        wrapper = StreamWrapper(stream, None)\n        self.assertEqual(wrapper.closed, True)\n\nclass AnsiToWin32Test(TestCase):\n\n    def testInit(self):\n        mockStdout = Mock()\n        auto = Mock()\n        stream = AnsiToWin32(mockStdout, autoreset=auto)\n        self.assertEqual(stream.wrapped, mockStdout)\n        self.assertEqual(stream.autoreset, auto)\n\n    @patch('colorama.ansitowin32.winterm', None)\n    @patch('colorama.ansitowin32.winapi_test', lambda *_: True)\n    def testStripIsTrueOnWindows(self):\n        with osname('nt'):\n            mockStdout = Mock()\n            stream = AnsiToWin32(mockStdout)\n            self.assertTrue(stream.strip)\n\n    def testStripIsFalseOffWindows(self):\n        with osname('posix'):\n            mockStdout = Mock(closed=False)\n            stream = AnsiToWin32(mockStdout)\n            self.assertFalse(stream.strip)\n\n    def testWriteStripsAnsi(self):\n        mockStdout = Mock()\n        stream = AnsiToWin32(mockStdout)\n        stream.wrapped = Mock()\n        stream.write_and_convert = Mock()\n        stream.strip = True\n\n        stream.write('abc')\n\n        self.assertFalse(stream.wrapped.write.called)\n        self.assertEqual(stream.write_and_convert.call_args, (('abc',), {}))\n\n    def testWriteDoesNotStripAnsi(self):\n        mockStdout = Mock()\n        stream = AnsiToWin32(mockStdout)\n        stream.wrapped = Mock()\n        stream.write_and_convert = Mock()\n        stream.strip = False\n        stream.convert = False\n\n        stream.write('abc')\n\n        self.assertFalse(stream.write_and_convert.called)\n        self.assertEqual(stream.wrapped.write.call_args, (('abc',), {}))\n\n    def assert_autoresets(self, convert, autoreset=True):\n        stream = AnsiToWin32(Mock())\n        stream.convert = convert\n        stream.reset_all = Mock()\n        stream.autoreset = autoreset\n        stream.winterm = Mock()\n\n        stream.write('abc')\n\n        self.assertEqual(stream.reset_all.called, autoreset)\n\n    def testWriteAutoresets(self):\n        self.assert_autoresets(convert=True)\n        self.assert_autoresets(convert=False)\n        self.assert_autoresets(convert=True, autoreset=False)\n        self.assert_autoresets(convert=False, autoreset=False)\n\n    def testWriteAndConvertWritesPlainText(self):\n        stream = AnsiToWin32(Mock())\n        stream.write_and_convert( 'abc' )\n        self.assertEqual( stream.wrapped.write.call_args, (('abc',), {}) )\n\n    def testWriteAndConvertStripsAllValidAnsi(self):\n        stream = AnsiToWin32(Mock())\n        stream.call_win32 = Mock()\n        data = [\n            'abc\\033[mdef',\n            'abc\\033[0mdef',\n            'abc\\033[2mdef',\n            'abc\\033[02mdef',\n            'abc\\033[002mdef',\n            'abc\\033[40mdef',\n            'abc\\033[040mdef',\n            'abc\\033[0;1mdef',\n            'abc\\033[40;50mdef',\n            'abc\\033[50;30;40mdef',\n            'abc\\033[Adef',\n            'abc\\033[0Gdef',\n            'abc\\033[1;20;128Hdef',\n        ]\n        for datum in data:\n            stream.wrapped.write.reset_mock()\n            stream.write_and_convert( datum )\n            self.assertEqual(\n               [args[0] for args in stream.wrapped.write.call_args_list],\n               [ ('abc',), ('def',) ]\n            )\n\n    def testWriteAndConvertSkipsEmptySnippets(self):\n        stream = AnsiToWin32(Mock())\n        stream.call_win32 = Mock()\n        stream.write_and_convert( '\\033[40m\\033[41m' )\n        self.assertFalse( stream.wrapped.write.called )\n\n    def testWriteAndConvertCallsWin32WithParamsAndCommand(self):\n        stream = AnsiToWin32(Mock())\n        stream.convert = True\n        stream.call_win32 = Mock()\n        stream.extract_params = Mock(return_value='params')\n        data = {\n            'abc\\033[adef':         ('a', 'params'),\n            'abc\\033[;;bdef':       ('b', 'params'),\n            'abc\\033[0cdef':        ('c', 'params'),\n            'abc\\033[;;0;;Gdef':    ('G', 'params'),\n            'abc\\033[1;20;128Hdef': ('H', 'params'),\n        }\n        for datum, expected in data.items():\n            stream.call_win32.reset_mock()\n            stream.write_and_convert( datum )\n            self.assertEqual( stream.call_win32.call_args[0], expected )\n\n    def test_reset_all_shouldnt_raise_on_closed_orig_stdout(self):\n        stream = StringIO()\n        converter = AnsiToWin32(stream)\n        stream.close()\n\n        converter.reset_all()\n\n    def test_wrap_shouldnt_raise_on_closed_orig_stdout(self):\n        stream = StringIO()\n        stream.close()\n        with \\\n            patch(\"colorama.ansitowin32.os.name\", \"nt\"), \\\n            patch(\"colorama.ansitowin32.winapi_test\", lambda: True):\n                converter = AnsiToWin32(stream)\n        self.assertTrue(converter.strip)\n        self.assertFalse(converter.convert)\n\n    def test_wrap_shouldnt_raise_on_missing_closed_attr(self):\n        with \\\n            patch(\"colorama.ansitowin32.os.name\", \"nt\"), \\\n            patch(\"colorama.ansitowin32.winapi_test\", lambda: True):\n                converter = AnsiToWin32(object())\n        self.assertTrue(converter.strip)\n        self.assertFalse(converter.convert)\n\n    def testExtractParams(self):\n        stream = AnsiToWin32(Mock())\n        data = {\n            '':               (0,),\n            ';;':             (0,),\n            '2':              (2,),\n            ';;002;;':        (2,),\n            '0;1':            (0, 1),\n            ';;003;;456;;':   (3, 456),\n            '11;22;33;44;55': (11, 22, 33, 44, 55),\n        }\n        for datum, expected in data.items():\n            self.assertEqual(stream.extract_params('m', datum), expected)\n\n    def testCallWin32UsesLookup(self):\n        listener = Mock()\n        stream = AnsiToWin32(listener)\n        stream.win32_calls = {\n            1: (lambda *_, **__: listener(11),),\n            2: (lambda *_, **__: listener(22),),\n            3: (lambda *_, **__: listener(33),),\n        }\n        stream.call_win32('m', (3, 1, 99, 2))\n        self.assertEqual(\n            [a[0][0] for a in listener.call_args_list],\n            [33, 11, 22] )\n\n    def test_osc_codes(self):\n        mockStdout = Mock()\n        stream = AnsiToWin32(mockStdout, convert=True)\n        with patch('colorama.ansitowin32.winterm') as winterm:\n            data = [\n                '\\033]0\\x07',                      # missing arguments\n                '\\033]0;foo\\x08',                  # wrong OSC command\n                '\\033]0;colorama_test_title\\x07',  # should work\n                '\\033]1;colorama_test_title\\x07',  # wrong set command\n                '\\033]2;colorama_test_title\\x07',  # should work\n                '\\033]' + ';' * 64 + '\\x08',       # see issue #247\n            ]\n            for code in data:\n                stream.write(code)\n            self.assertEqual(winterm.set_title.call_count, 2)\n\n    def test_native_windows_ansi(self):\n        with ExitStack() as stack:\n            def p(a, b):\n                stack.enter_context(patch(a, b, create=True))\n            # Pretend to be on Windows\n            p(\"colorama.ansitowin32.os.name\", \"nt\")\n            p(\"colorama.ansitowin32.winapi_test\", lambda: True)\n            p(\"colorama.win32.winapi_test\", lambda: True)\n            p(\"colorama.winterm.win32.windll\", \"non-None\")\n            p(\"colorama.winterm.get_osfhandle\", lambda _: 1234)\n\n            # Pretend that our mock stream has native ANSI support\n            p(\n                \"colorama.winterm.win32.GetConsoleMode\",\n                lambda _: ENABLE_VIRTUAL_TERMINAL_PROCESSING,\n            )\n            SetConsoleMode = Mock()\n            p(\"colorama.winterm.win32.SetConsoleMode\", SetConsoleMode)\n\n            stdout = Mock()\n            stdout.closed = False\n            stdout.isatty.return_value = True\n            stdout.fileno.return_value = 1\n\n            # Our fake console says it has native vt support, so AnsiToWin32 should\n            # enable that support and do nothing else.\n            stream = AnsiToWin32(stdout)\n            SetConsoleMode.assert_called_with(1234, ENABLE_VIRTUAL_TERMINAL_PROCESSING)\n            self.assertFalse(stream.strip)\n            self.assertFalse(stream.convert)\n            self.assertFalse(stream.should_wrap())\n\n            # Now let's pretend we're on an old Windows console, that doesn't have\n            # native ANSI support.\n            p(\"colorama.winterm.win32.GetConsoleMode\", lambda _: 0)\n            SetConsoleMode = Mock()\n            p(\"colorama.winterm.win32.SetConsoleMode\", SetConsoleMode)\n\n            stream = AnsiToWin32(stdout)\n            SetConsoleMode.assert_called_with(1234, ENABLE_VIRTUAL_TERMINAL_PROCESSING)\n            self.assertTrue(stream.strip)\n            self.assertTrue(stream.convert)\n            self.assertTrue(stream.should_wrap())\n\n\nif __name__ == '__main__':\n    main()"
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "build/examples/src/advanced_algorithms_showcase.py",
      "line_number": 1,
      "details": "#!/usr/bin/env python3\n\"\"\"Advanced Algorithms Showcase Example.\n\nThis example demonstrates all advanced algorithms implemented in the research module:\n1. Analog Physics-Informed Crossbar Networks (APICNs)\n2. Temporal Crossbar Cascading (TCC)\n3. Heterogeneous Precision Analog Computing (HPAC)\n4. Analog Multi-Physics Coupling (AMPC)\n5. Neuromorphic PDE Acceleration (NPA)\n\nIt shows how to use the integrated framework for automatic algorithm selection\nand provides comprehensive examples of breakthrough performance capabilities.\n\"\"\"\n\nimport sys\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\n# Add the project root to the path\nsys.path.insert(0, str(Path(__file__).parent.parent))\n\nfrom analog_pde_solver.core.equations import PoissonEquation, HeatEquation, WaveEquation\nfrom analog_pde_solver.research.integrated_solver_framework import (\n    AdvancedSolverFramework,\n    AlgorithmType,\n    ProblemCharacteristics\n)\nfrom analog_pde_solver.research.validation_benchmark_suite import (\n    ValidationBenchmarkSuite,\n    BenchmarkType\n)\n\n\ndef demonstrate_physics_informed_crossbar():\n    \"\"\"Demonstrate Analog Physics-Informed Crossbar Networks (APICNs).\"\"\"\n    print(\"=\" * 60)\n    print(\"ANALOG PHYSICS-INFORMED CROSSBAR NETWORKS (APICNs)\")\n    print(\"=\" * 60)\n    print(\"Embedding physics constraints directly into crossbar hardware\")\n    print()\n    \n    # Create a Poisson equation with conservation requirements\n    pde = PoissonEquation(\n        domain_size=(128, 128),\n        boundary_conditions='dirichlet',\n        source_function=lambda x, y: np.sin(np.pi * x) * np.sin(np.pi * y)\n    )\n    \n    # Initialize framework\n    framework = AdvancedSolverFramework(\n        base_crossbar_size=128,\n        performance_mode='accuracy'\n    )\n    \n    # Problem characteristics that favor physics-informed approach\n    characteristics = ProblemCharacteristics(\n        problem_size=(128, 128),\n        sparsity_level=0.2,\n        time_dependent=False,\n        multi_physics=False,\n        conservation_required=True,  # Key for physics-informed\n        accuracy_requirement=1e-8,\n        energy_budget=None,\n        real_time_requirement=False,\n        physics_constraints=['conservation', 'symmetry'],  # Physics constraints\n        boundary_complexity='simple'\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Domain size: {characteristics.problem_size}\")\n    print(f\"- Conservation required: {characteristics.conservation_required}\")\n    print(f\"- Physics constraints: {characteristics.physics_constraints}\")\n    print(f\"- Accuracy requirement: {characteristics.accuracy_requirement}\")\n    print()\n    \n    try:\n        # Solve with physics-informed algorithm\n        print(\"Solving with APICNs...\")\n        solution, solve_info = framework.solve_pde(\n            pde,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.PHYSICS_INFORMED\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Solution range: [{np.min(solution):.6f}, {np.max(solution):.6f}]\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n            print(f\"- Reasoning: {rec['reasoning']}\")\n        \n    except Exception as e:\n        print(f\"Physics-informed demonstration failed: {e}\")\n    \n    print()\n\n\ndef demonstrate_temporal_cascading():\n    \"\"\"Demonstrate Temporal Crossbar Cascading (TCC).\"\"\"\n    print(\"=\" * 60)\n    print(\"TEMPORAL CROSSBAR CASCADING (TCC)\")\n    print(\"=\" * 60)\n    print(\"Hardware pipelining of temporal discretization for 100\u00d7 speedup\")\n    print()\n    \n    # Create a time-dependent heat equation\n    heat_equation = HeatEquation(\n        domain_size=(128,),\n        boundary_conditions='dirichlet',\n        initial_condition=lambda x: np.sin(np.pi * x),\n        diffusivity=0.1\n    )\n    \n    framework = AdvancedSolverFramework(\n        base_crossbar_size=128,\n        performance_mode='speed'\n    )\n    \n    # Problem characteristics that favor temporal cascading\n    characteristics = ProblemCharacteristics(\n        problem_size=(128,),\n        sparsity_level=0.1,\n        time_dependent=True,  # Key for temporal cascading\n        multi_physics=False,\n        conservation_required=False,\n        accuracy_requirement=1e-6,\n        energy_budget=None,\n        real_time_requirement=True,  # Real-time favors cascading\n        physics_constraints=[],\n        boundary_complexity='simple'\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Domain size: {characteristics.problem_size}\")\n    print(f\"- Time dependent: {characteristics.time_dependent}\")\n    print(f\"- Real-time requirement: {characteristics.real_time_requirement}\")\n    print(f\"- Expected speedup: 100\u00d7\")\n    print()\n    \n    try:\n        # Solve with temporal cascading\n        print(\"Solving with TCC...\")\n        solution, solve_info = framework.solve_pde(\n            heat_equation,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.TEMPORAL_CASCADE,\n            time_span=(0.0, 1.0),\n            num_time_steps=100,\n            initial_solution=np.sin(np.pi * np.linspace(0, 1, 128))\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Final solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Pipeline stages: 4\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Estimated speedup: {rec['estimated_speedup']:.0f}\u00d7\")\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n        \n    except Exception as e:\n        print(f\"Temporal cascading demonstration failed: {e}\")\n    \n    print()\n\n\ndef demonstrate_heterogeneous_precision():\n    \"\"\"Demonstrate Heterogeneous Precision Analog Computing (HPAC).\"\"\"\n    print(\"=\" * 60)\n    print(\"HETEROGENEOUS PRECISION ANALOG COMPUTING (HPAC)\")\n    print(\"=\" * 60)\n    print(\"Adaptive precision allocation for 50\u00d7 energy reduction\")\n    print()\n    \n    # Create a large multi-scale problem\n    multiscale_pde = PoissonEquation(\n        domain_size=(256, 256),\n        boundary_conditions='dirichlet',\n        source_function=lambda x, y: (np.sin(5 * np.pi * x) * np.sin(5 * np.pi * y) +\n                                    0.1 * np.sin(50 * np.pi * x) * np.sin(50 * np.pi * y))\n    )\n    \n    framework = AdvancedSolverFramework(\n        base_crossbar_size=256,\n        performance_mode='energy'\n    )\n    \n    # Problem characteristics that favor heterogeneous precision\n    characteristics = ProblemCharacteristics(\n        problem_size=(256, 256),\n        sparsity_level=0.3,\n        time_dependent=False,\n        multi_physics=False,\n        conservation_required=False,\n        accuracy_requirement=1e-6,\n        energy_budget=1.0,  # Energy budget constraint\n        real_time_requirement=False,\n        physics_constraints=[],\n        boundary_complexity='complex'  # Multi-scale = complex\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Domain size: {characteristics.problem_size}\")\n    print(f\"- Multi-scale features: Yes (5\u00d7 and 50\u00d7 frequencies)\")\n    print(f\"- Energy budget: {characteristics.energy_budget}\")\n    print(f\"- Expected energy reduction: 50\u00d7\")\n    print()\n    \n    try:\n        # Solve with heterogeneous precision\n        print(\"Solving with HPAC...\")\n        solution, solve_info = framework.solve_pde(\n            multiscale_pde,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.HETEROGENEOUS_PRECISION,\n            initial_solution=np.random.random(256*256)\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Precision levels used: LOW, MEDIUM, HIGH, ULTRA\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Estimated energy savings: {rec['estimated_energy_savings']:.1%}\")\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n        \n        # Show precision distribution (simulated)\n        print(\""
    },
    {
      "category": "weak_crypto",
      "severity": "medium",
      "description": "Potential weak crypto detected",
      "file_path": "build/examples/src/advanced_algorithms_showcase.py",
      "line_number": 2,
      "details": "Precision Allocation:\")\n        print(\"- Low precision regions: 40% (smooth areas)\")\n        print(\"- Medium precision regions: 35% (moderate gradients)\")\n        print(\"- High precision regions: 20% (sharp features)\")\n        print(\"- Ultra precision regions: 5% (critical boundaries)\")\n        \n    except Exception as e:\n        print(f\"Heterogeneous precision demonstration failed: {e}\")\n    \n    print()\n\n\ndef demonstrate_multi_physics_coupling():\n    \"\"\"Demonstrate Analog Multi-Physics Coupling (AMPC).\"\"\"\n    print(\"=\" * 60)\n    print(\"ANALOG MULTI-PHYSICS COUPLING (AMPC)\")\n    print(\"=\" * 60)\n    print(\"Direct analog coupling eliminating 90% of interface overhead\")\n    print()\n    \n    framework = AdvancedSolverFramework(\n        base_crossbar_size=128,\n        enable_multi_physics=True,\n        performance_mode='balanced'\n    )\n    \n    # Multi-physics problem characteristics\n    characteristics = ProblemCharacteristics(\n        problem_size=(128, 128),\n        sparsity_level=0.2,\n        time_dependent=True,\n        multi_physics=True,  # Key for multi-physics coupling\n        conservation_required=True,  # Conservation across domains\n        accuracy_requirement=1e-6,\n        energy_budget=None,\n        real_time_requirement=False,\n        physics_constraints=['conservation', 'coupling'],\n        boundary_complexity='complex'\n    )\n    \n    print(\"Problem Setup:\")\n    print(f\"- Coupled domains: Thermal + Fluid\")\n    print(f\"- Domain size per physics: 64\u00d764 each\")\n    print(f\"- Conservation required: {characteristics.conservation_required}\")\n    print(f\"- Coupling type: Bidirectional thermal-fluid\")\n    print(f\"- Expected interface overhead reduction: 90%\")\n    print()\n    \n    try:\n        # Create dummy PDE for multi-physics (framework handles the coupling)\n        dummy_pde = PoissonEquation(\n            domain_size=(64, 64),\n            boundary_conditions='dirichlet'\n        )\n        \n        print(\"Solving with AMPC...\")\n        solution, solve_info = framework.solve_pde(\n            dummy_pde,\n            problem_characteristics=characteristics,\n            algorithm_preference=AlgorithmType.MULTI_PHYSICS,\n            time_span=(0.0, 1.0),\n            num_time_steps=50,\n            initial_conditions={\n                'thermal': np.random.random(64),\n                'fluid': np.random.random(64)\n            }\n        )\n        \n        print(\"Results:\")\n        print(f\"- Algorithm used: {solve_info['selected_algorithm']}\")\n        print(f\"- Solve time: {solve_info.get('total_framework_time', 0):.4f}s\")\n        print(f\"- Combined solution norm: {np.linalg.norm(solution):.6f}\")\n        print(f\"- Coupling domains: 2 (thermal, fluid)\")\n        \n        if 'algorithm_recommendation' in solve_info:\n            rec = solve_info['algorithm_recommendation']\n            print(f\"- Estimated speedup: {rec['estimated_speedup']:.0f}\u00d7\")\n            print(f\"- Algorithm confidence: {rec['confidence']:.2%}\")\n        \n        print(\""
    },
    {
      "category": "dangerous_imports",
      "severity": "low",
      "description": "Import of potentially dangerous module: pickle",
      "file_path": "analog_pde_solver/research/ml_acceleration.py",
      "line_number": 7,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "automation/dependency-updater.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "metrics/collect-metrics.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/autonomous-discovery.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/run-benchmarks.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/simple-discovery.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/update-backlog.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "scripts/validate-workflows.py",
      "line_number": null,
      "details": null
    },
    {
      "category": "executable_python",
      "severity": "low",
      "description": "Executable Python file (verify this is intentional)",
      "file_path": "monitoring/health-checks/health_check.py",
      "line_number": null,
      "details": null
    }
  ],
  "summary": {
    "total_findings": 92,
    "risk_score": 569,
    "risk_level": "HIGH",
    "categories": {
      "command_injection": {
        "count": 17,
        "severity_breakdown": {
          "high": 17
        }
      },
      "weak_crypto": {
        "count": 50,
        "severity_breakdown": {
          "medium": 50
        }
      },
      "dangerous_imports": {
        "count": 1,
        "severity_breakdown": {
          "low": 1
        }
      },
      "dangerous_functions": {
        "count": 6,
        "severity_breakdown": {
          "high": 6
        }
      },
      "hardcoded_secrets": {
        "count": 1,
        "severity_breakdown": {
          "high": 1
        }
      },
      "pickle_usage": {
        "count": 3,
        "severity_breakdown": {
          "medium": 3
        }
      },
      "executable_python": {
        "count": 8,
        "severity_breakdown": {
          "low": 8
        }
      },
      "git_history_secrets": {
        "count": 1,
        "severity_breakdown": {
          "medium": 1
        }
      },
      "config_credentials": {
        "count": 5,
        "severity_breakdown": {
          "high": 5
        }
      }
    },
    "recommendations": [
      "Address all HIGH severity findings immediately",
      "Review and fix MEDIUM severity findings",
      "Use environment variables or secure vaults for secrets",
      "Upgrade to stronger cryptographic algorithms",
      "Remove hardcoded credentials from configuration files",
      "Implement automated security scanning in CI/CD pipeline",
      "Conduct regular security code reviews",
      "Keep dependencies updated",
      "Use static analysis security testing (SAST) tools"
    ]
  }
}